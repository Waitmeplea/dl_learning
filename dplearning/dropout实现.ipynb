{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:12:28.116019Z",
     "start_time": "2025-03-18T00:12:26.789715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "sys.path.append('./book_material')\n",
    "from dataset.mnist import *\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "##wb是模型本身的参数因此放在构造函数中无需手动进行更改 其他则由输入的x决定\n",
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.w=w\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dw=None\n",
    "        self.db=None\n",
    "        self.dx=None\n",
    "    def forward(self,x):\n",
    "        if x.ndim==1:\n",
    "            x = x.reshape(1,-1)\n",
    "        self.x=x\n",
    "        output=np.dot(self.x, self.w)+self.b\n",
    "        return output\n",
    "    def backward(self,d_out):\n",
    "\n",
    "        self.dx=np.dot(d_out,self.w.T)\n",
    "        self.dw=np.dot(self.x.T,d_out)\n",
    "        self.db=np.sum(d_out,axis=0)\n",
    "        return self.dx\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask=x>0\n",
    "        out=self.mask*x\n",
    "        return out\n",
    "    def backward(self,d_out):\n",
    "        dx=self.mask*d_out\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "class Softmaxwithloss:\n",
    "    def __init__(self):\n",
    "        self.t=None\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "        self.dx=None\n",
    "        self.batch_size = None\n",
    "        self.w_rate=0.1\n",
    "        self.w=None\n",
    "    def forward(self,x):\n",
    "        if x.ndim==1:\n",
    "            x = x.reshape(1,-1)\n",
    "        self.x=x-np.max(x,axis=-1,keepdims=True)\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.y=np.exp(self.x)/np.sum(np.exp(self.x),axis=-1,keepdims=True)\n",
    "\n",
    "        return self.y\n",
    "    def loss(self,x,t):\n",
    "\n",
    "\n",
    "\n",
    "        out=self.forward(x)\n",
    "        self.t=t\n",
    "        if t.ndim!=1:\n",
    "            loss_rate=np.sum(-self.t*np.log(out+1e-7))/self.batch_size\n",
    "        else:\n",
    "            loss_rate=-np.sum(np.log(out[np.arange(len(t)),t]+1e-7))/self.batch_size\n",
    "\n",
    "\n",
    "        return  loss_rate\n",
    "\n",
    "    def backward(self,t=None):\n",
    "\n",
    "        self.t=t\n",
    "        if self.t.ndim!=1:\n",
    "            dx = (self.y - self.t) / self.batch_size\n",
    "        else:\n",
    "            y_c=self.y.copy()\n",
    "\n",
    "            y_c[np.arange(len(self.t)),self.t] -=1\n",
    "            dx=y_c/self.batch_size\n",
    "\n",
    "        self.dx=dx\n",
    "\n",
    "        return self.dx\n",
    "\n",
    "class dropout:\n",
    "    def __init__(self, rate=0.1):\n",
    "        self.rate = rate\n",
    "        self.mask = None\n",
    "        self.mode= 'train'\n",
    "    def forward(self, x):\n",
    "        if self.mode=='train':\n",
    "            self.mode = 'train'\n",
    "            self.mask = np.random.binomial(1,self.rate,size=x.shape)\n",
    "            self.mask = np.true_divide(self.mask,self.rate)\n",
    "            out=self.mask*x\n",
    "        else:\n",
    "            self.mode = 'test'\n",
    "            self.mask = np.ones(x.shape)\n",
    "            out=x\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        ###正向的时候 x m*n维  mask是m*n维 输出 m*n维 逐元素相乘\n",
    "        ###反向传播的时候 d_out m*n维 同样也是逐元素相乘\n",
    "        return d_out * self.mask\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayersNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_size_list=None,weight=1):\n",
    "        if hidden_size_list is None:\n",
    "            self.hidden_size_list = [100, 100, 100]\n",
    "        else:\n",
    "            self.hidden_size_list = hidden_size_list\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.params=dict()\n",
    "        self.sourcedata=None\n",
    "        self.layers= dict()\n",
    "        self.weight=weight\n",
    "        self.w_dict=None\n",
    "        #生成层\n",
    "        parameter_size_list=self.hidden_size_list\n",
    "        parameter_size_list.insert(0,input_size)\n",
    "        parameter_size_list.append(self.output_size)\n",
    "        self.hidden_size_list=parameter_size_list\n",
    "\n",
    "        for i in range(len(parameter_size_list) - 1): # 遍历所有 Affine 层\n",
    "            scale = np.sqrt(self.weight / parameter_size_list[i])\n",
    "            self.params['W' + str(i)] = np.random.randn(parameter_size_list[i], parameter_size_list[i + 1]) * scale # 使用 randn 初始化\n",
    "            self.params['b' + str(i)] = np.zeros(parameter_size_list[i + 1]) # 偏置初始化为 0\n",
    "            self.layers['affine' + str(i)] = Affine(self.params['W' + str(i)], self.params['b' + str(i)])\n",
    "\n",
    "            if i < len(parameter_size_list) - 2: # 除了最后一层 Affine，都添加 Relu 和dropout层\n",
    "                self.layers['relu' + str(i)] = Relu()\n",
    "                self.layers['dropout' + str(i)] = dropout(0.5)\n",
    "            else: # 最后一层 Affine 之后添加 Softmaxwithloss\n",
    "                self.layers['Activation_function'] = Softmaxwithloss()\n",
    "\n",
    "\n",
    "\n",
    "    def set_dropout_mode(self,mode):\n",
    "        if mode=='train':\n",
    "            pass\n",
    "        else:\n",
    "            for i in self.layers.keys():\n",
    "                if isinstance(self.layers[i],dropout):\n",
    "                    self.layers[i].mode=mode\n",
    "\n",
    "    def predict(self, x):\n",
    "        inputs=x\n",
    "        for key,func in self.layers.items():\n",
    "            inputs=func.forward(inputs)\n",
    "        return inputs\n",
    "\n",
    "    def loss(self,x,t,weight_decay_lambda=0):\n",
    "        w_decay=0\n",
    "        for w_key in self.params.keys():\n",
    "            if 'W' in w_key:\n",
    "                w_decay += 0.5*weight_decay_lambda * np.sum(self.params[w_key]**2)\n",
    "\n",
    "        inputs=x\n",
    "        for key,func in self.layers.items():\n",
    "            if key=='Activation_function':\n",
    "                loss_value=func.loss(inputs,t)+w_decay\n",
    "            else:\n",
    "                inputs=func.forward(inputs)\n",
    "\n",
    "\n",
    "\n",
    "        return loss_value\n",
    "\n",
    "    def backward(self,t,d_out=1):\n",
    "\n",
    "        back_list=list(self.layers.keys())\n",
    "        back_list.reverse()\n",
    "        d_out=d_out\n",
    "        for key in back_list:\n",
    "            if key=='Activation_function':\n",
    "\n",
    "                d_out=self.layers[key].backward(t=t)\n",
    "\n",
    "            else:\n",
    "                d_out=self.layers[key].backward(d_out)\n",
    "\n",
    "    def gradient(self,t,weight_decay_lambda=0):\n",
    "\n",
    "        self.backward(d_out=1,t=t)\n",
    "        grads=dict()\n",
    "        for idx in range(len(self.hidden_size_list)-1):\n",
    "            grads['W'+str(idx)]=self.layers['affine'+str(idx)].dw+weight_decay_lambda*(self.params['W'+str(idx)])\n",
    "            grads['b'+str(idx)]=self.layers['affine'+str(idx)].db\n",
    "        return grads\n",
    "    def accuracy(self,x,t):\n",
    "        if t.ndim!=1: t=np.argmax(t,axis=1)\n",
    "        y=np.argmax(self.predict(x),axis=1)\n",
    "        return np.sum(y==t)/y.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "mln7=MultiLayersNetwork(input_size=784,output_size=10,hidden_size_list=[100,100,100,100,100,100,100],weight=2)\n",
    "\n",
    "mask=np.random.choice(60000,size=1000)\n",
    "x_mask=x_train[mask]\n",
    "t_mask=t_train[mask]\n",
    "loss_list=[]\n",
    "mln7.set_dropout_mode('train')\n",
    "for i in range(100):\n",
    "    mask=np.random.choice(60000,size=300)\n",
    "    x_mask=x_train[mask]\n",
    "    t_mask=t_train[mask]\n",
    "    loss_list.append(mln7.loss(x_mask,t_mask,weight_decay_lambda=0.01))\n",
    "    grads=mln7.gradient(t=t_mask)\n",
    "    for key in grads.keys():\n",
    "        mln7.params[key] -=0.1*grads[key]\n",
    "loss_list"
   ],
   "id": "a4dca2dea2de5c3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.float64(14.218205349721082),\n",
       " np.float64(10.27567738146243),\n",
       " np.float64(9.644930252530497),\n",
       " np.float64(9.544747590057204),\n",
       " np.float64(9.43442221087102),\n",
       " np.float64(9.40402561782625),\n",
       " np.float64(9.413435427637806),\n",
       " np.float64(9.453263265789642),\n",
       " np.float64(9.370329414628838),\n",
       " np.float64(9.413192627064117),\n",
       " np.float64(9.36654227233143),\n",
       " np.float64(9.382299574428435),\n",
       " np.float64(9.350880970444708),\n",
       " np.float64(9.38601991386447),\n",
       " np.float64(9.365462348756425),\n",
       " np.float64(9.375933464415487),\n",
       " np.float64(9.388194942077835),\n",
       " np.float64(9.394797813986179),\n",
       " np.float64(9.349788938829414),\n",
       " np.float64(9.346006976563375),\n",
       " np.float64(9.364615386424239),\n",
       " np.float64(9.368642859629553),\n",
       " np.float64(9.355517902912158),\n",
       " np.float64(9.339060236043423),\n",
       " np.float64(9.368994544443456),\n",
       " np.float64(9.347884077786144),\n",
       " np.float64(9.350833450081971),\n",
       " np.float64(9.351138593061485),\n",
       " np.float64(9.355251535176814),\n",
       " np.float64(9.332714386766924),\n",
       " np.float64(9.350978075503898),\n",
       " np.float64(9.346840975268686),\n",
       " np.float64(9.33123062434414),\n",
       " np.float64(9.341225453554745),\n",
       " np.float64(9.368791752677584),\n",
       " np.float64(9.330965648360804),\n",
       " np.float64(9.345859613144588),\n",
       " np.float64(9.346708905681501),\n",
       " np.float64(9.347817473488076),\n",
       " np.float64(9.338715621080219),\n",
       " np.float64(9.341495603609609),\n",
       " np.float64(9.337643236205322),\n",
       " np.float64(9.313730198340195),\n",
       " np.float64(9.333219045047013),\n",
       " np.float64(9.297674495250282),\n",
       " np.float64(9.292439967352308),\n",
       " np.float64(9.32055151895378),\n",
       " np.float64(9.333654498518667),\n",
       " np.float64(9.339000954199046),\n",
       " np.float64(9.301307258780593),\n",
       " np.float64(9.265211265940406),\n",
       " np.float64(9.264349072967368),\n",
       " np.float64(9.276864684041607),\n",
       " np.float64(9.283426427543057),\n",
       " np.float64(9.318369274890292),\n",
       " np.float64(9.275135419323846),\n",
       " np.float64(9.267617831130575),\n",
       " np.float64(9.297675707743647),\n",
       " np.float64(9.222954456654335),\n",
       " np.float64(9.242049240994225),\n",
       " np.float64(9.27508431689762),\n",
       " np.float64(9.255061185061168),\n",
       " np.float64(9.285351993184154),\n",
       " np.float64(9.236888817025626),\n",
       " np.float64(9.22419768554319),\n",
       " np.float64(9.210893574930646),\n",
       " np.float64(9.224150209487687),\n",
       " np.float64(9.221145833275369),\n",
       " np.float64(9.187679338075846),\n",
       " np.float64(9.283039601115814),\n",
       " np.float64(9.25386850113241),\n",
       " np.float64(9.245999901653596),\n",
       " np.float64(9.214986075892638),\n",
       " np.float64(9.176074243982354),\n",
       " np.float64(9.190030034696798),\n",
       " np.float64(9.204199755254182),\n",
       " np.float64(9.23279512636891),\n",
       " np.float64(9.23157330757039),\n",
       " np.float64(9.179995023195088),\n",
       " np.float64(9.223381593395665),\n",
       " np.float64(9.212757971668646),\n",
       " np.float64(9.23130345409507),\n",
       " np.float64(9.240852035400657),\n",
       " np.float64(9.226305384405727),\n",
       " np.float64(9.271197569033522),\n",
       " np.float64(9.245779131462953),\n",
       " np.float64(9.220173135003368),\n",
       " np.float64(9.238776706448583),\n",
       " np.float64(9.251830553378063),\n",
       " np.float64(9.195822920409821),\n",
       " np.float64(9.216921815519028),\n",
       " np.float64(9.172763790191516),\n",
       " np.float64(9.082202667443285),\n",
       " np.float64(9.243762379613868),\n",
       " np.float64(9.181802263888635),\n",
       " np.float64(9.230552622574592),\n",
       " np.float64(9.168901475258046),\n",
       " np.float64(9.213725808699545),\n",
       " np.float64(9.176530797789754),\n",
       " np.float64(9.176795609384031)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T00:12:29.727212Z",
     "start_time": "2025-03-18T00:12:29.614649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mln7.set_dropout_mode('test')\n",
    "mln7.accuracy(x_test,t_test)"
   ],
   "id": "901bbe486320abc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.1589)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "631de5ce92c458a6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
