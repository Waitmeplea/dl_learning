{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-14T05:51:00.622190Z",
     "start_time": "2025-03-14T05:51:00.619378Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T05:51:06.731390Z",
     "start_time": "2025-03-14T05:51:06.708922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##wb是模型本身的参数因此放在构造函数中无需手动进行更改 其他则由输入的x决定\n",
    "class Affine:\n",
    "    def __init__(self,w,b):\n",
    "        self.w=w\n",
    "        self.b=b\n",
    "        self.x=None\n",
    "        self.dw=None\n",
    "        self.db=None\n",
    "        self.dx=None\n",
    "    def forward(self,x):\n",
    "        if x.ndim==1:\n",
    "            x = x.reshape(1,-1)\n",
    "        self.x=x\n",
    "        output=np.dot(self.x, self.w)+self.b\n",
    "        return output\n",
    "    def backward(self,d_out):\n",
    "        self.dx=np.dot(d_out,self.w.T)\n",
    "        self.dw=np.dot(self.x.T,d_out)\n",
    "        self.db=np.sum(d_out,axis=0)\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask=x>0\n",
    "        out=self.mask*x\n",
    "        return out\n",
    "    def backward(self,d_out):\n",
    "        dx=self.mask*d_out\n",
    "        return dx\n",
    "\n",
    "\n",
    "\n",
    "class Softmaxwithloss:\n",
    "    def __init__(self):\n",
    "        self.t=None\n",
    "        self.x=None\n",
    "        self.y=None\n",
    "        self.dx=None\n",
    "        self.batch_size = None\n",
    "    def forward(self,x):\n",
    "        if x.ndim==1:\n",
    "            x = x.reshape(1,-1)\n",
    "        self.x=x-np.max(x,axis=-1,keepdims=True)\n",
    "        self.batch_size = x.shape[0]\n",
    "        self.y=np.exp(self.x)/np.sum(np.exp(self.x),axis=-1,keepdims=True)\n",
    "        return self.y\n",
    "    def loss(self,x,t):\n",
    "        out=self.forward(x)\n",
    "        self.t=t\n",
    "        if t.ndim!=1:\n",
    "            loss_rate=np.sum(-self.t*np.log(out+1e-7))/self.batch_size\n",
    "        else:\n",
    "            loss_rate=-np.sum(np.log(out[np.arange(len(t)),t]+1e-7))/self.batch_size\n",
    "        return  loss_rate\n",
    "    \n",
    "    def backward(self):\n",
    "\n",
    "        if self.t.ndim!=1:\n",
    "            dx = (self.y - self.t) / self.batch_size\n",
    "        else:\n",
    "            y_c=self.y.copy()\n",
    "            # dx=(y_c[np.arange(len(self.t)),self.t]-1)/self.batch_size 这里错了\n",
    "            y_c[np.arange(len(self.t)),self.t] -=1\n",
    "            dx=y_c/self.batch_size\n",
    "        self.dx=dx\n",
    "        return self.dx\n"
   ],
   "id": "66f91ef2e2a40104",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiLayersNetwork:\n",
    "    def __init__(self, input_size, output_size, hidden_size_list=None):\n",
    "        if hidden_size_list is None:\n",
    "            self.hidden_size_list = [100, 100, 100]\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.output_size = output_size\n",
    "        self.params=dict()\n",
    "        self.sourcedata=None\n",
    "        self.layers= dict()\n",
    "        #生成层\n",
    "        parameter_size_list=self.hidden_size_list.insert(0,self.input_size)\n",
    "        parameter_size_list.append(self.output_size)\n",
    "        \n",
    "        ##倒数第二层之前全部用relu\n",
    "        for i in range(len(parameter_size_list)-2):\n",
    "            self.params['W'+str(i)]=np.random.rand(parameter_size_list[i],parameter_size_list[i+1])\n",
    "            self.params['b'+str(i)]=np.random.rand(parameter_size_list[i],parameter_size_list[i+1])\n",
    "            #每一层就自己层的权重和偏置\n",
    "            self.layers['affine'+str(i)]=Affine(self.params['W'+str(i)],self.params['b'+str(i)])\n",
    "            self.layers['Activation_function'+str(i)]=Relu()\n",
    "        \n",
    "        if i==len(parameter_size_list)-3:\n",
    "            i+=1\n",
    "            self.layers['affine'+str(i)]=Affine(self.params['W'+str(i)],self.params['b'+str(i)])\n",
    "            self.layers['Activation_function'+str(i)]=Softmaxwithloss()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs=x\n",
    "        for key,func in self.layers.items:\n",
    "            inputs=func.forward(inputs)\n",
    "\n",
    "            "
   ],
   "id": "1fcb75450a7fc1a4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T05:57:49.572738Z",
     "start_time": "2025-03-14T05:57:49.569688Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "891d28bdf7a2561a",
   "outputs": [],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
