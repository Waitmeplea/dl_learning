{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-21T07:52:37.449240Z",
     "start_time": "2025-05-21T07:52:28.661857Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "src, tgt = tokenize_nmt(preprocess_nmt(read_data_nmt()),num_examples=600)\n",
    "src_vocab = Vocal(src, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocal(tgt, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "src_data, src_valid = build_array_nmt(src, src_vocab, 10)\n",
    "tgt_data, tgt_valid = build_array_nmt(tgt, tgt_vocab, 10)\n",
    "dataset = torch.utils.data.TensorDataset(src_data, src_valid, tgt_data, tgt_valid)\n",
    "## 训练数据\n",
    "train_data = torch.utils.data.DataLoader(dataset=dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Bahdanau 注意力 只需要修改decoder部分",
   "id": "f5129c8ec1ef9a57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:13.971984Z",
     "start_time": "2025-05-21T07:54:13.958566Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def init_state(self, enc_outputs, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, *args, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(*args, **kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_x, dec_x, *args, **kwargs):\n",
    "        enc_outputs = self.encoder(enc_x, *args, **kwargs)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args, **kwargs)\n",
    "        return self.decoder(dec_x, dec_state, *args, **kwargs)\n",
    "\n",
    "class Seq2SeqEncoder_hmy(Encoder):\n",
    "    \"\"\"输入一个x batch_size*num_steps或num_steps*batch_size \n",
    "    输出output batch_size num_steps \n",
    "    隐藏状态 numlayers * batch_size * hidden_size\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,num_layers, dropout=0.1, *args, **kwargs):\n",
    "        super(Seq2SeqEncoder_hmy, self).__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size,num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, X, *args, **kwargs):\n",
    "        embed_x = self.embed(X)\n",
    "        outputs, state = self.rnn(embed_x)\n",
    "        return outputs, state\n",
    "    \n",
    "\n",
    "class AdditiveAttention_hmy(nn.Module):\n",
    "    \"\"\"加性注意力\"\"\"\n",
    "    def __init__(self,key_size,query_size,num_hiddens,dropout,**kwargs):\n",
    "        super(AdditiveAttention_hmy,self).__init__(**kwargs)\n",
    "        self.W_k = nn.Linear(key_size,num_hiddens,bias=False)\n",
    "        self.W_q = nn.Linear(query_size,num_hiddens,bias=False)\n",
    "        self.W_v = nn.Linear(num_hiddens,1,bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    # queries 维度应该是 batch_size * 要查询的数量 * q_size向量长度\n",
    "    # keys 维度是 batch_size * keys的数量（key-value)键值对 * key向量长度\n",
    "    # values与key相等 value_size可以不一样\n",
    "    def forward(self, queries, keys, values, valid_lens): ## valid_len从输入来的 屏蔽掉填充部分\n",
    "        queries,keys=self.W_q(queries),self.W_k(keys)\n",
    "        queries=queries.unsqueeze(2)\n",
    "        keys=keys.unsqueeze(1)\n",
    "        features = queries + keys\n",
    "        features = torch.tanh(features)\n",
    "        scores = self.W_v(features).squeeze(-1)\n",
    "        self.attention_weights = masked_softmax(scores, valid_lens)\n",
    "        attention_temp=self.dropout(self.attention_weights)\n",
    "        return torch.bmm(attention_temp, values)\n",
    "    \n",
    "    \n",
    "class AttentionDecoder(Decoder):\n",
    "    \"\"\"带有注意力机制解码器的基本接口\"\"\"\n",
    "    def __init__(self,**kwargs):\n",
    "        super(AttentionDecoder, self).__init__(**kwargs)\n",
    "    @property\n",
    "    def attention_weight(self):\n",
    "        raise NotImplementedError"
   ],
   "id": "3ee269894a468e8c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:14.392933Z",
     "start_time": "2025-05-21T07:54:14.383011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Seq2SeqAttentionDecoder(AttentionDecoder):\n",
    "    \"\"\"通过encoder传来的原始序列的编码信息 进行解码翻译\"\"\"\n",
    "    def __init__(self,vocab_size,embed_size,num_hiddens,num_layers,dropout=0,**kwargs):\n",
    "        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n",
    "        self.attention=AdditiveAttention_hmy(num_hiddens,num_hiddens,num_hiddens,dropout=dropout)\n",
    "        \n",
    "        self.embedding=nn.Embedding(vocab_size,embed_size)\n",
    "        self.rnn=nn.GRU(embed_size+num_hiddens,num_hiddens,num_layers,batch_first=True,dropout=dropout)\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "\n",
    "    def init_state(self,encoder_out,enc_valid_lens,*args):\n",
    "        outputs,hidden_state=encoder_out\n",
    "        return outputs,hidden_state,enc_valid_lens\n",
    "\n",
    "    def forward(self,X,state,*args,**kwargs):\n",
    "        #ouputs batch_size,num_steps,num_hiddens\n",
    "        enc_outputs, hidden_state, enc_valid_lens = state\n",
    "        X=self.embedding(X).permute(1,0,2)\n",
    "        dec_outputs, self._attention_weights = [], []\n",
    "        for x in X:\n",
    "            #，unsqueeze(1) 这一步的目的就是为了给 Decoder 的顶层隐藏状态显式地添加一个维度，用来表示 Query 的数量 (在这个时间步是 1)，\n",
    "            # 从而使其形状符合 Attention 模块期望的 (batch_size, num_queries, feature_size) 输入格式，使得 Attention 模块可以正确地进行批处理和内部计算。\n",
    "            # query的形状为(batch_size,1,num_hiddens)\n",
    "            query=hidden_state[-1].unsqueeze(1)\n",
    "            # qkv 和q的有效长度\n",
    "            # query batch_size,1,num_hiddens\n",
    "            # context batch_size,num_steps,num_hiddens\n",
    "            context=self.attention(query,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "            # x为batch_size,1,embed+hidden_size\n",
    "            x=torch.cat((context,x.unsqueeze(1)),dim=-1)\n",
    "            out,hidden_state=self.rnn(x,hidden_state)\n",
    "            dec_outputs.append(out)\n",
    "            self._attention_weights.append(self.attention.attention_weights)\n",
    "        dec_outputs = self.dense(torch.cat(dec_outputs, dim=1))\n",
    "        return dec_outputs, [enc_outputs, hidden_state,\n",
    "                                          enc_valid_lens]\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ],
   "id": "40a1b347a1b82404",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:14.517366Z",
     "start_time": "2025-05-21T07:54:14.511584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = Seq2SeqEncoder_hmy(vocab_size=10, embed_size=8, hidden_size=16,\n",
    "                             num_layers=2)\n",
    "\n",
    "decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8, num_hiddens=16,\n",
    "                                  num_layers=2)"
   ],
   "id": "8a5c5f40fcd868db",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:14.689881Z",
     "start_time": "2025-05-21T07:54:14.675457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = torch.zeros((4, 7), dtype=torch.long)  # (batch_size,num_steps)\n",
    "state = decoder.init_state(encoder(X), None)\n",
    "output, state = decoder(X, state)\n",
    "output.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape"
   ],
   "id": "acd4f30acd7c6260",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([4, 16]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:14.861489Z",
     "start_time": "2025-05-21T07:54:14.856400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    # 首先x是二维的 最内层维度是句子长度 注意：是训练集所以才知道句子真实长度\n",
    "    # 拿出总的长度 得到长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 然后用总长度生成一个1维的向量 使用函数扩展成2维以便与valid_len进行广播\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long), dim=0)\n",
    "    # mask在0维度扩充 valid在1维度扩充 因为每一个valid对应的是每一个x valid的数字其实是x的第二维向量\n",
    "    mask = (mask < torch.unsqueeze(valid_len, dim=1))  # 这里小于号就够了 因为<eos>所在位置的索引其实是valid_len-1\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "# 拓展的softmax因为对填充值进行softmax其实没有什么意义\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        # 交叉熵损失期望的两个输入 x是 batch_size vocab_size seq_lenth \n",
    "        # y 是 batch_size seq_len\n",
    "        unweight_loss = super().forward(pred, label)\n",
    "        weights_loss = unweight_loss * weights\n",
    "        return weights_loss.mean(dim=1)\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()"
   ],
   "id": "d4e9fcbce639c0fb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:54:15.364287Z",
     "start_time": "2025-05-21T07:54:15.357365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs = 0.005, 250\n",
    "\n",
    "encoder=Seq2SeqEncoder_hmy(vocab_size=len(src_vocab), embed_size=32, hidden_size=32,num_layers=2)\n",
    "decoder=Seq2SeqAttentionDecoder(vocab_size=len(tgt_vocab), embed_size=32, num_hiddens=32,num_layers=2)\n",
    "net=EncoderDecoder(encoder, decoder)"
   ],
   "id": "269edf30c66a23f1",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:55:55.621441Z",
     "start_time": "2025-05-21T07:54:15.514605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "for epoch in range(300):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src,src_valid,tgt,tgt_valid=batch\n",
    "        Y=torch.cat((torch.tensor([tgt_vocab['<bos>']]).unsqueeze(0).repeat(tgt.shape[0],1),tgt),dim=1)[:,:-1]\n",
    "        # 易错点1：训练的时候应该使用带有bos的Y 表示强制教学 此时输出的y_hat实际上是没有bos的\n",
    "        y_hat,_=net(src,Y,src_valid)\n",
    "        # 点2 因为输出的y_hat 没有bos 因此在计算loss的时候应该使用原始序列作为目标序列\n",
    "        l=loss(y_hat,tgt,tgt_valid).sum()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "    print(l)"
   ],
   "id": "a4382de9572cfb4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.7793, grad_fn=<SumBackward0>)\n",
      "tensor(45.9947, grad_fn=<SumBackward0>)\n",
      "tensor(41.8270, grad_fn=<SumBackward0>)\n",
      "tensor(39.1910, grad_fn=<SumBackward0>)\n",
      "tensor(37.8304, grad_fn=<SumBackward0>)\n",
      "tensor(36.4568, grad_fn=<SumBackward0>)\n",
      "tensor(35.1299, grad_fn=<SumBackward0>)\n",
      "tensor(33.8418, grad_fn=<SumBackward0>)\n",
      "tensor(32.4014, grad_fn=<SumBackward0>)\n",
      "tensor(31.1353, grad_fn=<SumBackward0>)\n",
      "tensor(29.9561, grad_fn=<SumBackward0>)\n",
      "tensor(29.0039, grad_fn=<SumBackward0>)\n",
      "tensor(27.0924, grad_fn=<SumBackward0>)\n",
      "tensor(26.7300, grad_fn=<SumBackward0>)\n",
      "tensor(24.4743, grad_fn=<SumBackward0>)\n",
      "tensor(23.2258, grad_fn=<SumBackward0>)\n",
      "tensor(22.2059, grad_fn=<SumBackward0>)\n",
      "tensor(21.5985, grad_fn=<SumBackward0>)\n",
      "tensor(19.7970, grad_fn=<SumBackward0>)\n",
      "tensor(18.5557, grad_fn=<SumBackward0>)\n",
      "tensor(17.7955, grad_fn=<SumBackward0>)\n",
      "tensor(17.2582, grad_fn=<SumBackward0>)\n",
      "tensor(16.7453, grad_fn=<SumBackward0>)\n",
      "tensor(15.8407, grad_fn=<SumBackward0>)\n",
      "tensor(14.9559, grad_fn=<SumBackward0>)\n",
      "tensor(14.1842, grad_fn=<SumBackward0>)\n",
      "tensor(13.2744, grad_fn=<SumBackward0>)\n",
      "tensor(13.3175, grad_fn=<SumBackward0>)\n",
      "tensor(12.3250, grad_fn=<SumBackward0>)\n",
      "tensor(11.7497, grad_fn=<SumBackward0>)\n",
      "tensor(11.4035, grad_fn=<SumBackward0>)\n",
      "tensor(11.5763, grad_fn=<SumBackward0>)\n",
      "tensor(11.7389, grad_fn=<SumBackward0>)\n",
      "tensor(10.6176, grad_fn=<SumBackward0>)\n",
      "tensor(10.2568, grad_fn=<SumBackward0>)\n",
      "tensor(9.6608, grad_fn=<SumBackward0>)\n",
      "tensor(9.1256, grad_fn=<SumBackward0>)\n",
      "tensor(8.8902, grad_fn=<SumBackward0>)\n",
      "tensor(8.5879, grad_fn=<SumBackward0>)\n",
      "tensor(8.2434, grad_fn=<SumBackward0>)\n",
      "tensor(7.8155, grad_fn=<SumBackward0>)\n",
      "tensor(7.7004, grad_fn=<SumBackward0>)\n",
      "tensor(8.0600, grad_fn=<SumBackward0>)\n",
      "tensor(7.5054, grad_fn=<SumBackward0>)\n",
      "tensor(7.4842, grad_fn=<SumBackward0>)\n",
      "tensor(7.4275, grad_fn=<SumBackward0>)\n",
      "tensor(7.1178, grad_fn=<SumBackward0>)\n",
      "tensor(6.7015, grad_fn=<SumBackward0>)\n",
      "tensor(6.4331, grad_fn=<SumBackward0>)\n",
      "tensor(6.2476, grad_fn=<SumBackward0>)\n",
      "tensor(5.9187, grad_fn=<SumBackward0>)\n",
      "tensor(5.6381, grad_fn=<SumBackward0>)\n",
      "tensor(5.4043, grad_fn=<SumBackward0>)\n",
      "tensor(5.6032, grad_fn=<SumBackward0>)\n",
      "tensor(5.3606, grad_fn=<SumBackward0>)\n",
      "tensor(5.8743, grad_fn=<SumBackward0>)\n",
      "tensor(5.2154, grad_fn=<SumBackward0>)\n",
      "tensor(5.2101, grad_fn=<SumBackward0>)\n",
      "tensor(4.9853, grad_fn=<SumBackward0>)\n",
      "tensor(4.9190, grad_fn=<SumBackward0>)\n",
      "tensor(4.5853, grad_fn=<SumBackward0>)\n",
      "tensor(4.1967, grad_fn=<SumBackward0>)\n",
      "tensor(4.2725, grad_fn=<SumBackward0>)\n",
      "tensor(4.3870, grad_fn=<SumBackward0>)\n",
      "tensor(4.2216, grad_fn=<SumBackward0>)\n",
      "tensor(3.8827, grad_fn=<SumBackward0>)\n",
      "tensor(3.5905, grad_fn=<SumBackward0>)\n",
      "tensor(3.8924, grad_fn=<SumBackward0>)\n",
      "tensor(3.4027, grad_fn=<SumBackward0>)\n",
      "tensor(3.3938, grad_fn=<SumBackward0>)\n",
      "tensor(3.4614, grad_fn=<SumBackward0>)\n",
      "tensor(3.2951, grad_fn=<SumBackward0>)\n",
      "tensor(3.1441, grad_fn=<SumBackward0>)\n",
      "tensor(3.0098, grad_fn=<SumBackward0>)\n",
      "tensor(2.9144, grad_fn=<SumBackward0>)\n",
      "tensor(3.2071, grad_fn=<SumBackward0>)\n",
      "tensor(2.9018, grad_fn=<SumBackward0>)\n",
      "tensor(2.9024, grad_fn=<SumBackward0>)\n",
      "tensor(2.7611, grad_fn=<SumBackward0>)\n",
      "tensor(2.6607, grad_fn=<SumBackward0>)\n",
      "tensor(2.5158, grad_fn=<SumBackward0>)\n",
      "tensor(2.4826, grad_fn=<SumBackward0>)\n",
      "tensor(2.5010, grad_fn=<SumBackward0>)\n",
      "tensor(2.4380, grad_fn=<SumBackward0>)\n",
      "tensor(2.4817, grad_fn=<SumBackward0>)\n",
      "tensor(2.3440, grad_fn=<SumBackward0>)\n",
      "tensor(2.3927, grad_fn=<SumBackward0>)\n",
      "tensor(2.2421, grad_fn=<SumBackward0>)\n",
      "tensor(2.2989, grad_fn=<SumBackward0>)\n",
      "tensor(2.4866, grad_fn=<SumBackward0>)\n",
      "tensor(2.3149, grad_fn=<SumBackward0>)\n",
      "tensor(2.2804, grad_fn=<SumBackward0>)\n",
      "tensor(2.2551, grad_fn=<SumBackward0>)\n",
      "tensor(2.3047, grad_fn=<SumBackward0>)\n",
      "tensor(2.2557, grad_fn=<SumBackward0>)\n",
      "tensor(2.1180, grad_fn=<SumBackward0>)\n",
      "tensor(2.1185, grad_fn=<SumBackward0>)\n",
      "tensor(2.0625, grad_fn=<SumBackward0>)\n",
      "tensor(2.0031, grad_fn=<SumBackward0>)\n",
      "tensor(2.0262, grad_fn=<SumBackward0>)\n",
      "tensor(2.1020, grad_fn=<SumBackward0>)\n",
      "tensor(2.1050, grad_fn=<SumBackward0>)\n",
      "tensor(2.3213, grad_fn=<SumBackward0>)\n",
      "tensor(2.4200, grad_fn=<SumBackward0>)\n",
      "tensor(2.1323, grad_fn=<SumBackward0>)\n",
      "tensor(2.1382, grad_fn=<SumBackward0>)\n",
      "tensor(1.9339, grad_fn=<SumBackward0>)\n",
      "tensor(1.9881, grad_fn=<SumBackward0>)\n",
      "tensor(1.8437, grad_fn=<SumBackward0>)\n",
      "tensor(2.0701, grad_fn=<SumBackward0>)\n",
      "tensor(1.9382, grad_fn=<SumBackward0>)\n",
      "tensor(2.0100, grad_fn=<SumBackward0>)\n",
      "tensor(1.8954, grad_fn=<SumBackward0>)\n",
      "tensor(1.8745, grad_fn=<SumBackward0>)\n",
      "tensor(1.8690, grad_fn=<SumBackward0>)\n",
      "tensor(1.8637, grad_fn=<SumBackward0>)\n",
      "tensor(1.8612, grad_fn=<SumBackward0>)\n",
      "tensor(1.8011, grad_fn=<SumBackward0>)\n",
      "tensor(1.7990, grad_fn=<SumBackward0>)\n",
      "tensor(1.9673, grad_fn=<SumBackward0>)\n",
      "tensor(1.8510, grad_fn=<SumBackward0>)\n",
      "tensor(1.6589, grad_fn=<SumBackward0>)\n",
      "tensor(1.7784, grad_fn=<SumBackward0>)\n",
      "tensor(1.8154, grad_fn=<SumBackward0>)\n",
      "tensor(1.8495, grad_fn=<SumBackward0>)\n",
      "tensor(1.8178, grad_fn=<SumBackward0>)\n",
      "tensor(1.7583, grad_fn=<SumBackward0>)\n",
      "tensor(1.6942, grad_fn=<SumBackward0>)\n",
      "tensor(1.7780, grad_fn=<SumBackward0>)\n",
      "tensor(1.7992, grad_fn=<SumBackward0>)\n",
      "tensor(2.1960, grad_fn=<SumBackward0>)\n",
      "tensor(1.7490, grad_fn=<SumBackward0>)\n",
      "tensor(1.6710, grad_fn=<SumBackward0>)\n",
      "tensor(1.6292, grad_fn=<SumBackward0>)\n",
      "tensor(1.7431, grad_fn=<SumBackward0>)\n",
      "tensor(1.6847, grad_fn=<SumBackward0>)\n",
      "tensor(1.7199, grad_fn=<SumBackward0>)\n",
      "tensor(1.6901, grad_fn=<SumBackward0>)\n",
      "tensor(1.7047, grad_fn=<SumBackward0>)\n",
      "tensor(1.6497, grad_fn=<SumBackward0>)\n",
      "tensor(1.5808, grad_fn=<SumBackward0>)\n",
      "tensor(1.6930, grad_fn=<SumBackward0>)\n",
      "tensor(1.6617, grad_fn=<SumBackward0>)\n",
      "tensor(1.5909, grad_fn=<SumBackward0>)\n",
      "tensor(1.7165, grad_fn=<SumBackward0>)\n",
      "tensor(1.8024, grad_fn=<SumBackward0>)\n",
      "tensor(1.5781, grad_fn=<SumBackward0>)\n",
      "tensor(1.6302, grad_fn=<SumBackward0>)\n",
      "tensor(1.7065, grad_fn=<SumBackward0>)\n",
      "tensor(1.8268, grad_fn=<SumBackward0>)\n",
      "tensor(1.8656, grad_fn=<SumBackward0>)\n",
      "tensor(1.6331, grad_fn=<SumBackward0>)\n",
      "tensor(1.7202, grad_fn=<SumBackward0>)\n",
      "tensor(1.7974, grad_fn=<SumBackward0>)\n",
      "tensor(1.5582, grad_fn=<SumBackward0>)\n",
      "tensor(1.9039, grad_fn=<SumBackward0>)\n",
      "tensor(1.7766, grad_fn=<SumBackward0>)\n",
      "tensor(1.6516, grad_fn=<SumBackward0>)\n",
      "tensor(1.7316, grad_fn=<SumBackward0>)\n",
      "tensor(1.7129, grad_fn=<SumBackward0>)\n",
      "tensor(1.6406, grad_fn=<SumBackward0>)\n",
      "tensor(1.6140, grad_fn=<SumBackward0>)\n",
      "tensor(1.5878, grad_fn=<SumBackward0>)\n",
      "tensor(1.6007, grad_fn=<SumBackward0>)\n",
      "tensor(1.5681, grad_fn=<SumBackward0>)\n",
      "tensor(1.5422, grad_fn=<SumBackward0>)\n",
      "tensor(1.5353, grad_fn=<SumBackward0>)\n",
      "tensor(1.5197, grad_fn=<SumBackward0>)\n",
      "tensor(1.5308, grad_fn=<SumBackward0>)\n",
      "tensor(1.6100, grad_fn=<SumBackward0>)\n",
      "tensor(1.5347, grad_fn=<SumBackward0>)\n",
      "tensor(1.5540, grad_fn=<SumBackward0>)\n",
      "tensor(1.5711, grad_fn=<SumBackward0>)\n",
      "tensor(1.5117, grad_fn=<SumBackward0>)\n",
      "tensor(1.4991, grad_fn=<SumBackward0>)\n",
      "tensor(1.5466, grad_fn=<SumBackward0>)\n",
      "tensor(1.5305, grad_fn=<SumBackward0>)\n",
      "tensor(1.5907, grad_fn=<SumBackward0>)\n",
      "tensor(1.5144, grad_fn=<SumBackward0>)\n",
      "tensor(1.5966, grad_fn=<SumBackward0>)\n",
      "tensor(1.4543, grad_fn=<SumBackward0>)\n",
      "tensor(1.5260, grad_fn=<SumBackward0>)\n",
      "tensor(1.5466, grad_fn=<SumBackward0>)\n",
      "tensor(1.5553, grad_fn=<SumBackward0>)\n",
      "tensor(1.5382, grad_fn=<SumBackward0>)\n",
      "tensor(1.5413, grad_fn=<SumBackward0>)\n",
      "tensor(1.5162, grad_fn=<SumBackward0>)\n",
      "tensor(1.9023, grad_fn=<SumBackward0>)\n",
      "tensor(1.7013, grad_fn=<SumBackward0>)\n",
      "tensor(1.9232, grad_fn=<SumBackward0>)\n",
      "tensor(1.6910, grad_fn=<SumBackward0>)\n",
      "tensor(1.6259, grad_fn=<SumBackward0>)\n",
      "tensor(1.7320, grad_fn=<SumBackward0>)\n",
      "tensor(1.5277, grad_fn=<SumBackward0>)\n",
      "tensor(1.5247, grad_fn=<SumBackward0>)\n",
      "tensor(1.7088, grad_fn=<SumBackward0>)\n",
      "tensor(1.7550, grad_fn=<SumBackward0>)\n",
      "tensor(1.6866, grad_fn=<SumBackward0>)\n",
      "tensor(1.5052, grad_fn=<SumBackward0>)\n",
      "tensor(1.5413, grad_fn=<SumBackward0>)\n",
      "tensor(1.6033, grad_fn=<SumBackward0>)\n",
      "tensor(1.6153, grad_fn=<SumBackward0>)\n",
      "tensor(1.5960, grad_fn=<SumBackward0>)\n",
      "tensor(1.5906, grad_fn=<SumBackward0>)\n",
      "tensor(1.5853, grad_fn=<SumBackward0>)\n",
      "tensor(1.5495, grad_fn=<SumBackward0>)\n",
      "tensor(1.4918, grad_fn=<SumBackward0>)\n",
      "tensor(1.4888, grad_fn=<SumBackward0>)\n",
      "tensor(1.5457, grad_fn=<SumBackward0>)\n",
      "tensor(1.5324, grad_fn=<SumBackward0>)\n",
      "tensor(1.5382, grad_fn=<SumBackward0>)\n",
      "tensor(1.5510, grad_fn=<SumBackward0>)\n",
      "tensor(1.4962, grad_fn=<SumBackward0>)\n",
      "tensor(1.4918, grad_fn=<SumBackward0>)\n",
      "tensor(1.5307, grad_fn=<SumBackward0>)\n",
      "tensor(1.4992, grad_fn=<SumBackward0>)\n",
      "tensor(1.5305, grad_fn=<SumBackward0>)\n",
      "tensor(1.5466, grad_fn=<SumBackward0>)\n",
      "tensor(1.5647, grad_fn=<SumBackward0>)\n",
      "tensor(1.4717, grad_fn=<SumBackward0>)\n",
      "tensor(1.5328, grad_fn=<SumBackward0>)\n",
      "tensor(1.5407, grad_fn=<SumBackward0>)\n",
      "tensor(1.5066, grad_fn=<SumBackward0>)\n",
      "tensor(1.4952, grad_fn=<SumBackward0>)\n",
      "tensor(1.5010, grad_fn=<SumBackward0>)\n",
      "tensor(1.5298, grad_fn=<SumBackward0>)\n",
      "tensor(1.4916, grad_fn=<SumBackward0>)\n",
      "tensor(1.5063, grad_fn=<SumBackward0>)\n",
      "tensor(1.5084, grad_fn=<SumBackward0>)\n",
      "tensor(1.4462, grad_fn=<SumBackward0>)\n",
      "tensor(1.5418, grad_fn=<SumBackward0>)\n",
      "tensor(1.6942, grad_fn=<SumBackward0>)\n",
      "tensor(1.4965, grad_fn=<SumBackward0>)\n",
      "tensor(1.5045, grad_fn=<SumBackward0>)\n",
      "tensor(1.5174, grad_fn=<SumBackward0>)\n",
      "tensor(1.5378, grad_fn=<SumBackward0>)\n",
      "tensor(1.5277, grad_fn=<SumBackward0>)\n",
      "tensor(1.5062, grad_fn=<SumBackward0>)\n",
      "tensor(1.5739, grad_fn=<SumBackward0>)\n",
      "tensor(1.4779, grad_fn=<SumBackward0>)\n",
      "tensor(1.9206, grad_fn=<SumBackward0>)\n",
      "tensor(1.6741, grad_fn=<SumBackward0>)\n",
      "tensor(1.5629, grad_fn=<SumBackward0>)\n",
      "tensor(1.5470, grad_fn=<SumBackward0>)\n",
      "tensor(1.5224, grad_fn=<SumBackward0>)\n",
      "tensor(1.4635, grad_fn=<SumBackward0>)\n",
      "tensor(1.4979, grad_fn=<SumBackward0>)\n",
      "tensor(1.4737, grad_fn=<SumBackward0>)\n",
      "tensor(1.5043, grad_fn=<SumBackward0>)\n",
      "tensor(1.5510, grad_fn=<SumBackward0>)\n",
      "tensor(1.6821, grad_fn=<SumBackward0>)\n",
      "tensor(1.4798, grad_fn=<SumBackward0>)\n",
      "tensor(1.6956, grad_fn=<SumBackward0>)\n",
      "tensor(1.4545, grad_fn=<SumBackward0>)\n",
      "tensor(1.8711, grad_fn=<SumBackward0>)\n",
      "tensor(1.6386, grad_fn=<SumBackward0>)\n",
      "tensor(1.5521, grad_fn=<SumBackward0>)\n",
      "tensor(1.7955, grad_fn=<SumBackward0>)\n",
      "tensor(1.6306, grad_fn=<SumBackward0>)\n",
      "tensor(1.4322, grad_fn=<SumBackward0>)\n",
      "tensor(1.7291, grad_fn=<SumBackward0>)\n",
      "tensor(1.5069, grad_fn=<SumBackward0>)\n",
      "tensor(1.5223, grad_fn=<SumBackward0>)\n",
      "tensor(1.7887, grad_fn=<SumBackward0>)\n",
      "tensor(1.5196, grad_fn=<SumBackward0>)\n",
      "tensor(1.5883, grad_fn=<SumBackward0>)\n",
      "tensor(1.5260, grad_fn=<SumBackward0>)\n",
      "tensor(1.4993, grad_fn=<SumBackward0>)\n",
      "tensor(1.4986, grad_fn=<SumBackward0>)\n",
      "tensor(1.5259, grad_fn=<SumBackward0>)\n",
      "tensor(1.4960, grad_fn=<SumBackward0>)\n",
      "tensor(1.4882, grad_fn=<SumBackward0>)\n",
      "tensor(1.4703, grad_fn=<SumBackward0>)\n",
      "tensor(1.4405, grad_fn=<SumBackward0>)\n",
      "tensor(1.4862, grad_fn=<SumBackward0>)\n",
      "tensor(1.4707, grad_fn=<SumBackward0>)\n",
      "tensor(1.5197, grad_fn=<SumBackward0>)\n",
      "tensor(1.5878, grad_fn=<SumBackward0>)\n",
      "tensor(1.4838, grad_fn=<SumBackward0>)\n",
      "tensor(1.4743, grad_fn=<SumBackward0>)\n",
      "tensor(1.4970, grad_fn=<SumBackward0>)\n",
      "tensor(1.4922, grad_fn=<SumBackward0>)\n",
      "tensor(1.5659, grad_fn=<SumBackward0>)\n",
      "tensor(1.5297, grad_fn=<SumBackward0>)\n",
      "tensor(1.4639, grad_fn=<SumBackward0>)\n",
      "tensor(1.5574, grad_fn=<SumBackward0>)\n",
      "tensor(1.5124, grad_fn=<SumBackward0>)\n",
      "tensor(1.5230, grad_fn=<SumBackward0>)\n",
      "tensor(1.5522, grad_fn=<SumBackward0>)\n",
      "tensor(1.5795, grad_fn=<SumBackward0>)\n",
      "tensor(1.4973, grad_fn=<SumBackward0>)\n",
      "tensor(1.5040, grad_fn=<SumBackward0>)\n",
      "tensor(1.4469, grad_fn=<SumBackward0>)\n",
      "tensor(1.4893, grad_fn=<SumBackward0>)\n",
      "tensor(1.4781, grad_fn=<SumBackward0>)\n",
      "tensor(1.5189, grad_fn=<SumBackward0>)\n",
      "tensor(1.5888, grad_fn=<SumBackward0>)\n",
      "tensor(1.4950, grad_fn=<SumBackward0>)\n",
      "tensor(1.4880, grad_fn=<SumBackward0>)\n",
      "tensor(1.5308, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:56:07.883929Z",
     "start_time": "2025-05-21T07:56:07.851822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# predict\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for src_sentence,tgt_sentence in zip(engs,fras):\n",
    "    src_tokens=[src_vocab[i] for i in src_sentence.split(' ')]+[src_vocab['<eos>']]\n",
    "    src_data=truncate_pad(src_tokens,10,1)\n",
    "    # 易错点 srcdata在生成完成后需要unsqueeze 因为原本是1维的需要增加一个批次维度\n",
    "    src_data=torch.tensor(src_data).unsqueeze(0)\n",
    "    # 易错点 需要加上这个批次的有效长度建议1维 或者无维度\n",
    "    enc_valid_len=torch.tensor([len(src_tokens)])\n",
    "    \n",
    "    enc_outputs=net.encoder(src_data,enc_valid_len)\n",
    "    state=net.decoder.init_state(enc_outputs,enc_valid_len)\n",
    "    # 易错点 这里必须得是long 因为embed层需要long输入 并且要unsequeeze 增加一个维度\n",
    "    dec_x=torch.tensor([tgt_vocab['<bos>']],dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    output_list=[]\n",
    "    for i in range(10):\n",
    "        output,state=net.decoder(dec_x,state)\n",
    "        # 易错点 需要用这一次的输出argmax之后 作为下一次的输入 因为输入必须得是long\n",
    "        dec_x=torch.argmax(output,dim=-1)\n",
    "\n",
    "        output_list.append(dec_x.squeeze(0).squeeze(0))\n",
    "        if tgt_vocab['<eos>']==torch.argmax(output,dim=-1).squeeze(0):\n",
    "            break\n",
    "    print([tgt_vocab.idx_to_token[i] for i in output_list])"
   ],
   "id": "117f5de5b833cc4b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['va', '!', '<eos>']\n",
      "[\"j'ai\", 'perdu', '.', '<eos>']\n",
      "['je', 'unk', '.', '<eos>']\n",
      "['je', 'suis', 'chez', 'moi', '.', '<eos>']\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "46c437eb62e82ac1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "987f7d7e19b8536c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
