{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-12T08:50:47.399355Z",
     "start_time": "2025-05-12T08:50:47.395190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ],
   "id": "51e95ac99dbb558b",
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T09:40:47.255672Z",
     "start_time": "2025-05-12T09:40:47.245232Z"
    }
   },
   "source": [
    "import torch \n",
    "from torch import nn \n",
    " \n",
    "class AttentionUpdateGateGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (Wu|Wr|Wn)\n",
    "        self.weight_ih = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, input_size))\n",
    "        # (Uu|Ur|Un)\n",
    "        self.weight_hh = nn.Parameter(\n",
    "            torch.Tensor(3 * hidden_size, hidden_size))\n",
    "        if bias:\n",
    "            # (b_iu|b_ir|b_in)\n",
    "            self.bias_ih = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "            # (b_hu|b_hr|b_hn)\n",
    "            self.bias_hh = nn.Parameter(torch.Tensor(3 * hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_ih', None)\n",
    "            self.register_parameter('bias_hh', None)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / (self.hidden_size)**0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "            \n",
    "    def forward(self, x, hx, att_score):\n",
    "        gi = F.linear(x, self.weight_ih, self.bias_ih)\n",
    "        gh = F.linear(hx, self.weight_hh, self.bias_hh)\n",
    "        i_r, i_u, i_n = gi.chunk(3, 1)\n",
    "        h_r, h_u, h_n = gh.chunk(3, 1)\n",
    " \n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        updategate = torch.sigmoid(i_u + h_u)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    " \n",
    "        updategate = att_score.view(-1, 1) * updategate\n",
    "        hy = (1-updategate)*hx +  updategate*newgate\n",
    " \n",
    "        return hy"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class AttentionUpdateGateGRUCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, bias=True):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bias = bias\n",
    "        # (Wxr|Wxz|Wxh)\n",
    "        self.weight_xrzh = nn.Parameter(\n",
    "            torch.ones(3 * hidden_size, input_size,dtype=torch.float32))\n",
    "        # (Hxr|Hxz|Hxh)\n",
    "        self.weight_hrzh = nn.Parameter(\n",
    "            torch.ones(3 * hidden_size, hidden_size,dtype=torch.float32))\n",
    "        if bias:\n",
    "            # (b)\n",
    "            self.bias_r = nn.Parameter(torch.zero(hidden_size))\n",
    "            self.bias_z = nn.Parameter(torch.zero(hidden_size))\n",
    "            self.bias_h = nn.Parameter(torch.zero(hidden_size))\n",
    "        else:\n",
    "            self.register_parameter('bias_rzh', None)\n",
    "        self.reset_parameters()\n",
    " \n",
    "    def reset_parameters(self):\n",
    "        # 常用初始化策略保证var(w)=1/hidden_size\n",
    "        stdv = 1.0 / self.hidden_size ** 0.5\n",
    "        for weight in self.parameters():\n",
    "            nn.init.uniform_(weight, -stdv, stdv)\n",
    "\n",
    "\n",
    "    def forward(self, x, hx, att_score):\n",
    "        X_r,X_z,X_h = F.linear(x, self.weight_xrzh).chunk(3, 1)\n",
    "        H_r,H_z,H_h = F.linear(hx, self.weight_hrzh).chunk(3, 1)\n",
    "\n",
    " \n",
    "        reset_gate = torch.sigmoid(X_r + H_r+self.bias_r)\n",
    "        update_gate = torch.sigmoid(X_z + H_z+self.bias_z)\n",
    "        hidden_gate_pre = torch.tanh(X_h + resetgate * h_n)\n",
    " \n",
    "        updategate = att_score.view(-1, 1) * updategate\n",
    "        hy = (1-updategate)*hx +  updategate*newgate\n",
    " \n",
    "        return hy"
   ],
   "id": "8035598f8f0b4871"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
