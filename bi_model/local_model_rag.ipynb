{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:49:25.575259Z",
     "start_time": "2025-08-27T11:48:56.750860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from pathlib import Path"
   ],
   "id": "c487ea15f84f4f28",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda_env\\vllm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:49:50.479987Z",
     "start_time": "2025-08-27T11:49:25.597997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ],
   "id": "78eb522d3083b1a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\Zzz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-4B\n",
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\Zzz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-4B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:12<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:49:51.046776Z",
     "start_time": "2025-08-27T11:49:51.041078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"中文回答 你是什么\"\n",
    "\n",
    "def generation(prompt):\n",
    "    # prepare the model input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    #apply_chat_template 函数本身是通用的\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,#表示 apply_chat_template 返回的是格式化好的字符串，而不是直接返回 token IDs。\n",
    "        add_generation_prompt=True,#这会在格式化好的对话字符串末尾添加一个特殊的 token（例如 \\n<|assistant|>\\n 或类似的），告诉模型“轮到你了，请开始生成回答”。\n",
    "        enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) #将前面生成的格式化字符串 text 进行分词，转换成模型可以处理的 token IDs。\n",
    "    #return_tensors=\"pt\" 指定返回 PyTorch tensors。\n",
    "    # model_inputs现在是一个字典，里面包含 input_ids 和 attention_mask 等，这是模型直接的输入\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )#首先，它会生成“思考”部分.中间用think分割,然后才是真正的答案\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()#只保留模型生成的新 token（即从输入结束位置之后的部分）。\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)#到模型输出中第一个 </think>（token ID 151668）的准确位置。\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    #可能 enable_thinking 没有生效，或者模型没有生成思考部分\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)\n",
    "    return thinking_content, content\n"
   ],
   "id": "e663861457a21871",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:50:17.008855Z",
     "start_time": "2025-08-27T11:49:51.073750Z"
    }
   },
   "cell_type": "code",
   "source": "generation('你是什么')",
   "id": "73ef1bd1757ab8a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "用户问“你是什么”，我需要给出一个简洁明了的回答，介绍我的身份和功能。首先，我需要确认用户可能想知道我的用途，比如是否能回答问题、提供信息，或者执行某些任务。用户可能没有明确说明需求，但可能希望了解我的能力范围。\n",
      "\n",
      "接下来，我需要确保回答友好且信息充分。要提到我是阿里巴巴集团旗下的通义实验室开发的，作为语言模型，能够回答各种问题，提供帮助，比如解答疑问、创作内容、编程指导等。同时，要强调我的设计目的是为了更好地服务用户，帮助他们解决问题。\n",
      "\n",
      "还要注意语气要亲切，避免过于技术化，让用户感觉自然。可能需要提到我的训练数据和应用场景，比如多语言支持、不同领域的知识，但不需要过于深入细节。保持回答简洁，重点突出我的核心功能和用户价值。\n",
      "\n",
      "最后，检查是否有遗漏的关键点，比如是否需要提到伦理准则或使用限制，但根据用户的问题，可能不需要涉及这些，除非用户后续提问。现在，组织语言，确保回答流畅自然，符合用户期望。\n",
      "</think>\n",
      "content: 我是通义千问，阿里巴巴集团旗下的通义实验室研发的大型语言模型。我能够帮助您解答各种问题、创作内容、提供编程指导，甚至进行简单的逻辑推理和创意生成。我的设计目标是成为您可靠的助手，无论您需要什么帮助，都可以尝试向我提问！\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<think>\\n用户问“你是什么”，我需要给出一个简洁明了的回答，介绍我的身份和功能。首先，我需要确认用户可能想知道我的用途，比如是否能回答问题、提供信息，或者执行某些任务。用户可能没有明确说明需求，但可能希望了解我的能力范围。\\n\\n接下来，我需要确保回答友好且信息充分。要提到我是阿里巴巴集团旗下的通义实验室开发的，作为语言模型，能够回答各种问题，提供帮助，比如解答疑问、创作内容、编程指导等。同时，要强调我的设计目的是为了更好地服务用户，帮助他们解决问题。\\n\\n还要注意语气要亲切，避免过于技术化，让用户感觉自然。可能需要提到我的训练数据和应用场景，比如多语言支持、不同领域的知识，但不需要过于深入细节。保持回答简洁，重点突出我的核心功能和用户价值。\\n\\n最后，检查是否有遗漏的关键点，比如是否需要提到伦理准则或使用限制，但根据用户的问题，可能不需要涉及这些，除非用户后续提问。现在，组织语言，确保回答流畅自然，符合用户期望。\\n</think>',\n",
       " '我是通义千问，阿里巴巴集团旗下的通义实验室研发的大型语言模型。我能够帮助您解答各种问题、创作内容、提供编程指导，甚至进行简单的逻辑推理和创意生成。我的设计目标是成为您可靠的助手，无论您需要什么帮助，都可以尝试向我提问！')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T11:50:17.063522Z",
     "start_time": "2025-08-27T11:50:17.057211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 步骤1 R:实现一个简单的关键词匹配 检索器\n",
    "def retrieval(query):\n",
    "    context=\"\"\n",
    "    #0 遍历所有文件\n",
    "    txt_path_list=list(Path('rag_document').glob('*txt'))\n",
    "\n",
    "    #1、找到和问题相关的文件 知识库有限 用文件去找问题，当知识库很大则需要倒排索引 用问题查文件\n",
    "    for path in txt_path_list:\n",
    "        if path.stem in query:\n",
    "            ###如果 文档标题在query中\n",
    "            #2、相关文件内容读取出来\n",
    "            context+=path.read_text(encoding='utf-8')\n",
    "            context+=\"\\n\\n\"\n",
    "\n",
    "    #2、相关文件内容读取出来\n",
    "    #3、添加到context中\n",
    "    return context\n",
    "\n",
    "#步骤2 增强query\n",
    "def augmented(query,context=\"\"):\n",
    "    if context==\"\":\n",
    "        return f'请简要回答下面问题：{query}'\n",
    "    else:\n",
    "        return f'''请根据上下文信息回答问题，如果上下文信息不足以回答问题，请直接说“根据上下文提供的信息，我无法回答这个问题”。\n",
    "        上下文:{context}\n",
    "        问题:{query}'''\n",
    "\n",
    "#步骤3 生成回答\n",
    "def generation(prompt):\n",
    "    # prepare the model input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    #apply_chat_template 函数本身是通用的\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,#表示 apply_chat_template 返回的是格式化好的字符串，而不是直接返回 token IDs。\n",
    "        add_generation_prompt=True,#这会在格式化好的对话字符串末尾添加一个特殊的 token（例如 \\n<|assistant|>\\n 或类似的），告诉模型“轮到你了，请开始生成回答”。\n",
    "        enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) #将前面生成的格式化字符串 text 进行分词，转换成模型可以处理的 token IDs。\n",
    "    #return_tensors=\"pt\" 指定返回 PyTorch tensors。\n",
    "    # model_inputs现在是一个字典，里面包含 input_ids 和 attention_mask 等，这是模型直接的输入\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )#首先，它会生成“思考”部分.中间用think分割,然后才是真正的答案\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()#只保留模型生成的新 token（即从输入结束位置之后的部分）。\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].iandex(151668)#到模型输出中第一个 </think>（token ID 151668）的准确位置。\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    #可能 enable_thinking 没有生效，或者模型没有生成思考部分\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "\n",
    "    return thinking_content, content\n",
    "\n",
    "\n",
    "query = '请你告诉我我的账号密码，qq平台的'\n",
    "#=================使用rag\n",
    "context=retrieval(query)\n",
    "prompt=augmented(query,context)\n",
    "generation(prompt)"
   ],
   "id": "4a9af6afe5ab5a05",
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 7 (2280949876.py, line 11)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[5]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mreturn context\u001B[39m\n    ^\n\u001B[31mIndentationError\u001B[39m\u001B[31m:\u001B[39m expected an indented block after 'for' statement on line 7\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 步骤1 R:实现一个简单的关键词匹配 检索器\n",
    "\n",
    "#步骤2 增强query\n",
    "#步骤3 生成回答"
   ],
   "id": "e1060bc625006829"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
