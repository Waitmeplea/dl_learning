{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:48:52.349908Z",
     "start_time": "2025-08-26T15:48:52.346126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "from pathlib import Path"
   ],
   "id": "c487ea15f84f4f28",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-25T13:02:15.742099Z",
     "start_time": "2025-08-25T13:02:15.738598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ],
   "id": "78eb522d3083b1a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:38:31.683078Z",
     "start_time": "2025-08-26T15:38:31.674377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"中文回答 你是什么\"\n",
    "\n",
    "def generation(prompt):\n",
    "    # prepare the model input\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    #apply_chat_template 函数本身是通用的\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,#表示 apply_chat_template 返回的是格式化好的字符串，而不是直接返回 token IDs。\n",
    "        add_generation_prompt=True,#这会在格式化好的对话字符串末尾添加一个特殊的 token（例如 \\n<|assistant|>\\n 或类似的），告诉模型“轮到你了，请开始生成回答”。\n",
    "        enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) #将前面生成的格式化字符串 text 进行分词，转换成模型可以处理的 token IDs。\n",
    "    #return_tensors=\"pt\" 指定返回 PyTorch tensors。\n",
    "    # model_inputs现在是一个字典，里面包含 input_ids 和 attention_mask 等，这是模型直接的输入\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=32768\n",
    "    )#首先，它会生成“思考”部分.中间用think分割,然后才是真正的答案\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()#只保留模型生成的新 token（即从输入结束位置之后的部分）。\n",
    "    # parsing thinking content\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)#到模型输出中第一个 </think>（token ID 151668）的准确位置。\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "    #可能 enable_thinking 没有生效，或者模型没有生成思考部分\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "    print(\"thinking content:\", thinking_content)\n",
    "    print(\"content:\", content)\n",
    "    return thinking_content, content\n"
   ],
   "id": "e663861457a21871",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:40:38.822497Z",
     "start_time": "2025-08-26T15:40:21.392378Z"
    }
   },
   "cell_type": "code",
   "source": "generation('你是什么')",
   "id": "73ef1bd1757ab8a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "用户问“你是什么”，我需要解释我的身份和功能。首先，我应该明确自己是阿里巴巴集团旗下的通义实验室研发的通义千问，是一款大型语言模型。接下来，要说明我的主要功能，比如回答问题、创作内容、编程、创作故事等。然后，要强调我的设计目标是成为有用的助手，帮助用户解决问题。同时，要提到我的训练数据和能力范围，比如基于大量文本数据，但不涉及敏感信息。还要保持友好和开放的态度，鼓励用户提问。需要确保回答简洁明了，避免技术术语过多，让用户容易理解。最后，检查是否有遗漏的重要信息，比如版权和使用条款，但可能不需要详细说明，保持回答自然流畅。\n",
      "</think>\n",
      "content: 我是通义千问，阿里巴巴集团旗下的通义实验室研发的大型语言模型。我能够回答各种问题、创作内容、编程、写故事、写诗、写剧本等，旨在成为你的有用助手。我的训练数据基于大量文本，但不涉及任何敏感信息。你可以随时向我提问，我会尽力帮助你！😊\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<think>\\n用户问“你是什么”，我需要解释我的身份和功能。首先，我应该明确自己是阿里巴巴集团旗下的通义实验室研发的通义千问，是一款大型语言模型。接下来，要说明我的主要功能，比如回答问题、创作内容、编程、创作故事等。然后，要强调我的设计目标是成为有用的助手，帮助用户解决问题。同时，要提到我的训练数据和能力范围，比如基于大量文本数据，但不涉及敏感信息。还要保持友好和开放的态度，鼓励用户提问。需要确保回答简洁明了，避免技术术语过多，让用户容易理解。最后，检查是否有遗漏的重要信息，比如版权和使用条款，但可能不需要详细说明，保持回答自然流畅。\\n</think>',\n",
       " '我是通义千问，阿里巴巴集团旗下的通义实验室研发的大型语言模型。我能够回答各种问题、创作内容、编程、写故事、写诗、写剧本等，旨在成为你的有用助手。我的训练数据基于大量文本，但不涉及任何敏感信息。你可以随时向我提问，我会尽力帮助你！😊')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:49:32.807338Z",
     "start_time": "2025-08-26T15:49:32.801757Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e926ea1b907f7b50",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('rag_document/算法知识点.txt'), WindowsPath('rag_document/账号密码.txt')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:11:17.834502Z",
     "start_time": "2025-08-26T15:11:17.825411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrieval(query):\n",
    "    context=\"\"\n",
    "    #0 遍历所有文件\n",
    "    txt_path_list=list(Path('rag_document').glob('*txt'))\n",
    "\n",
    "    #1、找到和问题相关的文件 知识库有限 用文件去找问题，当知识库很大则需要倒排索引 用问题查文件\n",
    "    for path in txt_path_list:\n",
    "\n",
    "    #2、相关文件内容读取出来\n",
    "    #3、添加到context中\n",
    "    return context"
   ],
   "id": "4a9af6afe5ab5a05",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 872,\n",
       " 198,\n",
       " 104811,\n",
       " 102104,\n",
       " 220,\n",
       " 56568,\n",
       " 102021,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T15:11:22.122046Z",
     "start_time": "2025-08-26T15:11:22.113424Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(model_inputs['input_ids'].tolist()[0], skip_special_tokens=True)",
   "id": "a02b536822934ddc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\n中文回答 你是什么\\nassistant\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 步骤1 R:实现一个简单的关键词匹配 检索器\n",
    "\n",
    "#步骤2 增强query\n",
    "#步骤3 生成回答"
   ],
   "id": "e1060bc625006829"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
