{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-01T06:15:06.623664Z",
     "start_time": "2025-09-01T06:15:06.602701Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "import torch\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:15:14.896855Z",
     "start_time": "2025-08-31T13:15:14.893414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#X:batchsize*num*input_size\n",
    "#ht-1 :batchsize*num*hidden_size"
   ],
   "id": "e0399a8cd09d02fc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T13:15:15.129590Z",
     "start_time": "2025-08-31T13:15:14.907235Z"
    }
   },
   "cell_type": "code",
   "source": "nn.init.xavier_uniform_(torch.randn(2,3,4))",
   "id": "b13f74694490b82a",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mnn\u001B[49m.init.xavier_uniform_(torch.randn(\u001B[32m2\u001B[39m,\u001B[32m3\u001B[39m,\u001B[32m4\u001B[39m))\n",
      "\u001B[31mNameError\u001B[39m: name 'nn' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class GRUCell(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size):\n",
    "        super(GRUCell, self).__init__()\n",
    "\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "\n",
    "        #更新门参数\n",
    "        self.Wz_x=nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.Wz_h=nn.Parameter(torch.randn(hidden_size,hidden_size))\n",
    "        self.bz=nn.Parameter(torch.zeros(hidden_size))\n",
    "        # 重置门参数\n",
    "        self.Wr_x=nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.Wr_h=nn.Parameter(torch.randn(hidden_size,hidden_size))\n",
    "        self.br=nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "        # 候选隐状态参数\n",
    "        self.Wh_x=nn.Parameter(torch.randn(input_size,hidden_size))\n",
    "        self.Wh_h=nn.Parameter(torch.randn(hidden_size,hidden_size))\n",
    "        self.bh=nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,X,hidden_state_pre):\n",
    "        \"\"\"\n",
    "        输入 X batch_size*input_size, ht-1  batch_size*hidden_size\n",
    "        \"\"\"\n",
    "        # 1、计算更新门\n",
    "        z_gate=torch.sigmoid(torch.mm(X,self.Wz_x)+torch.mm(hidden_state_pre,self.Wz_h)+self.bz )\n",
    "        #2、计算重置门\n",
    "        r_gate=torch.sigmoid(torch.mm(X,self.Wr_x)+torch.mm(hidden_state_pre,self.Wr_h)+self.br )\n",
    "\n",
    "        # 3、计算候选隐状态\n",
    "        h_tmp=torch.tanh(torch.mm(X,self.Wh_x)+torch.mm((hidden_state_pre*r_gate),self.Wh_h)+self.bh )\n",
    "\n",
    "        # 4、计算当前步隐藏层\n",
    "\n",
    "        h=z_gate*hidden_state_pre+(1-z_gate)*h_tmp\n",
    "\n",
    "        return h\n"
   ],
   "id": "bf1fd49e7c6085f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# qkv q与k点积/d 再与v相乘\n",
    "\n",
    "## 缩放点积注意力\n",
    "\n",
    "# q batch_size*查询个数*d\n",
    "# k batch_size*键值对个数*d\n",
    "# v batch*键值对个数*hidden_size\n",
    "class DotAttention(nn.Module):\n",
    "    def __init__(self,dropout=0.1):\n",
    "        super(DotAttention, self).__init__()\n",
    "\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    def forward(self,q,k,v,mask):\n",
    "        k=k.transpose(-1,-2)\n",
    "        d_size=q.shape[-1]\n",
    "        attention_score=torch.bmm(q,k)/torch.sqrt(torch.tensor(d_size))\n",
    "        attention_score[~mask]=torch.tensor(-1e6)\n",
    "        attention_weights=torch.softmax(attention_score,dim=-1)\n",
    "        return torch.bmm(self.dropout(attention_weights),v)"
   ],
   "id": "44809aa028f26248",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 掩蔽softmax实现\n",
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X,valid_lens=None):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    注意：X是三维的 一般是 B Q K-V数量\n",
    "     掩码的目的在于 避免Q关注到无关的或者禁止的K-V\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    shape=X.shape\n",
    "    # valid_lens可以是1d或者2d 如果是1d则必须长为B 如果是2d则必须是B*Q\n",
    "    if valid_lens.dim() == 1:\n",
    "        # 如果是1d则扩展成2d 形状B Q\n",
    "        valid_lens = valid_lens.unsqueeze(1).repeat(1, shape[1])\n",
    "    # 因为sequence函数要求的X是2d valid_lens是1d\n",
    "    X=X.reshape(-1,shape[-1])\n",
    "    valid_lens=valid_lens.reshape(-1)\n",
    "    X_mask=sequence_mask(X, valid_lens,value=-1e6)\n",
    "    # 完事之后还要把输出的形状转为X原本的形状\n",
    "    return F.softmax(X_mask, dim=-1).reshape(shape)"
   ],
   "id": "ef47f2bad1a57b10"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#@save\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量，valid_lens:1D或2D张量\n",
    "\n",
    "    shape = X.shape\n",
    "\n",
    "    valid_lens = valid_lens.reshape(-1)\n",
    "    # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmaxb输出为0\n",
    "    X = d2l.sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                          value=-1e6)\n",
    "    return nn.functional.softmax(X.reshape(shape), dim=-1)"
   ],
   "id": "99f41b4fef6e3a82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "# 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))"
   ],
   "id": "aaf662360523d0c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:24:55.971869Z",
     "start_time": "2025-08-31T14:24:55.968364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#@save\n",
    "def masked_softmax(X, valid_lens):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量 b,query_num,key_num ，valid_lens:2D张量 b,query_num\n",
    "    x_shape=X.shape\n",
    "    X=X.reshape(-1,x_shape[-1]) # b*query_num,key_num\n",
    "    max_len=X.shape[-1]\n",
    "    valid_lens=valid_lens.reshape(-1)# b*query_num\n",
    "    mask=torch.unsqueeze(torch.arange(0,max_len,dtype=torch.long),dim=0)# 1,key_num\n",
    "\n",
    "    #此时mask b*query_num key_num\n",
    "    mask=(mask<torch.unsqueeze(valid_lens,-1))\n",
    "    X[~mask]=-1e6\n",
    "\n",
    "    return F.softmax(X, dim=-1).reshape(x_shape)\n",
    "    # mask = (mask<valid_len.unsqueeze(1))\n"
   ],
   "id": "df2bb6deab8b2984",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:24:56.375519Z",
     "start_time": "2025-08-31T14:24:56.372949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X=torch.zeros(1,3,4)\n",
    "valid_lens=torch.tensor([[1,2,3]])"
   ],
   "id": "319b85425641e69a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:24:56.660090Z",
     "start_time": "2025-08-31T14:24:56.655689Z"
    }
   },
   "cell_type": "code",
   "source": "masked_softmax(X,valid_lens)",
   "id": "6c311be586ad4f10",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       0., -1000000., -1000000., -1000000.],\n",
       "        [       0.,        0., -1000000., -1000000.],\n",
       "        [       0.,        0.,        0., -1000000.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:25:53.573466Z",
     "start_time": "2025-08-31T14:25:53.568539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1))\n",
    "    X[~mask] = value\n",
    "    return X"
   ],
   "id": "d3b7d153d4cf8470",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-31T14:25:53.999586Z",
     "start_time": "2025-08-31T14:25:53.969516Z"
    }
   },
   "cell_type": "code",
   "source": "sequence_mask(X,valid_lens)",
   "id": "f49641a313f91098",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expect 2d tensor",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43msequence_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvalid_lens\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[25]\u001B[39m\u001B[32m, line 21\u001B[39m, in \u001B[36msequence_mask\u001B[39m\u001B[34m(X, valid_len, value)\u001B[39m\n\u001B[32m     17\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\u001B[39;00m\n\u001B[32m     18\u001B[39m \u001B[33;03m   意味着 第i个batch_size的序列有效长度为 valid_len[i]\u001B[39;00m\n\u001B[32m     19\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m     20\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m X.dim() != \u001B[32m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m valid_len.dim() != \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m21\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m'\u001B[39m\u001B[33mExpect 2d tensor\u001B[39m\u001B[33m'\u001B[39m)\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# 获取num_steps长度\u001B[39;00m\n\u001B[32m     23\u001B[39m maxlen = X.shape[\u001B[32m1\u001B[39m]\n",
      "\u001B[31mValueError\u001B[39m: Expect 2d tensor"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T06:57:45.391016Z",
     "start_time": "2025-09-01T06:57:45.366717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,q_size,k_size,v_size,hidden_size,num_heads,dropout=0.1):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        \n",
    "        self.num_heads=num_heads\n",
    "        self.W_q=nn.Linear(q_size,hidden_size,bias=False)\n",
    "        self.W_k=nn.Linear(k_size,hidden_size,bias=False)\n",
    "        self.W_v=nn.Linear(v_size,hidden_size,bias=False)\n",
    "        \n",
    "        self.W_o=nn.Linear(hidden_size,hidden_size,bias=False)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\t\t\n",
    "    def forward(self,q,k,v,mask):\n",
    "        \"\"\"\n",
    "        mask与q shape相等\n",
    "        \"\"\"\n",
    "        query=self.W_q(q)\n",
    "        key=self.W_q(k)\n",
    "        value=self.W_q(v)\n",
    "        \n",
    "        q_shape=query.shape\n",
    "        k_shape=key.shape\n",
    "        \n",
    "        query=query.reshape(q_shape[0],q_shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        key=key.reshape(k_shape[0],k_shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        value=value.reshape(k_shape[0],k_shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        \n",
    "        query_m=query.reshape(-1,query.shape[2],query.shape[3])\n",
    "        key_m=key.reshape(-1,key.shape[2],key.shape[3])\n",
    "        value_m=value.reshape(-1,value.shape[2],value.shape[3])\n",
    "        \n",
    "        ############缩放点积注意力###############\n",
    "        d=query_m.shape[-1]\n",
    "        \n",
    "        attention_score=torch.bmm(query_m,key_m.permute(0,2,1))/math.sqrt(d)\n",
    "        \n",
    "        if mask:\n",
    "            mask=mask.repeat_interval(self.num_heads,dim=0)\n",
    "        \n",
    "            attention_score[~mask]=torch.tensor(-1e9)\n",
    "        \n",
    "        attention_weight=torch.nn.functional.softmax(attention_score,dim=-1)## b*num_heads,q_num,k_num\n",
    "        \n",
    "        attention_result=torch.bmm(self.dropout(attention_weight),value_m)\n",
    "        \n",
    "        attention_result=attention_result.reshape(q_shape[0],self.num_heads,q_shape[1],-1).permute(0,2,1,3).reshape(q_shape[0],q_shape[1],-1)\n",
    "        \n",
    "        return self.W_o(attention_result)"
   ],
   "id": "9c52e4a4d31caef1",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T06:57:45.864813Z",
     "start_time": "2025-09-01T06:57:45.851459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_hiddens, num_heads = 100, 5\n",
    "attention = MultiHeadAttention(num_hiddens, num_hiddens, num_hiddens,\n",
    "                               num_hiddens, num_heads, 0.5)\n",
    "attention.eval()"
   ],
   "id": "f61eabbd20607cce",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T06:57:46.269571Z",
     "start_time": "2025-09-01T06:57:46.254090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y,mask=None).shape"
   ],
   "id": "d07e02b3d5d8b524",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bcf099fe50841f59"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
