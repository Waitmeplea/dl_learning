{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T16:16:10.807600Z",
     "start_time": "2025-08-24T16:16:10.801437Z"
    }
   },
   "cell_type": "code",
   "source": "import sys",
   "id": "78eb522d3083b1a4",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T16:20:27.070236Z",
     "start_time": "2025-08-24T16:20:20.505956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen3-4B\"\n",
    "\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")"
   ],
   "id": "1e636daef600ef4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\Zzz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-4B\n",
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\Zzz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-4B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T16:21:32.877816Z",
     "start_time": "2025-08-24T16:21:31.357873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ],
   "id": "2ba70b23ce1d0d30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: C:\\Users\\Zzz\\.cache\\modelscope\\hub\\models\\Qwen\\Qwen3-4B\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T16:21:35.075397Z",
     "start_time": "2025-08-24T16:21:35.009078Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bcfbccce49803290",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-24T16:23:04.268488Z",
     "start_time": "2025-08-24T16:23:03.234283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 准备输入文本\n",
    "input_text = \"这是一个用来测试的句子。\"\n",
    "\n",
    "inputs = tokenizer(input_text, padding=True, return_tensors='pt')\n",
    "outputs = model(**inputs)\n",
    "last_hidden_state = outputs.last_hidden_state # [batch_size, sequence_length, hidden_size]\n",
    "attention_mask = inputs['attention_mask'] # [batch_size, sequence_length]"
   ],
   "id": "2cc7f520f35e715c",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mRuntimeError\u001B[39m                              Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[12]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m      2\u001B[39m last_hidden_state = outputs.last_hidden_state \u001B[38;5;66;03m# [batch_size, sequence_length, hidden_size]\u001B[39;00m\n\u001B[32m      3\u001B[39m attention_mask = inputs[\u001B[33m'\u001B[39m\u001B[33mattention_mask\u001B[39m\u001B[33m'\u001B[39m] \u001B[38;5;66;03m# [batch_size, sequence_length]\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\transformers\\utils\\generic.py:959\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    957\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    958\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m959\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    960\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    961\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:481\u001B[39m, in \u001B[36mQwen3ForCausalLM.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001B[39m\n\u001B[32m    444\u001B[39m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[32m    445\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m    446\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m    457\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m    458\u001B[39m ) -> CausalLMOutputWithPast:\n\u001B[32m    459\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    460\u001B[39m \u001B[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[32m    461\u001B[39m \u001B[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    479\u001B[39m \u001B[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001B[39;00m\n\u001B[32m    480\u001B[39m \u001B[33;03m    ```\"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m481\u001B[39m     outputs: BaseModelOutputWithPast = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    482\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    483\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    484\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    485\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    486\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    487\u001B[39m \u001B[43m        \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    488\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    489\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    492\u001B[39m     hidden_states = outputs.last_hidden_state\n\u001B[32m    493\u001B[39m     \u001B[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\transformers\\utils\\generic.py:1083\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1080\u001B[39m                 module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n\u001B[32m   1081\u001B[39m                 monkey_patched_layers.append((module, original_forward))\n\u001B[32m-> \u001B[39m\u001B[32m1083\u001B[39m outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1084\u001B[39m \u001B[38;5;66;03m# Restore original forward methods\u001B[39;00m\n\u001B[32m   1085\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m module, original_forward \u001B[38;5;129;01min\u001B[39;00m monkey_patched_layers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\transformers\\models\\qwen3\\modeling_qwen3.py:366\u001B[39m, in \u001B[36mQwen3Model.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001B[39m\n\u001B[32m    363\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mYou must specify exactly one of input_ids or inputs_embeds\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    365\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m inputs_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m366\u001B[39m     inputs_embeds = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43membed_tokens\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;129;01mand\u001B[39;00m past_key_values \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    369\u001B[39m     past_key_values = DynamicCache()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1771\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1772\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1773\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1779\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1780\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1781\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1782\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1783\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1784\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1786\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1787\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:192\u001B[39m, in \u001B[36mEmbedding.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    191\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m192\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    193\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    194\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    195\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    196\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mmax_norm\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    197\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    198\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    199\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    200\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\conda_env\\vllm\\Lib\\site-packages\\torch\\nn\\functional.py:2546\u001B[39m, in \u001B[36membedding\u001B[39m\u001B[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001B[39m\n\u001B[32m   2540\u001B[39m     \u001B[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001B[39;00m\n\u001B[32m   2541\u001B[39m     \u001B[38;5;66;03m# XXX: equivalent to\u001B[39;00m\n\u001B[32m   2542\u001B[39m     \u001B[38;5;66;03m# with torch.no_grad():\u001B[39;00m\n\u001B[32m   2543\u001B[39m     \u001B[38;5;66;03m#   torch.embedding_renorm_\u001B[39;00m\n\u001B[32m   2544\u001B[39m     \u001B[38;5;66;03m# remove once script supports set_grad_enabled\u001B[39;00m\n\u001B[32m   2545\u001B[39m     _no_grad_embedding_renorm_(weight, \u001B[38;5;28minput\u001B[39m, max_norm, norm_type)\n\u001B[32m-> \u001B[39m\u001B[32m2546\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscale_grad_by_freq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msparse\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mRuntimeError\u001B[39m: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# 准备输入文本\n",
    "input_text = \"这是一个用来测试的句子。\"\n",
    "# 或者一个文本列表\n",
    "# input_text = [\"这是一个用来测试的句子。\", \"这是第二个句子。\"]\n",
    "\n",
    "# 调用 pipeline 获取 embedding\n",
    "# pipeline 的输出通常是一个包含 embedding 的列表，如果输入是列表的话\n",
    "results = text_embedding_pipeline(input_text)"
   ],
   "id": "ebc88105417e69bd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T01:27:45.947930Z",
     "start_time": "2025-08-23T01:27:20.326008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# prepare the model input\n",
    "prompt = \"中文回答 你是什么\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)# 返回pytorch tensor ，把文字输入转成模型要的输入\n",
    "\n",
    "# 开始生成\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() # len(model_inputs.input_ids[0]） 是输入的长度，要非输入以外的 则是模型的输出\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668) # 这里output_ids[::-1] 反过来就是为了找到最后一个think的标识 然后再拿长度减 得到最后一个think的真实index\n",
    "except ValueError:\n",
    "    index = 0 #没有说明没有think\n",
    "\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ],
   "id": "73ef1bd1757ab8a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "嗯，用户问“你是什么”，我需要回答这个问题。首先，我需要确定用户想了解什么。可能他们想知道我的身份、功能，或者我的用途。作为通义千问，我是一个大型语言模型，可以回答各种问题，创作文本，进行对话等。\n",
      "\n",
      "接下来，我需要考虑用户可能的背景。可能是普通用户，想了解我的能力；也可能是开发者，想了解我的技术细节。不过，问题比较简短，可能只是普通用户想确认我的身份。\n",
      "\n",
      "然后，我需要确保回答准确且简洁。需要说明我是阿里巴巴集团旗下的通义实验室研发的，用于回答问题、创作文本、编程、写作等。同时，可以提到我的特点，比如多语言支持、理解上下文等。\n",
      "\n",
      "还要注意用户可能的深层需求。比如，他们可能想确认我是否可靠，或者是否有特定功能。所以需要突出我的优势，比如广泛的知识、多语言支持、对话能力等。\n",
      "\n",
      "最后，检查是否有遗漏的信息，比如是否需要提到伦理准则或应用场景。但问题比较直接，可能不需要过多扩展。保持回答简洁明了，同时涵盖主要功能。\n",
      "</think>\n",
      "content: 我是通义千问，阿里巴巴集团旗下的通义实验室研发的大规模语言模型。我能够回答各种问题、创作文本、编程、写作、翻译、数据分析等，支持多语言交流。我的设计目标是提供准确、有用、自然的对话体验，同时遵循伦理准则和法律法规。如果你有任何问题或需要帮助，欢迎随时提问！\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T01:52:54.205989Z",
     "start_time": "2025-08-23T01:52:54.197815Z"
    }
   },
   "cell_type": "code",
   "source": "output_ids",
   "id": "c3cae78571050a0e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151667,\n",
       " 198,\n",
       " 106287,\n",
       " 3837,\n",
       " 20002,\n",
       " 56007,\n",
       " 2073,\n",
       " 56568,\n",
       " 102021,\n",
       " 33590,\n",
       " 35946,\n",
       " 85106,\n",
       " 102104,\n",
       " 105073,\n",
       " 1773,\n",
       " 101140,\n",
       " 3837,\n",
       " 35946,\n",
       " 85106,\n",
       " 60610,\n",
       " 20002,\n",
       " 99172,\n",
       " 99794,\n",
       " 99245,\n",
       " 1773,\n",
       " 87267,\n",
       " 99650,\n",
       " 109623,\n",
       " 97611,\n",
       " 101294,\n",
       " 5373,\n",
       " 98380,\n",
       " 3837,\n",
       " 100631,\n",
       " 97611,\n",
       " 105795,\n",
       " 1773,\n",
       " 100622,\n",
       " 31935,\n",
       " 64559,\n",
       " 99320,\n",
       " 56007,\n",
       " 3837,\n",
       " 35946,\n",
       " 101909,\n",
       " 101951,\n",
       " 102064,\n",
       " 104949,\n",
       " 3837,\n",
       " 73670,\n",
       " 102104,\n",
       " 100646,\n",
       " 86119,\n",
       " 3837,\n",
       " 104223,\n",
       " 108704,\n",
       " 3837,\n",
       " 71817,\n",
       " 105051,\n",
       " 49567,\n",
       " 3407,\n",
       " 104326,\n",
       " 3837,\n",
       " 35946,\n",
       " 85106,\n",
       " 101118,\n",
       " 20002,\n",
       " 87267,\n",
       " 9370,\n",
       " 102193,\n",
       " 1773,\n",
       " 104560,\n",
       " 100714,\n",
       " 20002,\n",
       " 3837,\n",
       " 99172,\n",
       " 99794,\n",
       " 97611,\n",
       " 99788,\n",
       " 24968,\n",
       " 74763,\n",
       " 104560,\n",
       " 113129,\n",
       " 3837,\n",
       " 99172,\n",
       " 99794,\n",
       " 97611,\n",
       " 99361,\n",
       " 104449,\n",
       " 1773,\n",
       " 100632,\n",
       " 3837,\n",
       " 86119,\n",
       " 99792,\n",
       " 98237,\n",
       " 99534,\n",
       " 3837,\n",
       " 87267,\n",
       " 100009,\n",
       " 100714,\n",
       " 20002,\n",
       " 99172,\n",
       " 81167,\n",
       " 97611,\n",
       " 101294,\n",
       " 3407,\n",
       " 101889,\n",
       " 3837,\n",
       " 35946,\n",
       " 85106,\n",
       " 103944,\n",
       " 102104,\n",
       " 102188,\n",
       " 100136,\n",
       " 110485,\n",
       " 1773,\n",
       " 85106,\n",
       " 66394,\n",
       " 104198,\n",
       " 107076,\n",
       " 100338,\n",
       " 111477,\n",
       " 31935,\n",
       " 64559,\n",
       " 104800,\n",
       " 100048,\n",
       " 9370,\n",
       " 3837,\n",
       " 100751,\n",
       " 102104,\n",
       " 86119,\n",
       " 5373,\n",
       " 104223,\n",
       " 108704,\n",
       " 5373,\n",
       " 110569,\n",
       " 5373,\n",
       " 105293,\n",
       " 49567,\n",
       " 1773,\n",
       " 91572,\n",
       " 3837,\n",
       " 73670,\n",
       " 104496,\n",
       " 97611,\n",
       " 100772,\n",
       " 3837,\n",
       " 101912,\n",
       " 42140,\n",
       " 102064,\n",
       " 100143,\n",
       " 5373,\n",
       " 101128,\n",
       " 102285,\n",
       " 16744,\n",
       " 49567,\n",
       " 3407,\n",
       " 104019,\n",
       " 60533,\n",
       " 20002,\n",
       " 87267,\n",
       " 9370,\n",
       " 114246,\n",
       " 100354,\n",
       " 1773,\n",
       " 101912,\n",
       " 3837,\n",
       " 99650,\n",
       " 87267,\n",
       " 99172,\n",
       " 81167,\n",
       " 35946,\n",
       " 64471,\n",
       " 101650,\n",
       " 3837,\n",
       " 100631,\n",
       " 107189,\n",
       " 105149,\n",
       " 98380,\n",
       " 1773,\n",
       " 99999,\n",
       " 85106,\n",
       " 102017,\n",
       " 97611,\n",
       " 100661,\n",
       " 3837,\n",
       " 101912,\n",
       " 100789,\n",
       " 107232,\n",
       " 5373,\n",
       " 42140,\n",
       " 102064,\n",
       " 100143,\n",
       " 5373,\n",
       " 105051,\n",
       " 99788,\n",
       " 49567,\n",
       " 3407,\n",
       " 100161,\n",
       " 3837,\n",
       " 101071,\n",
       " 107189,\n",
       " 114078,\n",
       " 105427,\n",
       " 3837,\n",
       " 101912,\n",
       " 64471,\n",
       " 85106,\n",
       " 104496,\n",
       " 112811,\n",
       " 108023,\n",
       " 57191,\n",
       " 116541,\n",
       " 1773,\n",
       " 77288,\n",
       " 86119,\n",
       " 99792,\n",
       " 101041,\n",
       " 3837,\n",
       " 87267,\n",
       " 104689,\n",
       " 106071,\n",
       " 106375,\n",
       " 1773,\n",
       " 100662,\n",
       " 102104,\n",
       " 110485,\n",
       " 30858,\n",
       " 34187,\n",
       " 3837,\n",
       " 91572,\n",
       " 102994,\n",
       " 99558,\n",
       " 98380,\n",
       " 8997,\n",
       " 151668,\n",
       " 271,\n",
       " 104198,\n",
       " 31935,\n",
       " 64559,\n",
       " 99320,\n",
       " 56007,\n",
       " 3837,\n",
       " 107076,\n",
       " 100338,\n",
       " 111477,\n",
       " 31935,\n",
       " 64559,\n",
       " 104800,\n",
       " 100048,\n",
       " 104197,\n",
       " 100176,\n",
       " 102064,\n",
       " 104949,\n",
       " 1773,\n",
       " 35946,\n",
       " 100006,\n",
       " 102104,\n",
       " 100646,\n",
       " 86119,\n",
       " 5373,\n",
       " 104223,\n",
       " 108704,\n",
       " 5373,\n",
       " 110569,\n",
       " 5373,\n",
       " 105293,\n",
       " 5373,\n",
       " 105395,\n",
       " 5373,\n",
       " 111540,\n",
       " 49567,\n",
       " 3837,\n",
       " 100143,\n",
       " 42140,\n",
       " 102064,\n",
       " 101069,\n",
       " 1773,\n",
       " 97611,\n",
       " 70500,\n",
       " 100160,\n",
       " 20412,\n",
       " 99553,\n",
       " 102188,\n",
       " 5373,\n",
       " 115404,\n",
       " 5373,\n",
       " 99795,\n",
       " 9370,\n",
       " 105051,\n",
       " 101904,\n",
       " 3837,\n",
       " 91572,\n",
       " 106466,\n",
       " 112811,\n",
       " 108023,\n",
       " 33108,\n",
       " 106221,\n",
       " 1773,\n",
       " 102056,\n",
       " 110117,\n",
       " 86119,\n",
       " 57191,\n",
       " 85106,\n",
       " 100364,\n",
       " 3837,\n",
       " 100437,\n",
       " 102422,\n",
       " 107666,\n",
       " 6313,\n",
       " 151645]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a02b536822934ddc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
