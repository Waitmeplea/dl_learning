{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.561383Z",
     "start_time": "2025-06-02T10:31:13.309820Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from dplearning_second_part.limu_dplearning.utils.useful_func import masked_softmax"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.589416Z",
     "start_time": "2025-06-02T10:31:18.577777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "# 从0实现一个Encoderblock\n",
    "#1、点积注意力\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1,**kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, valid_lens=None):\n",
    "        #q.shape[-1]是静态维度值（整数）将其包装为张量是冗余操作\n",
    "        # d_lens=torch.tensor(q.shape[-1],device=q.device)\n",
    "        d_lens=q.shape[-1]\n",
    "        #对于标量值，PyTorch会自动处理设备兼容性 所以不用显示todevice\n",
    "        attention_scores=torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_lens)\n",
    "        self.attention_weights=masked_softmax(attention_scores, valid_lens)\n",
    "        return torch.matmul(self.dropout(self.attention_weights),v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,hidden_size,num_heads,dropout=0.1,bias=False,**kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert hidden_size%num_heads==0,'整除条件不满足！'\n",
    "        # 三个调整size的 全连接\n",
    "        # 易错点 这里的全连接层都是没有偏置项 因为后续会有layer_normal 即使添加偏置项后续也会在减均值的过程中被吸收掉\n",
    "        #         一个更广义的规则：\n",
    "        # 如果一个线性层（或卷积层）的输出紧接着一个归一化层（Batch Norm, Layer Norm, Instance Norm, Group Norm），那么这个线性层/卷积层中的偏置项就是冗余的，通常会将其设置为 False。\n",
    "        self.W_q=nn.Linear(query_size,hidden_size,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,hidden_size,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,hidden_size,bias=bias)\n",
    "        # 最终输出用的全连接\n",
    "        self.W_o=nn.Linear(hidden_size,hidden_size,bias=bias)\n",
    "        # 注意力函数\n",
    "        self.attention=DotProductAttention(dropout=dropout)\n",
    "        # 头数\n",
    "        self.num_heads=num_heads\n",
    "        # 隐藏层数\n",
    "        self.hidden_size=hidden_size\n",
    "\n",
    "\n",
    "    def forward(self,q,k,v,valid_lens=None):\n",
    "        #调整qkv最后一层\n",
    "        # reshape出头数 并放在第二各维度 避免影响遮掩的softmax\n",
    "        # 错了一个地方 self.hidden_size/self.num_heads结果默认是浮点即使结果是整数 reshape无法接受浮点 因此要用//\n",
    "        # q_temp=self.W_q(q).reshape(q.shape[0],q.shape[1],self.num_heads,self.hidden_size/self.num_heads).permute(0,2,1,3)\n",
    "        q_temp=self.W_q(q).reshape(q.shape[0],q.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "        k_temp=self.W_k(k).reshape(k.shape[0],k.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "        v_temp=self.W_v(v).reshape(v.shape[0],v.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "\n",
    "        # 转为三维 将 1 2维度合并\n",
    "        q_temp=q_temp.reshape(-1,q.shape[1],self.hidden_size//self.num_heads)\n",
    "        k_temp=k_temp.reshape(-1,k.shape[1],self.hidden_size//self.num_heads)\n",
    "        v_temp=v_temp.reshape(-1,v.shape[1],self.hidden_size//self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "        # 这里很重要有一个知识点 看上面 其实是在batch_size 后增加了一个维度num_head 然后又reshape成batch_size*num_heads\n",
    "        # 这跟torch和numpy的存储方式有关系 contiguous (行主序)  当然也正是这种存储方式才使得我们要把num_heads 挪到第二维\n",
    "        # 由于每一个batch下增加的多个num_heads 其实都是归属在这个样本下的不同的注意力头的结果 对于这个样本其实他的valid_lens是不变的 也需要重复num_heads次\n",
    "        # 所以对于valid_lens 最简单的做法就是复制num_head次就行 所以使用repeat_interleave\n",
    "        # 当valid_lens 为2d明显要在batch_size维度进行复制，dim=0\n",
    "        # 当valid_lens为1维时，维度大小=batch_size 这跟我们实现的masked_softmax函数有关 显然也是在batch_size维度复制 所以无论valid_lens为多少维度 都是在dim=0维复制\n",
    "            valid_lens=valid_lens.repeat_interleave(self.num_heads,dim=0)\n",
    "\n",
    "\n",
    "        attention_result_total=self.attention(q_temp,k_temp,v_temp,valid_lens)\n",
    "        outputs=attention_result_total.reshape(q.shape[0],self.num_heads,q.shape[1],-1).permute(0,2,1,3).reshape(q.shape[0],q.shape[1],-1)\n",
    "        return self.W_o(outputs)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_len,hidden_size,dropout=0.1,**kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P=torch.zeros(1,max_len,hidden_size)\n",
    "        # 易错点这里建议不用除法， 直接 ：：2 否则少一个\n",
    "        self.temp=torch.arange(1,max_len+1).unsqueeze(1)/(torch.pow(10000,torch.arange(0,hidden_size,2)/hidden_size))\n",
    "        #1,2 用 1位置  如果一共只有3个 那就是 只有\n",
    "        self.P[:,:,0::2]=torch.sin(self.temp)\n",
    "        self.P[:,:,1::2]=torch.cos(self.temp)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 注意p和x在第二个维度不一定一样,device也不一定一样\n",
    "        x = x + self.P[:,:x.shape[1],:].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,norm_shape,dropout=0.1,**kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.norm=nn.LayerNorm(norm_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x,y):\n",
    "        return self.norm(x+self.dropout(y))\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffninput_size,ffnhidden_size,ffnoutput_size,**kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffninput_size,ffnhidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(ffnhidden_size,ffnoutput_size)\n",
    "    def forward(self,x):\n",
    "        x_temp = self.relu(self.dense1(x))\n",
    "        return self.dense2(x_temp)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,hidden_size,num_heads,norm_shape,ffninput_size,ffnhidden_size,dropout=0.1,bias=False,**kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # 位置编码 max=1000 hidden_size 和query的size一样 不是在块里完成的\n",
    "        # self.position_enc = PositionalEncoding(1000,query_size,dropout=dropout)\n",
    "        # 多头自注意力key_size,query_size,value_size,hidden_size这四个应该是全都相等\n",
    "        self.attention=MultiHeadAttention(key_size,query_size,value_size,hidden_size,num_heads,dropout=dropout,bias=bias)\n",
    "        #位置前馈 ffninput_size=ffnoutput_size=hidden_size\n",
    "        self.position_ffn=PositionWiseFFN(ffninput_size,ffnhidden_size,hidden_size,**kwargs)\n",
    "        # norm_shape = (l,hidden_size)\n",
    "        self.add_norm=AddNorm(norm_shape,dropout=dropout)\n",
    "\n",
    "    def forward(self,x_position,valid_lens=None):\n",
    "        y_attention=self.attention(x_position,x_position,x_position,valid_lens=valid_lens)\n",
    "        x_first=self.add_norm(x_position,y_attention)\n",
    "        return self.add_norm(x_first,self.position_ffn(x_first))\n"
   ],
   "id": "ec21eeaa92ac51a1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.608892Z",
     "start_time": "2025-06-02T10:31:18.593474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.ones((2,100,24))\n",
    "valid_lens=torch.tensor([3,2])"
   ],
   "id": "139505ce5e9fa36d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.837106Z",
     "start_time": "2025-06-02T10:31:18.787793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder_blk=EncoderBlock(key_size=24,query_size=24,value_size=24,hidden_size=24,num_heads=8,norm_shape=[100,24],ffninput_size=24,ffnhidden_size=48,dropout=0.5)\n",
    "encoder_blk.eval()\n",
    "encoder_blk(x,valid_lens)"
   ],
   "id": "d6476f69ae0c5b82",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         ...,\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600]],\n",
       "\n",
       "        [[ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         ...,\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600],\n",
       "         [ 0.0045,  1.3411, -0.2827,  ..., -1.1575,  1.0506,  0.0600]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.853279Z",
     "start_time": "2025-06-02T10:31:18.849803Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,key_size,query_size,value_size,hidden_size,num_head,norm_shape,\n",
    "                 num_layers,ffninput_size,ffnhidden_size,dropout=0.1,bias=False,*args):\n",
    "        super(TransformerEncoder, self).__init__(*args)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.position_embedding = PositionalEncoding(1000,hidden_size,dropout=dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f'{i}'+'blk'\n",
    "                                 ,EncoderBlock(hidden_size,hidden_size,hidden_size,hidden_size,num_head,norm_shape,ffninput_size,ffnhidden_size,dropout=dropout,bias=bias))\n",
    "    def forward(self,x,valid_lens=None):\n",
    "        x = self.embedding(x)\n",
    "        # torch.sqrt的输入必须是tensor\n",
    "        # 当一个 torch.Tensor 与一个 Python 标量进行算术运算（如加、减、乘、除）时，PyTorch 会自动将该标量广播 (broadcast) 到张量的所有元素上，并进行操作。\n",
    "        x_position=self.position_embedding(x*torch.sqrt(torch.tensor(self.hidden_size)))\n",
    "        self.attention_weights=[None]*len(self.blks)\n",
    "        # 易错点 这个地方不能这么写因为 X valid_lens是两个参数 sequential只支持一个参数传递\n",
    "        # return self.blks(x_position,valid_lens)\n",
    "        for num,module in enumerate(self.blks):\n",
    "            x_position=module(x_position,valid_lens=valid_lens)\n",
    "            self.attention_weights[num]=module.attention.attention.attention_weights\n",
    "        return x_position"
   ],
   "id": "ed5555a64640855d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.897992Z",
     "start_time": "2025-06-02T10:31:18.866931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24,2, [100, 24],4, 24, 48, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ],
   "id": "7dd3de3c5b063dca",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.908362Z",
     "start_time": "2025-06-02T10:31:18.904160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_tokens_and_segments(tokens_a,tokens_b=None):\n",
    "    tokens=['<cls>']+tokens_a+['<sep>']\n",
    "    segments=[0]*len(tokens)\n",
    "    if tokens_b is not None:\n",
    "        tokens=tokens+tokens_b+['<sep>']\n",
    "        segments=segments+[1]*(len(tokens_b)+1)\n",
    "    return tokens,segments\n",
    "get_tokens_and_segments(tokens_a=['a'])"
   ],
   "id": "84cd2dffcfaf1d86",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<cls>', 'a', '<sep>'], [0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.918997Z",
     "start_time": "2025-06-02T10:31:18.914972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    \"\"\"BERT编码器\"\"\"\n",
    "    def __init__(self,vocab_size,hidden_size,num_head,norm_shape,ffninput_size\n",
    "                 ,ffnhidden_size,num_layers,dropout=0.1,bias=False,max_lens=1000,key_size=768,query_size=768,value_size=768,**kwargs):\n",
    "        super(BERTEncoder, self).__init__(**kwargs)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size,hidden_size)\n",
    "        self.segment_embedding = nn.Embedding(2,hidden_size)\n",
    "        self.blks=nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f'{i}'+'blk',EncoderBlock(key_size,query_size,value_size\n",
    "                    ,hidden_size,num_head,norm_shape,ffninput_size,ffnhidden_size,dropout=dropout,bias=bias))\n",
    "        # 可学习的位置参数\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1,max_lens,hidden_size))\n",
    "    def forward(self,tokens,segments,valid_lens=None):\n",
    "        tokens,segments=self.token_embedding(tokens),self.segment_embedding(segments)\n",
    "        x=tokens+segments+self.position_embedding.repeat(tokens.shape[0],1,1)[:,:tokens.shape[1],:]\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            x=blk(x,valid_lens=valid_lens)\n",
    "        return x"
   ],
   "id": "57f16d3620c2a44c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:18.985870Z",
     "start_time": "2025-06-02T10:31:18.926872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab_size, hidden_size, ffnhidden_size, num_heads = 10000, 768, 1024, 4\n",
    "norm_shape, ffninput_size, num_layers, dropout = [768], 768, 2, 0.2\n",
    "encoder = BERTEncoder(vocab_size, hidden_size, num_heads, norm_shape, ffninput_size,\n",
    "                      ffnhidden_size, num_layers, dropout)"
   ],
   "id": "ca6b3a4d62f8c9d",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:19.007281Z",
     "start_time": "2025-06-02T10:31:18.992836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokens = torch.randint(0, vocab_size, (2, 8))\n",
    "segments = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 1], [0, 0, 0, 1, 1, 1, 1, 1]])\n",
    "encoded_X = encoder(tokens, segments, None)\n",
    "encoded_X.shape"
   ],
   "id": "ace233bd493de7cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:19.081841Z",
     "start_time": "2025-06-02T10:31:19.080404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MaskLM(nn.Module):\n",
    "    def __init__(self,vocab_size,hidden_size,inputs_size,**kwargs):\n",
    "        super(MaskLM, self).__init__(**kwargs)\n",
    "        self.mlp=nn.Sequential(nn.Linear(inputs_size,hidden_size),\n",
    "                               nn.ReLU(),\n",
    "                               nn.LayerNorm(hidden_size),\n",
    "                               nn.Linear(hidden_size,vocab_size),\n",
    "                               )\n",
    "    def forward(self, X, pred_positions):\n",
    "        num_pred_positions = pred_positions.shape[1]\n"
   ],
   "id": "e3ab485479ce9682",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T10:31:19.104451Z",
     "start_time": "2025-06-02T10:31:19.102894Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6bf5bf1cdfeb0ec2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "208cb9110a25469a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
