{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T01:11:01.388214Z",
     "start_time": "2025-05-21T01:11:01.381066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.useful_func import *\n",
    "from torch import optim\n",
    "\n",
    "# pack_padded_sequence打包需要mask的方法\n",
    "# import torch\n",
    "# from torch.nn.utils.rnn import pack_padded_sequence\n",
    "# \n",
    "# # 输入数据：批次大小 3，序列最大长度 5，词向量维度 2\n",
    "# padded_sequences = torch.randn(5, 3, 2)  # shape (seq_len, batch_size, input_size)\n",
    "# lengths = torch.tensor([5, 3, 2], dtype=torch.long)  # 实际长度（已降序排列）\n",
    "# \n",
    "# # 打包序列\n",
    "# packed = pack_padded_sequence(\n",
    "#     input=padded_sequences,\n",
    "#     lengths=lengths,\n",
    "#     batch_first=False,\n",
    "#     enforce_sorted=True\n",
    "# )"
   ],
   "id": "39d2e720c0ce90fb",
   "outputs": [],
   "execution_count": 312
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T02:01:24.429292Z",
     "start_time": "2025-05-21T02:01:17.859553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src, tgt = tokenize_nmt(preprocess_nmt(read_data_nmt()),num_examples=600)\n",
    "src_vocab = Vocal(src, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocal(tgt, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "src_data, src_valid = build_array_nmt(src, src_vocab, 10)\n",
    "tgt_data, tgt_valid = build_array_nmt(tgt, tgt_vocab, 10)\n",
    "dataset = torch.utils.data.TensorDataset(src_data, src_valid, tgt_data, tgt_valid)\n",
    "## 训练数据\n",
    "train_data = torch.utils.data.DataLoader(dataset=dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Encoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def forward(self, X, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Decoder, self).__init__()\n",
    "        pass\n",
    "\n",
    "    def init_state(self, enc_outputs, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, X, state, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, *args, **kwargs):\n",
    "        super(EncoderDecoder, self).__init__(*args, **kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, enc_x, dec_x, *args, **kwargs):\n",
    "        enc_outputs = self.encoder(enc_x, *args, **kwargs)\n",
    "        dec_state = self.decoder.init_state(enc_outputs, *args, **kwargs)\n",
    "        return self.decoder(dec_x, dec_state, *args, **kwargs)\n",
    "\n",
    "class Seq2SeqEncoder(Encoder):\n",
    "    \"\"\"输入一个x batch_size*num_steps或num_steps*batch_size \n",
    "    输出output batch_size num_steps \n",
    "    隐藏状态 numlayers * batch_size * hidden_size\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,num_layers, dropout=0.1, *args, **kwargs):\n",
    "        super(Seq2SeqEncoder, self).__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size, hidden_size,num_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, X, batchfirst=True, *args, **kwargs):\n",
    "        embed_x = self.embed(X)\n",
    "\n",
    "        outputs, state = self.rnn(embed_x)\n",
    "        return outputs, state\n",
    "\n",
    "class Seq2SeqDecoder(Decoder):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size,num_layers, dropout=0.1, *args, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(*args, **kwargs)\n",
    "        self.embed=nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size+hidden_size, hidden_size,num_layers, dropout=dropout, batch_first=True)\n",
    "        self.dense=nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, *args, **kwargs):\n",
    "        # outputs 形状为 batch_size,num_steps,embed_size\n",
    "        # state 形状为 num_layers batch_size hidden_size\n",
    "        state=enc_outputs[1]\n",
    "        return state\n",
    "\n",
    "    def forward(self, dec_x,state, *args, **kwargs):\n",
    "        # batch_size numteps emed_size\n",
    "        dec_x = self.embed(dec_x)\n",
    "        # state 为 batch_size hidden_size\n",
    "        context = state[-1].unsqueeze(1).repeat(1,dec_x.shape[1],1)\n",
    "        inputs=torch.cat([dec_x, context], dim=-1)\n",
    "        outputs, state = self.rnn(inputs,state)\n",
    "        seq=self.dense(outputs)\n",
    "\n",
    "        return seq, state\n"
   ],
   "id": "6bcef6a128f45953",
   "outputs": [],
   "execution_count": 360
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T02:39:26.460350Z",
     "start_time": "2025-05-21T02:39:26.450751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "encoder=Seq2SeqEncoder(vocab_size=src_vocab.__len__(),embed_size=32,hidden_size=32,num_layers=2)\n",
    "decoder=Seq2SeqDecoder(vocab_size=tgt_vocab.__len__(),embed_size=32,hidden_size=32,num_layers=2)\n",
    "net=EncoderDecoder(encoder,decoder)\n",
    "src_vocab.__len__(),tgt_vocab.__len__()\n",
    "\n",
    "# 初始化模型参数\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.GRU:\n",
    "        for name,param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    # 首先x是二维的 最内层维度是句子长度 注意：是训练集所以才知道句子真实长度\n",
    "    # 拿出总的长度 得到长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 然后用总长度生成一个1维的向量 使用函数扩展成2维以便与valid_len进行广播\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long), dim=0)\n",
    "    # mask在0维度扩充 valid在1维度扩充 因为每一个valid对应的是每一个x valid的数字其实是x的第二维向量\n",
    "    mask = (mask < torch.unsqueeze(valid_len, dim=1))  # 这里小于号就够了 因为<eos>所在位置的索引其实是valid_len-1\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "# 拓展的softmax因为对填充值进行softmax其实没有什么意义\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        # 交叉熵损失期望的两个输入 x是 batch_size vocab_size seq_lenth \n",
    "        # y 是 batch_size seq_len\n",
    "        unweight_loss = super().forward(pred, label)\n",
    "        weights_loss = unweight_loss * weights\n",
    "        return weights_loss.mean(dim=1)\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "\n"
   ],
   "id": "a1df2f5b5dd34aba",
   "outputs": [],
   "execution_count": 404
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T02:40:29.675771Z",
     "start_time": "2025-05-21T02:39:26.848944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "for epoch in range(300):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src,src_valid,tgt,tgt_valid=batch\n",
    "        Y=torch.cat((torch.tensor([tgt_vocab['<bos>']]).unsqueeze(0).repeat(tgt.shape[0],1),tgt),dim=1)[:,:-1]\n",
    "        # 易错点1：训练的时候应该使用带有bos的Y 表示强制教学 此时输出的y_hat实际上是没有bos的\n",
    "        y_hat,_=net(src,Y,src_valid)\n",
    "        # 点2 因为输出的y_hat 没有bos 因此在计算loss的时候应该使用原始序列作为目标序列\n",
    "        l=loss(y_hat,tgt,tgt_valid).sum()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "    print(l)"
   ],
   "id": "866057f72b1e6184",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(52.7925, grad_fn=<SumBackward0>)\n",
      "tensor(44.9963, grad_fn=<SumBackward0>)\n",
      "tensor(41.9487, grad_fn=<SumBackward0>)\n",
      "tensor(40.0511, grad_fn=<SumBackward0>)\n",
      "tensor(38.0886, grad_fn=<SumBackward0>)\n",
      "tensor(35.7034, grad_fn=<SumBackward0>)\n",
      "tensor(34.1993, grad_fn=<SumBackward0>)\n",
      "tensor(32.3679, grad_fn=<SumBackward0>)\n",
      "tensor(30.4093, grad_fn=<SumBackward0>)\n",
      "tensor(28.7496, grad_fn=<SumBackward0>)\n",
      "tensor(27.1362, grad_fn=<SumBackward0>)\n",
      "tensor(25.7173, grad_fn=<SumBackward0>)\n",
      "tensor(24.3811, grad_fn=<SumBackward0>)\n",
      "tensor(23.1102, grad_fn=<SumBackward0>)\n",
      "tensor(21.8070, grad_fn=<SumBackward0>)\n",
      "tensor(20.7296, grad_fn=<SumBackward0>)\n",
      "tensor(19.9570, grad_fn=<SumBackward0>)\n",
      "tensor(19.3106, grad_fn=<SumBackward0>)\n",
      "tensor(18.4051, grad_fn=<SumBackward0>)\n",
      "tensor(17.4071, grad_fn=<SumBackward0>)\n",
      "tensor(17.2090, grad_fn=<SumBackward0>)\n",
      "tensor(16.4874, grad_fn=<SumBackward0>)\n",
      "tensor(15.6214, grad_fn=<SumBackward0>)\n",
      "tensor(15.0747, grad_fn=<SumBackward0>)\n",
      "tensor(14.5523, grad_fn=<SumBackward0>)\n",
      "tensor(14.7569, grad_fn=<SumBackward0>)\n",
      "tensor(13.7731, grad_fn=<SumBackward0>)\n",
      "tensor(13.9662, grad_fn=<SumBackward0>)\n",
      "tensor(12.7871, grad_fn=<SumBackward0>)\n",
      "tensor(12.4463, grad_fn=<SumBackward0>)\n",
      "tensor(11.6398, grad_fn=<SumBackward0>)\n",
      "tensor(11.1005, grad_fn=<SumBackward0>)\n",
      "tensor(10.6023, grad_fn=<SumBackward0>)\n",
      "tensor(10.5521, grad_fn=<SumBackward0>)\n",
      "tensor(10.4094, grad_fn=<SumBackward0>)\n",
      "tensor(10.0233, grad_fn=<SumBackward0>)\n",
      "tensor(10.1157, grad_fn=<SumBackward0>)\n",
      "tensor(9.6256, grad_fn=<SumBackward0>)\n",
      "tensor(9.1565, grad_fn=<SumBackward0>)\n",
      "tensor(8.9866, grad_fn=<SumBackward0>)\n",
      "tensor(8.9862, grad_fn=<SumBackward0>)\n",
      "tensor(8.9775, grad_fn=<SumBackward0>)\n",
      "tensor(8.3446, grad_fn=<SumBackward0>)\n",
      "tensor(8.0062, grad_fn=<SumBackward0>)\n",
      "tensor(7.6127, grad_fn=<SumBackward0>)\n",
      "tensor(7.5019, grad_fn=<SumBackward0>)\n",
      "tensor(7.3901, grad_fn=<SumBackward0>)\n",
      "tensor(7.3668, grad_fn=<SumBackward0>)\n",
      "tensor(7.2833, grad_fn=<SumBackward0>)\n",
      "tensor(6.8545, grad_fn=<SumBackward0>)\n",
      "tensor(6.7581, grad_fn=<SumBackward0>)\n",
      "tensor(6.7977, grad_fn=<SumBackward0>)\n",
      "tensor(6.7496, grad_fn=<SumBackward0>)\n",
      "tensor(6.5743, grad_fn=<SumBackward0>)\n",
      "tensor(6.2820, grad_fn=<SumBackward0>)\n",
      "tensor(6.1347, grad_fn=<SumBackward0>)\n",
      "tensor(6.0312, grad_fn=<SumBackward0>)\n",
      "tensor(6.1650, grad_fn=<SumBackward0>)\n",
      "tensor(6.1014, grad_fn=<SumBackward0>)\n",
      "tensor(5.8727, grad_fn=<SumBackward0>)\n",
      "tensor(5.9283, grad_fn=<SumBackward0>)\n",
      "tensor(5.7378, grad_fn=<SumBackward0>)\n",
      "tensor(6.0250, grad_fn=<SumBackward0>)\n",
      "tensor(5.6173, grad_fn=<SumBackward0>)\n",
      "tensor(5.4602, grad_fn=<SumBackward0>)\n",
      "tensor(5.4736, grad_fn=<SumBackward0>)\n",
      "tensor(5.2256, grad_fn=<SumBackward0>)\n",
      "tensor(5.0134, grad_fn=<SumBackward0>)\n",
      "tensor(4.9974, grad_fn=<SumBackward0>)\n",
      "tensor(5.4862, grad_fn=<SumBackward0>)\n",
      "tensor(5.3433, grad_fn=<SumBackward0>)\n",
      "tensor(5.3615, grad_fn=<SumBackward0>)\n",
      "tensor(5.2032, grad_fn=<SumBackward0>)\n",
      "tensor(4.7881, grad_fn=<SumBackward0>)\n",
      "tensor(4.6090, grad_fn=<SumBackward0>)\n",
      "tensor(4.8114, grad_fn=<SumBackward0>)\n",
      "tensor(4.5278, grad_fn=<SumBackward0>)\n",
      "tensor(4.4131, grad_fn=<SumBackward0>)\n",
      "tensor(4.3249, grad_fn=<SumBackward0>)\n",
      "tensor(4.0327, grad_fn=<SumBackward0>)\n",
      "tensor(3.7239, grad_fn=<SumBackward0>)\n",
      "tensor(3.9817, grad_fn=<SumBackward0>)\n",
      "tensor(3.7855, grad_fn=<SumBackward0>)\n",
      "tensor(3.6497, grad_fn=<SumBackward0>)\n",
      "tensor(3.6315, grad_fn=<SumBackward0>)\n",
      "tensor(3.6201, grad_fn=<SumBackward0>)\n",
      "tensor(3.5152, grad_fn=<SumBackward0>)\n",
      "tensor(3.6791, grad_fn=<SumBackward0>)\n",
      "tensor(3.3786, grad_fn=<SumBackward0>)\n",
      "tensor(3.4697, grad_fn=<SumBackward0>)\n",
      "tensor(3.3763, grad_fn=<SumBackward0>)\n",
      "tensor(3.2206, grad_fn=<SumBackward0>)\n",
      "tensor(3.2781, grad_fn=<SumBackward0>)\n",
      "tensor(3.4686, grad_fn=<SumBackward0>)\n",
      "tensor(3.6741, grad_fn=<SumBackward0>)\n",
      "tensor(3.3206, grad_fn=<SumBackward0>)\n",
      "tensor(3.5924, grad_fn=<SumBackward0>)\n",
      "tensor(3.2442, grad_fn=<SumBackward0>)\n",
      "tensor(3.2486, grad_fn=<SumBackward0>)\n",
      "tensor(2.9229, grad_fn=<SumBackward0>)\n",
      "tensor(2.6914, grad_fn=<SumBackward0>)\n",
      "tensor(3.0342, grad_fn=<SumBackward0>)\n",
      "tensor(2.7757, grad_fn=<SumBackward0>)\n",
      "tensor(2.6628, grad_fn=<SumBackward0>)\n",
      "tensor(2.6228, grad_fn=<SumBackward0>)\n",
      "tensor(2.6199, grad_fn=<SumBackward0>)\n",
      "tensor(2.8894, grad_fn=<SumBackward0>)\n",
      "tensor(2.7984, grad_fn=<SumBackward0>)\n",
      "tensor(2.4737, grad_fn=<SumBackward0>)\n",
      "tensor(2.4853, grad_fn=<SumBackward0>)\n",
      "tensor(2.5123, grad_fn=<SumBackward0>)\n",
      "tensor(2.4354, grad_fn=<SumBackward0>)\n",
      "tensor(2.2868, grad_fn=<SumBackward0>)\n",
      "tensor(2.3341, grad_fn=<SumBackward0>)\n",
      "tensor(2.1755, grad_fn=<SumBackward0>)\n",
      "tensor(2.3299, grad_fn=<SumBackward0>)\n",
      "tensor(2.1521, grad_fn=<SumBackward0>)\n",
      "tensor(2.2259, grad_fn=<SumBackward0>)\n",
      "tensor(2.1012, grad_fn=<SumBackward0>)\n",
      "tensor(2.1821, grad_fn=<SumBackward0>)\n",
      "tensor(1.9857, grad_fn=<SumBackward0>)\n",
      "tensor(2.1727, grad_fn=<SumBackward0>)\n",
      "tensor(2.1016, grad_fn=<SumBackward0>)\n",
      "tensor(2.1525, grad_fn=<SumBackward0>)\n",
      "tensor(2.0733, grad_fn=<SumBackward0>)\n",
      "tensor(2.1184, grad_fn=<SumBackward0>)\n",
      "tensor(2.0418, grad_fn=<SumBackward0>)\n",
      "tensor(1.9618, grad_fn=<SumBackward0>)\n",
      "tensor(2.0650, grad_fn=<SumBackward0>)\n",
      "tensor(2.6535, grad_fn=<SumBackward0>)\n",
      "tensor(2.0971, grad_fn=<SumBackward0>)\n",
      "tensor(2.0084, grad_fn=<SumBackward0>)\n",
      "tensor(2.1487, grad_fn=<SumBackward0>)\n",
      "tensor(2.0284, grad_fn=<SumBackward0>)\n",
      "tensor(1.9718, grad_fn=<SumBackward0>)\n",
      "tensor(2.2593, grad_fn=<SumBackward0>)\n",
      "tensor(2.1176, grad_fn=<SumBackward0>)\n",
      "tensor(2.6031, grad_fn=<SumBackward0>)\n",
      "tensor(2.0475, grad_fn=<SumBackward0>)\n",
      "tensor(1.9130, grad_fn=<SumBackward0>)\n",
      "tensor(1.9065, grad_fn=<SumBackward0>)\n",
      "tensor(1.9420, grad_fn=<SumBackward0>)\n",
      "tensor(2.0057, grad_fn=<SumBackward0>)\n",
      "tensor(1.8479, grad_fn=<SumBackward0>)\n",
      "tensor(1.8072, grad_fn=<SumBackward0>)\n",
      "tensor(2.0145, grad_fn=<SumBackward0>)\n",
      "tensor(1.9000, grad_fn=<SumBackward0>)\n",
      "tensor(1.8268, grad_fn=<SumBackward0>)\n",
      "tensor(1.8041, grad_fn=<SumBackward0>)\n",
      "tensor(1.9149, grad_fn=<SumBackward0>)\n",
      "tensor(1.9182, grad_fn=<SumBackward0>)\n",
      "tensor(1.9199, grad_fn=<SumBackward0>)\n",
      "tensor(1.8215, grad_fn=<SumBackward0>)\n",
      "tensor(1.8285, grad_fn=<SumBackward0>)\n",
      "tensor(1.7802, grad_fn=<SumBackward0>)\n",
      "tensor(2.0597, grad_fn=<SumBackward0>)\n",
      "tensor(1.9597, grad_fn=<SumBackward0>)\n",
      "tensor(1.8945, grad_fn=<SumBackward0>)\n",
      "tensor(1.7975, grad_fn=<SumBackward0>)\n",
      "tensor(1.9320, grad_fn=<SumBackward0>)\n",
      "tensor(1.9785, grad_fn=<SumBackward0>)\n",
      "tensor(1.9093, grad_fn=<SumBackward0>)\n",
      "tensor(1.9145, grad_fn=<SumBackward0>)\n",
      "tensor(1.8639, grad_fn=<SumBackward0>)\n",
      "tensor(1.8427, grad_fn=<SumBackward0>)\n",
      "tensor(1.7772, grad_fn=<SumBackward0>)\n",
      "tensor(1.7941, grad_fn=<SumBackward0>)\n",
      "tensor(1.8349, grad_fn=<SumBackward0>)\n",
      "tensor(1.9412, grad_fn=<SumBackward0>)\n",
      "tensor(1.8598, grad_fn=<SumBackward0>)\n",
      "tensor(1.9568, grad_fn=<SumBackward0>)\n",
      "tensor(1.6774, grad_fn=<SumBackward0>)\n",
      "tensor(1.8030, grad_fn=<SumBackward0>)\n",
      "tensor(1.8378, grad_fn=<SumBackward0>)\n",
      "tensor(1.7412, grad_fn=<SumBackward0>)\n",
      "tensor(1.7403, grad_fn=<SumBackward0>)\n",
      "tensor(1.7009, grad_fn=<SumBackward0>)\n",
      "tensor(1.6219, grad_fn=<SumBackward0>)\n",
      "tensor(1.7849, grad_fn=<SumBackward0>)\n",
      "tensor(1.6396, grad_fn=<SumBackward0>)\n",
      "tensor(2.1279, grad_fn=<SumBackward0>)\n",
      "tensor(1.7032, grad_fn=<SumBackward0>)\n",
      "tensor(1.7815, grad_fn=<SumBackward0>)\n",
      "tensor(1.7844, grad_fn=<SumBackward0>)\n",
      "tensor(1.6244, grad_fn=<SumBackward0>)\n",
      "tensor(1.7694, grad_fn=<SumBackward0>)\n",
      "tensor(1.7306, grad_fn=<SumBackward0>)\n",
      "tensor(1.6893, grad_fn=<SumBackward0>)\n",
      "tensor(1.7088, grad_fn=<SumBackward0>)\n",
      "tensor(1.6081, grad_fn=<SumBackward0>)\n",
      "tensor(1.5823, grad_fn=<SumBackward0>)\n",
      "tensor(1.6481, grad_fn=<SumBackward0>)\n",
      "tensor(1.7214, grad_fn=<SumBackward0>)\n",
      "tensor(1.6756, grad_fn=<SumBackward0>)\n",
      "tensor(1.6080, grad_fn=<SumBackward0>)\n",
      "tensor(1.5024, grad_fn=<SumBackward0>)\n",
      "tensor(1.7303, grad_fn=<SumBackward0>)\n",
      "tensor(1.6582, grad_fn=<SumBackward0>)\n",
      "tensor(1.5760, grad_fn=<SumBackward0>)\n",
      "tensor(1.5847, grad_fn=<SumBackward0>)\n",
      "tensor(1.6652, grad_fn=<SumBackward0>)\n",
      "tensor(1.5757, grad_fn=<SumBackward0>)\n",
      "tensor(1.6925, grad_fn=<SumBackward0>)\n",
      "tensor(1.6131, grad_fn=<SumBackward0>)\n",
      "tensor(1.6467, grad_fn=<SumBackward0>)\n",
      "tensor(1.7362, grad_fn=<SumBackward0>)\n",
      "tensor(1.7016, grad_fn=<SumBackward0>)\n",
      "tensor(1.6649, grad_fn=<SumBackward0>)\n",
      "tensor(1.7291, grad_fn=<SumBackward0>)\n",
      "tensor(1.6383, grad_fn=<SumBackward0>)\n",
      "tensor(1.5031, grad_fn=<SumBackward0>)\n",
      "tensor(1.4635, grad_fn=<SumBackward0>)\n",
      "tensor(1.6857, grad_fn=<SumBackward0>)\n",
      "tensor(1.7415, grad_fn=<SumBackward0>)\n",
      "tensor(1.8153, grad_fn=<SumBackward0>)\n",
      "tensor(1.5946, grad_fn=<SumBackward0>)\n",
      "tensor(1.6684, grad_fn=<SumBackward0>)\n",
      "tensor(1.5150, grad_fn=<SumBackward0>)\n",
      "tensor(1.6043, grad_fn=<SumBackward0>)\n",
      "tensor(1.6650, grad_fn=<SumBackward0>)\n",
      "tensor(1.6667, grad_fn=<SumBackward0>)\n",
      "tensor(1.6506, grad_fn=<SumBackward0>)\n",
      "tensor(1.8592, grad_fn=<SumBackward0>)\n",
      "tensor(1.6241, grad_fn=<SumBackward0>)\n",
      "tensor(1.6164, grad_fn=<SumBackward0>)\n",
      "tensor(1.4159, grad_fn=<SumBackward0>)\n",
      "tensor(1.7091, grad_fn=<SumBackward0>)\n",
      "tensor(1.6258, grad_fn=<SumBackward0>)\n",
      "tensor(1.5923, grad_fn=<SumBackward0>)\n",
      "tensor(1.5873, grad_fn=<SumBackward0>)\n",
      "tensor(1.5512, grad_fn=<SumBackward0>)\n",
      "tensor(1.6730, grad_fn=<SumBackward0>)\n",
      "tensor(1.6478, grad_fn=<SumBackward0>)\n",
      "tensor(1.6324, grad_fn=<SumBackward0>)\n",
      "tensor(1.5534, grad_fn=<SumBackward0>)\n",
      "tensor(1.6129, grad_fn=<SumBackward0>)\n",
      "tensor(1.6714, grad_fn=<SumBackward0>)\n",
      "tensor(1.6609, grad_fn=<SumBackward0>)\n",
      "tensor(1.5802, grad_fn=<SumBackward0>)\n",
      "tensor(1.5356, grad_fn=<SumBackward0>)\n",
      "tensor(1.5337, grad_fn=<SumBackward0>)\n",
      "tensor(2.0986, grad_fn=<SumBackward0>)\n",
      "tensor(1.6547, grad_fn=<SumBackward0>)\n",
      "tensor(1.7441, grad_fn=<SumBackward0>)\n",
      "tensor(1.7716, grad_fn=<SumBackward0>)\n",
      "tensor(1.6779, grad_fn=<SumBackward0>)\n",
      "tensor(1.5641, grad_fn=<SumBackward0>)\n",
      "tensor(1.5999, grad_fn=<SumBackward0>)\n",
      "tensor(1.5477, grad_fn=<SumBackward0>)\n",
      "tensor(1.6939, grad_fn=<SumBackward0>)\n",
      "tensor(1.7687, grad_fn=<SumBackward0>)\n",
      "tensor(1.7537, grad_fn=<SumBackward0>)\n",
      "tensor(1.4750, grad_fn=<SumBackward0>)\n",
      "tensor(1.6386, grad_fn=<SumBackward0>)\n",
      "tensor(1.5390, grad_fn=<SumBackward0>)\n",
      "tensor(1.7601, grad_fn=<SumBackward0>)\n",
      "tensor(1.5953, grad_fn=<SumBackward0>)\n",
      "tensor(1.5246, grad_fn=<SumBackward0>)\n",
      "tensor(1.6388, grad_fn=<SumBackward0>)\n",
      "tensor(1.5659, grad_fn=<SumBackward0>)\n",
      "tensor(2.1525, grad_fn=<SumBackward0>)\n",
      "tensor(1.6931, grad_fn=<SumBackward0>)\n",
      "tensor(1.5206, grad_fn=<SumBackward0>)\n",
      "tensor(1.5693, grad_fn=<SumBackward0>)\n",
      "tensor(1.6995, grad_fn=<SumBackward0>)\n",
      "tensor(1.5675, grad_fn=<SumBackward0>)\n",
      "tensor(1.6884, grad_fn=<SumBackward0>)\n",
      "tensor(1.5978, grad_fn=<SumBackward0>)\n",
      "tensor(1.5819, grad_fn=<SumBackward0>)\n",
      "tensor(1.5425, grad_fn=<SumBackward0>)\n",
      "tensor(1.5705, grad_fn=<SumBackward0>)\n",
      "tensor(1.5929, grad_fn=<SumBackward0>)\n",
      "tensor(1.5878, grad_fn=<SumBackward0>)\n",
      "tensor(1.5815, grad_fn=<SumBackward0>)\n",
      "tensor(1.6400, grad_fn=<SumBackward0>)\n",
      "tensor(1.5086, grad_fn=<SumBackward0>)\n",
      "tensor(1.7558, grad_fn=<SumBackward0>)\n",
      "tensor(1.5609, grad_fn=<SumBackward0>)\n",
      "tensor(1.6722, grad_fn=<SumBackward0>)\n",
      "tensor(1.5613, grad_fn=<SumBackward0>)\n",
      "tensor(1.4468, grad_fn=<SumBackward0>)\n",
      "tensor(2.2836, grad_fn=<SumBackward0>)\n",
      "tensor(1.7624, grad_fn=<SumBackward0>)\n",
      "tensor(1.5542, grad_fn=<SumBackward0>)\n",
      "tensor(1.8267, grad_fn=<SumBackward0>)\n",
      "tensor(1.5932, grad_fn=<SumBackward0>)\n",
      "tensor(1.5181, grad_fn=<SumBackward0>)\n",
      "tensor(1.5208, grad_fn=<SumBackward0>)\n",
      "tensor(1.5310, grad_fn=<SumBackward0>)\n",
      "tensor(1.4576, grad_fn=<SumBackward0>)\n",
      "tensor(1.6052, grad_fn=<SumBackward0>)\n",
      "tensor(2.1946, grad_fn=<SumBackward0>)\n",
      "tensor(1.6230, grad_fn=<SumBackward0>)\n",
      "tensor(1.5543, grad_fn=<SumBackward0>)\n",
      "tensor(1.5064, grad_fn=<SumBackward0>)\n",
      "tensor(1.5584, grad_fn=<SumBackward0>)\n",
      "tensor(1.5247, grad_fn=<SumBackward0>)\n",
      "tensor(1.5397, grad_fn=<SumBackward0>)\n",
      "tensor(1.5700, grad_fn=<SumBackward0>)\n",
      "tensor(1.5251, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 405
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T02:42:08.813395Z",
     "start_time": "2025-05-21T02:42:08.778502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# predict\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for src_sentence,tgt_sentence in zip(engs,fras):\n",
    "    src_tokens=[src_vocab[i] for i in src_sentence.split(' ')]+[src_vocab['<eos>']]\n",
    "    src_data=truncate_pad(src_tokens,10,1)\n",
    "    # 易错点 srcdata在生成完成后需要unsqueeze 因为原本是1维的需要增加一个批次维度\n",
    "    src_data=torch.tensor(src_data).unsqueeze(0)\n",
    "    # 易错点 需要加上这个批次的有效长度建议1维 或者无维度\n",
    "    enc_valid_len=torch.tensor([len(src_tokens)])\n",
    "    \n",
    "    enc_outputs=net.encoder(src_data,enc_valid_len)\n",
    "    state=net.decoder.init_state(enc_outputs,enc_valid_len)\n",
    "    # 易错点 这里必须得是long 因为embed层需要long输入 并且要unsequeeze 增加一个维度\n",
    "    dec_x=torch.tensor([tgt_vocab['<bos>']],dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    output_list=[]\n",
    "    for i in range(10):\n",
    "        output,state=net.decoder(dec_x,state)\n",
    "        # 易错点 需要用这一次的输出argmax之后 作为下一次的输入 因为输入必须得是long\n",
    "        dec_x=torch.argmax(output,dim=-1)\n",
    "\n",
    "        output_list.append(dec_x.squeeze(0).squeeze(0))\n",
    "        if tgt_vocab['<eos>']==torch.argmax(output,dim=-1).squeeze(0):\n",
    "            break\n",
    "    print([tgt_vocab.idx_to_token[i] for i in output_list])"
   ],
   "id": "fb5862d746e5f693",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['va', 'au', 'unk', '!', '<eos>']\n",
      "[\"j'ai\", 'unk', '.', '<eos>']\n",
      "['il', 'est', 'riche', 'unk', '.', '<eos>']\n",
      "['je', 'suis', 'chez', 'bonne', 'aboient', '?', '?', 'aboient', 'aboient', 'aboient']\n"
     ]
    }
   ],
   "execution_count": 408
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6c6c4255dccb1bc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5e77f3fca5853ef2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7388edcde4f484b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
