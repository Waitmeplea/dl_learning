{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.504258Z",
     "start_time": "2025-05-28T01:05:24.458477Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T02:34:17.685407Z",
     "start_time": "2025-05-28T02:34:17.675601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 掩蔽softmax实现\n",
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X,valid_lens=None):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    注意：X是三维的 一般是 B Q K-V数量\n",
    "     掩码的目的在于 避免Q关注到无关的或者禁止的K-V\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    shape=X.shape\n",
    "    # valid_lens可以是1d或者2d 如果是1d则必须长为B 如果是2d则必须是B*Q\n",
    "    if valid_lens.dim() == 1:\n",
    "        # 如果是1d则扩展成2d 形状B Q \n",
    "        valid_lens = valid_lens.unsqueeze(1).repeat(1, shape[1])\n",
    "    # 因为sequence函数要求的X是2d valid_lens是1d\n",
    "    X=X.reshape(-1,shape[-1])\n",
    "    valid_lens=valid_lens.reshape(-1)\n",
    "    X_mask=sequence_mask(X, valid_lens,value=-1e6)\n",
    "    # 完事之后还要把输出的形状转为X原本的形状 \n",
    "    return F.softmax(X_mask, dim=-1).reshape(shape)"
   ],
   "id": "18e7b29624619a09",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.570376Z",
     "start_time": "2025-05-28T01:05:28.553008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建一个足够长的P 长是指第2个维度 第一个维度应该是batch_size\n",
    "        # 每一个batch用的是一套位置编码 如果不用一套的话就无法学习到位置信息泛化能力极差\n",
    "        self.P=torch.zeros(1,max_len,num_hiddens)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        X=torch.arange(max_len,dtype=torch.float32).reshape(-1,1)/\\\n",
    "            torch.pow(10000,torch.arange(0,num_hiddens,2)/num_hiddens)\n",
    "        self.P[:,:,0::2]=torch.sin(X)\n",
    "        self.P[:,:,1::2]=torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X+self.P[:,:X.shape[1],:].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "# 先实现一个点积注意力\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_weights = None\n",
    "    # q(b,step,embed_size)\n",
    "    # k(b,键值对个数,embed_size)\n",
    "    # v(b,键值对个数,embed_size)\n",
    "    def forward(self, q, k, v,valid_lens):\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "        self.attention_weights=masked_softmax(attn_weights,valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), v)\n",
    "\n",
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    # qkv各自的embed_size, 隐藏层大小 头数量\n",
    "    # 需要并行运算多个头 因此num_hiddens 必须能够整除以num_heads\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q=nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o=nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "\n",
    "    def forward(self, q, k, v,valid_lens=None):\n",
    "\n",
    "        queries=self.W_q(q)\n",
    "        keys=self.W_k(k)\n",
    "        values=self.W_v(v)\n",
    "\n",
    "        # 在这一步需要对qkv拆分为多头 并行计算attention\n",
    "        queries=queries.reshape(queries.shape[0],queries.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        keys=keys.reshape(keys.shape[0],keys.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        values=values.reshape(values.shape[0],values.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "\n",
    "        queries=queries.reshape(-1,queries.shape[2],queries.shape[3])\n",
    "        keys=keys.reshape(-1,keys.shape[2],keys.shape[3])\n",
    "        values=values.reshape(-1,values.shape[2],values.shape[3])\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        attn_weights=self.attention(queries,keys,values,valid_lens)\n",
    "        attn_weights=attn_weights.reshape(-1,self.num_heads,attn_weights.shape[1],attn_weights.shape[2])\n",
    "        attn_weights=attn_weights.permute(0,2,1,3)\n",
    "        attn_weights=attn_weights.reshape(attn_weights.shape[0],attn_weights.shape[1],-1)\n",
    "\n",
    "        return self.W_o(attn_weights)\n"
   ],
   "id": "d38ebef52929b469",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.576239Z",
     "start_time": "2025-05-28T01:05:28.570376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_outputs,**kwargs):\n",
    "        super(PositionWiseFFN,self).__init__(**kwargs)\n",
    "        self.dense1=nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(ffn_num_hiddens,ffn_num_outputs)\n",
    "    def forward(self,x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ],
   "id": "5564bf4d2e4464d8",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.590305Z",
     "start_time": "2025-05-28T01:05:28.576239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ffn=PositionWiseFFN(ffn_num_input=4,ffn_num_hiddens=4,ffn_num_outputs=8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2,3,4))).shape"
   ],
   "id": "1204f7b63ecc1632",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.594672Z",
     "start_time": "2025-05-28T01:05:28.590305Z"
    }
   },
   "cell_type": "code",
   "source": "# 残差连接和层规范化",
   "id": "fc174191f0bf2b8b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.601364Z",
     "start_time": "2025-05-28T01:05:28.594672Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)"
   ],
   "id": "7bd318b369af9d76",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.609593Z",
     "start_time": "2025-05-28T01:05:28.601364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.tensor([[1,2],[2,3]],dtype=torch.float32)\n",
    "x,x.shape"
   ],
   "id": "ad278bb0ac7ab58d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [2., 3.]]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.614972Z",
     "start_time": "2025-05-28T01:05:28.609593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差连接和层规范化\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,dropout,**kwargs):\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.ln=nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ],
   "id": "d879ec795cb9e9c6",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.622013Z",
     "start_time": "2025-05-28T01:05:28.614972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()"
   ],
   "id": "a66677e2919f8b59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddNorm(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (ln): LayerNorm((3, 4), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.629109Z",
     "start_time": "2025-05-28T01:05:28.622013Z"
    }
   },
   "cell_type": "code",
   "source": "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape",
   "id": "acc33c8074dfc490",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.636691Z",
     "start_time": "2025-05-28T01:05:28.629109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,use_bias=False,**kwargs):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=use_bias)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm2=AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self,X,valid_lens):\n",
    "        Y=self.addnorm1(X,self.attention(X,X,X,valid_lens))\n",
    "        return self.addnorm2(Y,self.ffn(Y))"
   ],
   "id": "d7cc7bfcb9c9e3e9",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T06:28:06.020831Z",
     "start_time": "2025-05-28T06:28:06.014188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.ones((2,100,24))\n",
    "valid_lens=torch.tensor([3,2])\n",
    "encoder_blk=EncoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=8,dropout=0.5)\n",
    "encoder_blk.eval()"
   ],
   "id": "b00281a03e505be3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (attention): MultiHeadAttention(\n",
       "    (attention): DotProductAttention(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (W_q): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_k): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_v): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_o): Linear(in_features=24, out_features=24, bias=False)\n",
       "  )\n",
       "  (addnorm1): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ffn): PositionWiseFFN(\n",
       "    (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  )\n",
       "  (addnorm2): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.659071Z",
     "start_time": "2025-05-28T01:05:28.646355Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk(x,valid_lens).shape",
   "id": "1ca9657b5a63e5bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.670620Z",
     "start_time": "2025-05-28T01:05:28.659071Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk.attention.attention.attention_weights",
   "id": "e8ed2966391c7a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.678846Z",
     "start_time": "2025-05-28T01:05:28.670620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i)\n",
    "                                 ,EncoderBlock(key_size,query_size,value_size,num_hiddens\n",
    "                                               ,norm_shape,ffn_num_input,ffn_num_hiddens\n",
    "                                               ,num_heads,dropout,use_bias))\n",
    "    def forward(self,X,valid_lens,*args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放， 因为嵌入层会把整个层的元素都压缩在均值为0方差为1的分布中 \n",
    "        # 因此当num_hiddens越大单个值会越小所以这么乘 保证每个元素也是在-1 1之间\n",
    "        # 然后再与位置编码相加。\n",
    "        X=self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ],
   "id": "5e5c0f360463c19c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.695486Z",
     "start_time": "2025-05-28T01:05:28.678846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ],
   "id": "ac16a9590078dc5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T06:30:39.021336Z",
     "start_time": "2025-05-28T06:30:39.012717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的第i个块\"\"\"\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i,**kwargs):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.i=i\n",
    "        self.attention1=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.attention2=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm2=AddNorm(norm_shape,dropout)\n",
    "        self.ffn=PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm3=AddNorm(norm_shape, dropout)\n",
    "    def forward(self,X,state):\n",
    "        \"\"\"X每次的输入 可能是初始输入也可能是上次的输出 形状B L H\n",
    "            state: 一个包含解码器运行状态的列表。\n",
    "            state[0]: enc_outputs - 编码器的最终输出。这个在整个解码过程中都是固定的。形状： (batch_size, enc_num_steps, num_hiddens)。\n",
    "            state[1]: enc_valid_lens - 编码器输入的有效长度。用于对编码器输出进行填充掩码。形状： (batch_size,)。\n",
    "            state[2]: 一个列表，用于存储每个解码器块在自注意力层中积累的键值对历史。这对于高效的**序列生成（推理）**至关重要。\n",
    "            \"\"\"\n",
    "        enc_outputs,enc_valid_lens=state[0],state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values=X\n",
    "        else:# 其实这里主要是推理时发挥作用 因为训练时传入的是一个完成的X 直接复制给key_values\n",
    "            # 这个key_values每次都会叠加 叠加的是本次的X\n",
    "            key_values=torch.cat((state[2][self.i],X),dim=1)\n",
    "        state[2][self.i]=key_values\n",
    "        if self.training:\n",
    "            batch_size,num_steps,_=X.shape\n",
    "            # dec_valid_lens的开头:(batch_size,num_steps),其中每一行是[1,2,...,num_steps] 处理后就变成 batch_size*num_steps的矩阵了\n",
    "            # 基于这个valid_lens矩阵，训练状态下会生成一个解码有效长度目的是最后得到一个下三角矩阵 保证每一步x只能关注到当前之前步的编码\n",
    "            dec_valid_lens=torch.arange(1,num_steps+1,device=X.device).repeat(batch_size,1)\n",
    "        else:\n",
    "            dec_valid_lens=None\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X,key_values,key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        # 编码器-解码器注意力\n",
    "        # enc_outputs 形状（batch_size,num_steps,num_hiddens）\n",
    "        Y2 = self.attention2(Y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        Z = self.addnorm2(Y,Y2)\n",
    "        return self.addnorm3(Z,self.ffn(Z)),state\n"
   ],
   "id": "ea5997d91b547af8",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T06:30:39.593665Z",
     "start_time": "2025-05-28T06:30:39.587833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder_blk=DecoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=4,dropout=0.5,i=0)\n",
    "decoder_blk.eval()\n",
    "X=torch.ones((2,100,24))\n",
    "state=[encoder_blk(X,valid_lens),valid_lens,[None]]\n",
    "state[0].shape,decoder_blk(X, state)[0].shape"
   ],
   "id": "dd5db11c3b823262",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:24:10.035305Z",
     "start_time": "2025-05-28T07:24:10.013722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,**kwargs):\n",
    "        super(TransformerDecoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i),\n",
    "                                 DecoderBlock(key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i))\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "    def init_state(self,enc_outputs,enc_valid_lens,*args):\n",
    "        return [enc_outputs,enc_valid_lens,[None]*self.num_layers]\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        # 将X放大避免整个embeding 被位置编码的大小淹没 因为位置编码是-1 1之间\n",
    "        X = self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            X,state = blk(X, state)\n",
    "            # 保留自注意力权重信息\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # 保留编码解码注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "            \n",
    "        return self.dense(X),state\n",
    "        \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ],
   "id": "511684e7933b9191",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:50:40.691290Z",
     "start_time": "2025-05-28T07:50:40.682758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_hiddens,num_layers,dropout,batch_size,num_steps=32,2,0.1,64,10\n",
    "lr,num_epochs,device=0.005,200,'cpu'\n",
    "ffn_num_input=32\n",
    "ffn_num_hiddens=64\n",
    "num_heads=4\n",
    "key_size,query_size,value_size=32,32,32\n",
    "norm_shape=[32]"
   ],
   "id": "72414a099f78a9d2",
   "outputs": [],
   "execution_count": 120
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from dplearning_second_part.limu_dplearning.utils.useful_func import tokenize_nmt, Vocal, build_array_nmt, \\\n",
    "    preprocess_nmt, read_data_nmt\n",
    "\n",
    "src, tgt = tokenize_nmt(preprocess_nmt(read_data_nmt()),num_examples=600)\n",
    "src_vocab = Vocal(src, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocal(tgt, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "src_data, src_valid = build_array_nmt(src, src_vocab, 10)\n",
    "tgt_data, tgt_valid = build_array_nmt(tgt, tgt_vocab, 10)\n",
    "dataset = torch.utils.data.TensorDataset(src_data, src_valid, tgt_data, tgt_valid)\n",
    "## 训练数据\n",
    "train_data = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False)"
   ],
   "id": "c2bd7dfcb549bd5c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "837ddd29753f00c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
