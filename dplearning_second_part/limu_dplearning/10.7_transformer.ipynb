{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:04.846936Z",
     "start_time": "2025-06-01T02:54:57.621485Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dplearning_second_part.limu_dplearning.utils.useful_func import tokenize_nmt, Vocal, build_array_nmt, \\\n",
    "    preprocess_nmt, read_data_nmt\n",
    "\n",
    "src, tgt = tokenize_nmt(preprocess_nmt(read_data_nmt()),num_examples=600)\n",
    "src_vocab = Vocal(src, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocal(tgt, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "src_data, src_valid = build_array_nmt(src, src_vocab, 10)\n",
    "tgt_data, tgt_valid = build_array_nmt(tgt, tgt_vocab, 10)\n",
    "dataset = torch.utils.data.TensorDataset(src_data, src_valid, tgt_data, tgt_valid)\n",
    "## 训练数据\n",
    "train_data = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:04.854088Z",
     "start_time": "2025-06-01T02:55:04.850557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 掩蔽softmax实现\n",
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X,valid_lens=None):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    注意：X是三维的 一般是 B Q K-V数量\n",
    "     掩码的目的在于 避免Q关注到无关的或者禁止的K-V\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    shape=X.shape\n",
    "    # valid_lens可以是1d或者2d 如果是1d则必须长为B 如果是2d则必须是B*Q\n",
    "    if valid_lens.dim() == 1:\n",
    "        # 如果是1d则扩展成2d 形状B Q \n",
    "        valid_lens = valid_lens.unsqueeze(1).repeat(1, shape[1])\n",
    "    # 因为sequence函数要求的X是2d valid_lens是1d\n",
    "    X=X.reshape(-1,shape[-1])\n",
    "    valid_lens=valid_lens.reshape(-1)\n",
    "    X_mask=sequence_mask(X, valid_lens,value=-1e6)\n",
    "    # 完事之后还要把输出的形状转为X原本的形状 \n",
    "    return F.softmax(X_mask, dim=-1).reshape(shape)"
   ],
   "id": "18e7b29624619a09",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:04.969010Z",
     "start_time": "2025-06-01T02:55:04.963115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建一个足够长的P 长是指第2个维度 第一个维度应该是batch_size\n",
    "        # 每一个batch用的是一套位置编码 如果不用一套的话就无法学习到位置信息泛化能力极差\n",
    "        self.P=torch.zeros(1,max_len,num_hiddens)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        X=torch.arange(max_len,dtype=torch.float32).reshape(-1,1)/\\\n",
    "            torch.pow(10000,torch.arange(0,num_hiddens,2)/num_hiddens)\n",
    "        self.P[:,:,0::2]=torch.sin(X)\n",
    "        self.P[:,:,1::2]=torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X+self.P[:,:X.shape[1],:].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "# 先实现一个点积注意力\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_weights = None\n",
    "    # q(b,step,embed_size)\n",
    "    # k(b,键值对个数,embed_size)\n",
    "    # v(b,键值对个数,embed_size)\n",
    "    def forward(self, q, k, v,valid_lens):\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "        self.attention_weights=masked_softmax(attn_weights,valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), v)\n",
    "\n",
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    # qkv各自的embed_size, 隐藏层大小 头数量\n",
    "    # 需要并行运算多个头 因此num_hiddens 必须能够整除以num_heads\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q=nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o=nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "\n",
    "    def forward(self, q, k, v,valid_lens=None):\n",
    "\n",
    "        queries=self.W_q(q)\n",
    "        keys=self.W_k(k)\n",
    "        values=self.W_v(v)\n",
    "\n",
    "        # 在这一步需要对qkv拆分为多头 并行计算attention\n",
    "        queries=queries.reshape(queries.shape[0],queries.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        keys=keys.reshape(keys.shape[0],keys.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        values=values.reshape(values.shape[0],values.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "\n",
    "        queries=queries.reshape(-1,queries.shape[2],queries.shape[3])\n",
    "        keys=keys.reshape(-1,keys.shape[2],keys.shape[3])\n",
    "        values=values.reshape(-1,values.shape[2],values.shape[3])\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        attn_weights=self.attention(queries,keys,values,valid_lens)\n",
    "        attn_weights=attn_weights.reshape(-1,self.num_heads,attn_weights.shape[1],attn_weights.shape[2])\n",
    "        attn_weights=attn_weights.permute(0,2,1,3)\n",
    "        outputs=attn_weights.reshape(attn_weights.shape[0],attn_weights.shape[1],-1)\n",
    "\n",
    "        return self.W_o(outputs)\n"
   ],
   "id": "d38ebef52929b469",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:04.996096Z",
     "start_time": "2025-06-01T02:55:04.992612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_outputs,**kwargs):\n",
    "        super(PositionWiseFFN,self).__init__(**kwargs)\n",
    "        self.dense1=nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(ffn_num_hiddens,ffn_num_outputs)\n",
    "    def forward(self,x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ],
   "id": "5564bf4d2e4464d8",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.035514Z",
     "start_time": "2025-06-01T02:55:05.029473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ffn=PositionWiseFFN(ffn_num_input=4,ffn_num_hiddens=4,ffn_num_outputs=8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2,3,4))).shape"
   ],
   "id": "1204f7b63ecc1632",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.048621Z",
     "start_time": "2025-06-01T02:55:05.045180Z"
    }
   },
   "cell_type": "code",
   "source": "# 残差连接和层规范化",
   "id": "fc174191f0bf2b8b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.067160Z",
     "start_time": "2025-06-01T02:55:05.063452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)"
   ],
   "id": "7bd318b369af9d76",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.082238Z",
     "start_time": "2025-06-01T02:55:05.077365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.tensor([[1,2],[2,3]],dtype=torch.float32)\n",
    "x,x.shape"
   ],
   "id": "ad278bb0ac7ab58d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [2., 3.]]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.107099Z",
     "start_time": "2025-06-01T02:55:05.102958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差连接和层规范化\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,dropout,**kwargs):\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.ln=nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ],
   "id": "d879ec795cb9e9c6",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.117240Z",
     "start_time": "2025-06-01T02:55:05.113984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()"
   ],
   "id": "a66677e2919f8b59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddNorm(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (ln): LayerNorm((3, 4), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.134860Z",
     "start_time": "2025-06-01T02:55:05.130037Z"
    }
   },
   "cell_type": "code",
   "source": "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape",
   "id": "acc33c8074dfc490",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.155458Z",
     "start_time": "2025-06-01T02:55:05.152072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,use_bias=False,**kwargs):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=use_bias)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm2=AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self,X,valid_lens):\n",
    "        Y=self.addnorm1(X,self.attention(X,X,X,valid_lens))\n",
    "        return self.addnorm2(Y,self.ffn(Y))"
   ],
   "id": "d7cc7bfcb9c9e3e9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.167928Z",
     "start_time": "2025-06-01T02:55:05.164218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.ones((2,100,24))\n",
    "valid_lens=torch.tensor([3,2])\n",
    "encoder_blk=EncoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=8,dropout=0.5)\n",
    "encoder_blk.eval()"
   ],
   "id": "b00281a03e505be3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (attention): MultiHeadAttention(\n",
       "    (attention): DotProductAttention(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (W_q): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_k): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_v): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_o): Linear(in_features=24, out_features=24, bias=False)\n",
       "  )\n",
       "  (addnorm1): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ffn): PositionWiseFFN(\n",
       "    (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  )\n",
       "  (addnorm2): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:09.697436Z",
     "start_time": "2025-06-01T02:55:09.690810Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk(x,valid_lens)",
   "id": "1ca9657b5a63e5bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         ...,\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758]],\n",
       "\n",
       "        [[ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         ...,\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758],\n",
       "         [ 1.3973,  0.5727, -0.5850,  ...,  0.0553,  0.0989,  1.5758]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.221791Z",
     "start_time": "2025-06-01T02:55:05.215957Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk.attention.attention.attention_weights",
   "id": "e8ed2966391c7a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-01T02:55:05.236076Z",
     "start_time": "2025-06-01T02:55:05.232605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i)\n",
    "                                 ,EncoderBlock(key_size,query_size,value_size,num_hiddens\n",
    "                                               ,norm_shape,ffn_num_input,ffn_num_hiddens\n",
    "                                               ,num_heads,dropout,use_bias))\n",
    "    def forward(self,X,valid_lens,*args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放， 因为嵌入层会把整个层的元素都压缩在均值为0方差为1的分布中 \n",
    "        # 因此当num_hiddens越大单个值会越小所以这么乘 保证每个元素也是在-1 1之间\n",
    "        # 然后再与位置编码相加。\n",
    "        X=self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ],
   "id": "5e5c0f360463c19c",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T01:05:28.695486Z",
     "start_time": "2025-05-28T01:05:28.678846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ],
   "id": "ac16a9590078dc5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T06:30:39.021336Z",
     "start_time": "2025-05-28T06:30:39.012717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的第i个块\"\"\"\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i,**kwargs):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.i=i\n",
    "        self.attention1=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.attention2=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm2=AddNorm(norm_shape,dropout)\n",
    "        self.ffn=PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm3=AddNorm(norm_shape, dropout)\n",
    "    def forward(self,X,state):\n",
    "        \"\"\"X每次的输入 可能是初始输入也可能是上次的输出 形状B L H\n",
    "            state: 一个包含解码器运行状态的列表。\n",
    "            state[0]: enc_outputs - 编码器的最终输出。这个在整个解码过程中都是固定的。形状： (batch_size, enc_num_steps, num_hiddens)。\n",
    "            state[1]: enc_valid_lens - 编码器输入的有效长度。用于对编码器输出进行填充掩码。形状： (batch_size,)。\n",
    "            state[2]: 一个列表，用于存储每个解码器块在自注意力层中积累的键值对历史。这对于高效的**序列生成（推理）**至关重要。\n",
    "            \"\"\"\n",
    "        enc_outputs,enc_valid_lens=state[0],state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values=X\n",
    "        else:# 其实这里主要是推理时发挥作用 因为训练时传入的是一个完成的X 直接复制给key_values\n",
    "            # 这个key_values每次都会叠加 叠加的是本次的X\n",
    "            key_values=torch.cat((state[2][self.i],X),dim=1)\n",
    "        state[2][self.i]=key_values\n",
    "        if self.training:\n",
    "            batch_size,num_steps,_=X.shape\n",
    "            # dec_valid_lens的开头:(batch_size,num_steps),其中每一行是[1,2,...,num_steps] 处理后就变成 batch_size*num_steps的矩阵了\n",
    "            # 基于这个valid_lens矩阵，训练状态下会生成一个解码有效长度目的是最后得到一个下三角矩阵 保证每一步x只能关注到当前之前步的编码\n",
    "            dec_valid_lens=torch.arange(1,num_steps+1,device=X.device).repeat(batch_size,1)\n",
    "        else:\n",
    "            dec_valid_lens=None\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X,key_values,key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        # 编码器-解码器注意力\n",
    "        # enc_outputs 形状（batch_size,num_steps,num_hiddens）\n",
    "        Y2 = self.attention2(Y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        Z = self.addnorm2(Y,Y2)\n",
    "        return self.addnorm3(Z,self.ffn(Z)),state\n"
   ],
   "id": "ea5997d91b547af8",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T06:30:39.593665Z",
     "start_time": "2025-05-28T06:30:39.587833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder_blk=DecoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=4,dropout=0.5,i=0)\n",
    "decoder_blk.eval()\n",
    "X=torch.ones((2,100,24))\n",
    "state=[encoder_blk(X,valid_lens),valid_lens,[None]]\n",
    "state[0].shape,decoder_blk(X, state)[0].shape"
   ],
   "id": "dd5db11c3b823262",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T07:24:10.035305Z",
     "start_time": "2025-05-28T07:24:10.013722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,**kwargs):\n",
    "        super(TransformerDecoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i),\n",
    "                                 DecoderBlock(key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i))\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "    def init_state(self,enc_outputs,enc_valid_lens,*args):\n",
    "        return [enc_outputs,enc_valid_lens,[None]*self.num_layers]\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        # 将X放大避免整个embeding 被位置编码的大小淹没 因为位置编码是-1 1之间\n",
    "        X = self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            X,state = blk(X, state)\n",
    "            # 保留自注意力权重信息\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # 保留编码解码注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "            \n",
    "        return self.dense(X),state\n",
    "        \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ],
   "id": "511684e7933b9191",
   "outputs": [],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:03:55.232419Z",
     "start_time": "2025-05-28T08:03:55.224662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_hiddens,num_layers,dropout,batch_size,num_steps=32,2,0.1,64,10\n",
    "lr,num_epochs,device=0.005,200,'cpu'\n",
    "ffn_num_input=32\n",
    "ffn_num_hiddens=64\n",
    "num_heads=4\n",
    "key_size,query_size,value_size=32,32,32\n",
    "norm_shape=[32]"
   ],
   "id": "72414a099f78a9d2",
   "outputs": [],
   "execution_count": 126
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:04:02.238299Z",
     "start_time": "2025-05-28T08:03:55.731946Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c2bd7dfcb549bd5c",
   "outputs": [],
   "execution_count": 127
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T08:04:02.255179Z",
     "start_time": "2025-05-28T08:04:02.238299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    len(src_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)"
   ],
   "id": "837ddd29753f00c1",
   "outputs": [],
   "execution_count": 128
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:40:59.306476Z",
     "start_time": "2025-05-28T09:40:59.300168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,enc_x,dec_x,*args):\n",
    "        enc_outputs=self.encoder(enc_x,*args)\n",
    "        state=self.decoder.init_state(enc_outputs,*args)\n",
    "        dec_outputs=self.decoder(dec_x,state)\n",
    "        return dec_outputs"
   ],
   "id": "37096b9058598ca6",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:40:59.939846Z",
     "start_time": "2025-05-28T09:40:59.935737Z"
    }
   },
   "cell_type": "code",
   "source": "net = EncoderDecoder(encoder, decoder)",
   "id": "84b75ba3c700144b",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:42:55.771017Z",
     "start_time": "2025-05-28T09:41:53.621492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from d2l.torch import grad_clipping\n",
    "from torch import optim\n",
    "# 初始化模型参数\n",
    "def xavier_init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.GRU:\n",
    "        for name,param in m.named_parameters():\n",
    "            if \"weight\" in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "net.apply(xavier_init_weights)\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    # 首先x是二维的 最内层维度是句子长度 注意：是训练集所以才知道句子真实长度\n",
    "    # 拿出总的长度 得到长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 然后用总长度生成一个1维的向量 使用函数扩展成2维以便与valid_len进行广播\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long), dim=0)\n",
    "    # mask在0维度扩充 valid在1维度扩充 因为每一个valid对应的是每一个x valid的数字其实是x的第二维向量\n",
    "    mask = (mask < torch.unsqueeze(valid_len, dim=1))  # 这里小于号就够了 因为<eos>所在位置的索引其实是valid_len-1\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "# 拓展的softmax因为对填充值进行softmax其实没有什么意义\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        # 交叉熵损失期望的两个输入 x是 batch_size vocab_size seq_lenth \n",
    "        # y 是 batch_size seq_len\n",
    "        unweight_loss = super().forward(pred, label)\n",
    "        weights_loss = unweight_loss * weights\n",
    "        return weights_loss.mean(dim=1)\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()\n",
    "\n",
    "# train\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "for epoch in range(300):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src,src_valid,tgt,tgt_valid=batch\n",
    "        Y=torch.cat((torch.tensor([tgt_vocab['<bos>']]).unsqueeze(0).repeat(tgt.shape[0],1),tgt),dim=1)[:,:-1]\n",
    "        # 易错点1：训练的时候应该使用带有bos的Y 表示强制教学 此时输出的y_hat实际上是没有bos的\n",
    "        y_hat,_=net(src,Y,src_valid)\n",
    "        # 点2 因为输出的y_hat 没有bos 因此在计算loss的时候应该使用原始序列作为目标序列\n",
    "        l=loss(y_hat,tgt,tgt_valid).sum()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "    print(l)"
   ],
   "id": "b996d767ef33940d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(48.5138, grad_fn=<SumBackward0>)\n",
      "tensor(42.6275, grad_fn=<SumBackward0>)\n",
      "tensor(35.3484, grad_fn=<SumBackward0>)\n",
      "tensor(32.0224, grad_fn=<SumBackward0>)\n",
      "tensor(30.7526, grad_fn=<SumBackward0>)\n",
      "tensor(29.0918, grad_fn=<SumBackward0>)\n",
      "tensor(27.2134, grad_fn=<SumBackward0>)\n",
      "tensor(25.2844, grad_fn=<SumBackward0>)\n",
      "tensor(26.1529, grad_fn=<SumBackward0>)\n",
      "tensor(24.1607, grad_fn=<SumBackward0>)\n",
      "tensor(22.9639, grad_fn=<SumBackward0>)\n",
      "tensor(22.0323, grad_fn=<SumBackward0>)\n",
      "tensor(22.4386, grad_fn=<SumBackward0>)\n",
      "tensor(23.0290, grad_fn=<SumBackward0>)\n",
      "tensor(21.0733, grad_fn=<SumBackward0>)\n",
      "tensor(20.5501, grad_fn=<SumBackward0>)\n",
      "tensor(19.8416, grad_fn=<SumBackward0>)\n",
      "tensor(19.4524, grad_fn=<SumBackward0>)\n",
      "tensor(17.3452, grad_fn=<SumBackward0>)\n",
      "tensor(17.8581, grad_fn=<SumBackward0>)\n",
      "tensor(15.7499, grad_fn=<SumBackward0>)\n",
      "tensor(16.4918, grad_fn=<SumBackward0>)\n",
      "tensor(18.0145, grad_fn=<SumBackward0>)\n",
      "tensor(16.0790, grad_fn=<SumBackward0>)\n",
      "tensor(18.5534, grad_fn=<SumBackward0>)\n",
      "tensor(16.3859, grad_fn=<SumBackward0>)\n",
      "tensor(14.9754, grad_fn=<SumBackward0>)\n",
      "tensor(13.9042, grad_fn=<SumBackward0>)\n",
      "tensor(13.1142, grad_fn=<SumBackward0>)\n",
      "tensor(14.5243, grad_fn=<SumBackward0>)\n",
      "tensor(14.1506, grad_fn=<SumBackward0>)\n",
      "tensor(13.8136, grad_fn=<SumBackward0>)\n",
      "tensor(11.7518, grad_fn=<SumBackward0>)\n",
      "tensor(13.0823, grad_fn=<SumBackward0>)\n",
      "tensor(14.2987, grad_fn=<SumBackward0>)\n",
      "tensor(11.9344, grad_fn=<SumBackward0>)\n",
      "tensor(11.9929, grad_fn=<SumBackward0>)\n",
      "tensor(10.5816, grad_fn=<SumBackward0>)\n",
      "tensor(11.7877, grad_fn=<SumBackward0>)\n",
      "tensor(9.7502, grad_fn=<SumBackward0>)\n",
      "tensor(10.0462, grad_fn=<SumBackward0>)\n",
      "tensor(10.3194, grad_fn=<SumBackward0>)\n",
      "tensor(11.0506, grad_fn=<SumBackward0>)\n",
      "tensor(11.1066, grad_fn=<SumBackward0>)\n",
      "tensor(10.3902, grad_fn=<SumBackward0>)\n",
      "tensor(10.6597, grad_fn=<SumBackward0>)\n",
      "tensor(9.4686, grad_fn=<SumBackward0>)\n",
      "tensor(9.4435, grad_fn=<SumBackward0>)\n",
      "tensor(9.4852, grad_fn=<SumBackward0>)\n",
      "tensor(9.8851, grad_fn=<SumBackward0>)\n",
      "tensor(10.1718, grad_fn=<SumBackward0>)\n",
      "tensor(8.5702, grad_fn=<SumBackward0>)\n",
      "tensor(9.3811, grad_fn=<SumBackward0>)\n",
      "tensor(7.4302, grad_fn=<SumBackward0>)\n",
      "tensor(8.8458, grad_fn=<SumBackward0>)\n",
      "tensor(8.6223, grad_fn=<SumBackward0>)\n",
      "tensor(9.1564, grad_fn=<SumBackward0>)\n",
      "tensor(9.1846, grad_fn=<SumBackward0>)\n",
      "tensor(9.9058, grad_fn=<SumBackward0>)\n",
      "tensor(8.9515, grad_fn=<SumBackward0>)\n",
      "tensor(7.5656, grad_fn=<SumBackward0>)\n",
      "tensor(8.3146, grad_fn=<SumBackward0>)\n",
      "tensor(6.7632, grad_fn=<SumBackward0>)\n",
      "tensor(7.2124, grad_fn=<SumBackward0>)\n",
      "tensor(8.5497, grad_fn=<SumBackward0>)\n",
      "tensor(5.9741, grad_fn=<SumBackward0>)\n",
      "tensor(6.1495, grad_fn=<SumBackward0>)\n",
      "tensor(9.6317, grad_fn=<SumBackward0>)\n",
      "tensor(8.3924, grad_fn=<SumBackward0>)\n",
      "tensor(7.0494, grad_fn=<SumBackward0>)\n",
      "tensor(7.4093, grad_fn=<SumBackward0>)\n",
      "tensor(6.4033, grad_fn=<SumBackward0>)\n",
      "tensor(7.5947, grad_fn=<SumBackward0>)\n",
      "tensor(7.4471, grad_fn=<SumBackward0>)\n",
      "tensor(7.0705, grad_fn=<SumBackward0>)\n",
      "tensor(5.8615, grad_fn=<SumBackward0>)\n",
      "tensor(5.7724, grad_fn=<SumBackward0>)\n",
      "tensor(6.6067, grad_fn=<SumBackward0>)\n",
      "tensor(6.9659, grad_fn=<SumBackward0>)\n",
      "tensor(6.1002, grad_fn=<SumBackward0>)\n",
      "tensor(5.9539, grad_fn=<SumBackward0>)\n",
      "tensor(7.6598, grad_fn=<SumBackward0>)\n",
      "tensor(7.0564, grad_fn=<SumBackward0>)\n",
      "tensor(6.5815, grad_fn=<SumBackward0>)\n",
      "tensor(6.8690, grad_fn=<SumBackward0>)\n",
      "tensor(7.6984, grad_fn=<SumBackward0>)\n",
      "tensor(6.3773, grad_fn=<SumBackward0>)\n",
      "tensor(7.1601, grad_fn=<SumBackward0>)\n",
      "tensor(7.3226, grad_fn=<SumBackward0>)\n",
      "tensor(6.6969, grad_fn=<SumBackward0>)\n",
      "tensor(7.5320, grad_fn=<SumBackward0>)\n",
      "tensor(6.1231, grad_fn=<SumBackward0>)\n",
      "tensor(5.8327, grad_fn=<SumBackward0>)\n",
      "tensor(6.1127, grad_fn=<SumBackward0>)\n",
      "tensor(7.0206, grad_fn=<SumBackward0>)\n",
      "tensor(5.4273, grad_fn=<SumBackward0>)\n",
      "tensor(5.7487, grad_fn=<SumBackward0>)\n",
      "tensor(5.3318, grad_fn=<SumBackward0>)\n",
      "tensor(4.2246, grad_fn=<SumBackward0>)\n",
      "tensor(4.1823, grad_fn=<SumBackward0>)\n",
      "tensor(5.0265, grad_fn=<SumBackward0>)\n",
      "tensor(5.0832, grad_fn=<SumBackward0>)\n",
      "tensor(5.7066, grad_fn=<SumBackward0>)\n",
      "tensor(4.7333, grad_fn=<SumBackward0>)\n",
      "tensor(4.9063, grad_fn=<SumBackward0>)\n",
      "tensor(4.9957, grad_fn=<SumBackward0>)\n",
      "tensor(6.4367, grad_fn=<SumBackward0>)\n",
      "tensor(3.6907, grad_fn=<SumBackward0>)\n",
      "tensor(5.5447, grad_fn=<SumBackward0>)\n",
      "tensor(5.8699, grad_fn=<SumBackward0>)\n",
      "tensor(6.1181, grad_fn=<SumBackward0>)\n",
      "tensor(5.1315, grad_fn=<SumBackward0>)\n",
      "tensor(4.8934, grad_fn=<SumBackward0>)\n",
      "tensor(4.9211, grad_fn=<SumBackward0>)\n",
      "tensor(4.8102, grad_fn=<SumBackward0>)\n",
      "tensor(4.9627, grad_fn=<SumBackward0>)\n",
      "tensor(4.4052, grad_fn=<SumBackward0>)\n",
      "tensor(4.9815, grad_fn=<SumBackward0>)\n",
      "tensor(4.7530, grad_fn=<SumBackward0>)\n",
      "tensor(4.1104, grad_fn=<SumBackward0>)\n",
      "tensor(4.6164, grad_fn=<SumBackward0>)\n",
      "tensor(6.4738, grad_fn=<SumBackward0>)\n",
      "tensor(4.5275, grad_fn=<SumBackward0>)\n",
      "tensor(5.1109, grad_fn=<SumBackward0>)\n",
      "tensor(4.5943, grad_fn=<SumBackward0>)\n",
      "tensor(4.4359, grad_fn=<SumBackward0>)\n",
      "tensor(3.5269, grad_fn=<SumBackward0>)\n",
      "tensor(4.3834, grad_fn=<SumBackward0>)\n",
      "tensor(5.2496, grad_fn=<SumBackward0>)\n",
      "tensor(4.1332, grad_fn=<SumBackward0>)\n",
      "tensor(5.3077, grad_fn=<SumBackward0>)\n",
      "tensor(3.7644, grad_fn=<SumBackward0>)\n",
      "tensor(3.8385, grad_fn=<SumBackward0>)\n",
      "tensor(6.7730, grad_fn=<SumBackward0>)\n",
      "tensor(3.6291, grad_fn=<SumBackward0>)\n",
      "tensor(5.3776, grad_fn=<SumBackward0>)\n",
      "tensor(4.8503, grad_fn=<SumBackward0>)\n",
      "tensor(4.3429, grad_fn=<SumBackward0>)\n",
      "tensor(3.7830, grad_fn=<SumBackward0>)\n",
      "tensor(4.1231, grad_fn=<SumBackward0>)\n",
      "tensor(4.2218, grad_fn=<SumBackward0>)\n",
      "tensor(3.5530, grad_fn=<SumBackward0>)\n",
      "tensor(4.9903, grad_fn=<SumBackward0>)\n",
      "tensor(3.9596, grad_fn=<SumBackward0>)\n",
      "tensor(4.6465, grad_fn=<SumBackward0>)\n",
      "tensor(3.4407, grad_fn=<SumBackward0>)\n",
      "tensor(4.1917, grad_fn=<SumBackward0>)\n",
      "tensor(4.5149, grad_fn=<SumBackward0>)\n",
      "tensor(4.2616, grad_fn=<SumBackward0>)\n",
      "tensor(4.0066, grad_fn=<SumBackward0>)\n",
      "tensor(4.2312, grad_fn=<SumBackward0>)\n",
      "tensor(4.5165, grad_fn=<SumBackward0>)\n",
      "tensor(3.7128, grad_fn=<SumBackward0>)\n",
      "tensor(4.1146, grad_fn=<SumBackward0>)\n",
      "tensor(3.2504, grad_fn=<SumBackward0>)\n",
      "tensor(2.8534, grad_fn=<SumBackward0>)\n",
      "tensor(4.2433, grad_fn=<SumBackward0>)\n",
      "tensor(3.5023, grad_fn=<SumBackward0>)\n",
      "tensor(5.0909, grad_fn=<SumBackward0>)\n",
      "tensor(3.9677, grad_fn=<SumBackward0>)\n",
      "tensor(4.0546, grad_fn=<SumBackward0>)\n",
      "tensor(3.5510, grad_fn=<SumBackward0>)\n",
      "tensor(4.0712, grad_fn=<SumBackward0>)\n",
      "tensor(2.7722, grad_fn=<SumBackward0>)\n",
      "tensor(5.5789, grad_fn=<SumBackward0>)\n",
      "tensor(4.4194, grad_fn=<SumBackward0>)\n",
      "tensor(5.1537, grad_fn=<SumBackward0>)\n",
      "tensor(2.9905, grad_fn=<SumBackward0>)\n",
      "tensor(3.0427, grad_fn=<SumBackward0>)\n",
      "tensor(3.9835, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[145], line 54\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m# 点2 因为输出的y_hat 没有bos 因此在计算loss的时候应该使用原始序列作为目标序列\u001B[39;00m\n\u001B[0;32m     53\u001B[0m l\u001B[38;5;241m=\u001B[39mloss(y_hat,tgt,tgt_valid)\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m---> 54\u001B[0m \u001B[43ml\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m grad_clipping(net, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     56\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 145
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T09:41:00.859062Z",
     "start_time": "2025-05-28T09:41:00.851145Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7aef42da1e27db77",
   "outputs": [],
   "execution_count": 143
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "81cc56cdfd972945"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
