{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.019018Z",
     "start_time": "2025-05-28T13:05:59.616722Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dplearning_second_part.limu_dplearning.utils.useful_func import tokenize_nmt, Vocal, build_array_nmt, \\\n",
    "    preprocess_nmt, read_data_nmt\n",
    "\n",
    "src, tgt = tokenize_nmt(preprocess_nmt(read_data_nmt()),num_examples=600)\n",
    "src_vocab = Vocal(src, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "tgt_vocab = Vocal(tgt, min_feq=2,\n",
    "                  reserved_tokens=['<pad>', '<bos>', '<eos>'])\n",
    "src_data, src_valid = build_array_nmt(src, src_vocab, 10)\n",
    "tgt_data, tgt_valid = build_array_nmt(tgt, tgt_vocab, 10)\n",
    "dataset = torch.utils.data.TensorDataset(src_data, src_valid, tgt_data, tgt_valid)\n",
    "## 训练数据\n",
    "train_data = torch.utils.data.DataLoader(dataset=dataset, batch_size=32, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.026421Z",
     "start_time": "2025-05-28T13:06:04.023147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 掩蔽softmax实现\n",
    "def sequence_mask(X,valid_len, value=0):\n",
    "    \"\"\"X必须为2维batch_size* num_steps，valid_len必须为1维batch\n",
    "       意味着 第i个batch_size的序列有效长度为 valid_len[i]\n",
    "    \"\"\"\n",
    "    if X.dim() != 2 or valid_len.dim() != 1:\n",
    "        raise ValueError('Expect 2d tensor')\n",
    "    # 获取num_steps长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 生成顺序序列，用于与valid_len比较\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long),dim=0)\n",
    "    # 判断需要掩码的部分，这里必须要保证mask和vilid_len形状相等 否则会出问题\n",
    "    mask = (mask<valid_len.unsqueeze(1).repeat(1, maxlen))\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "def masked_softmax(X,valid_lens=None):\n",
    "    \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\n",
    "    注意：X是三维的 一般是 B Q K-V数量\n",
    "     掩码的目的在于 避免Q关注到无关的或者禁止的K-V\n",
    "    \"\"\"\n",
    "    if valid_lens is None:\n",
    "        return F.softmax(X, dim=-1)\n",
    "    shape=X.shape\n",
    "    # valid_lens可以是1d或者2d 如果是1d则必须长为B 如果是2d则必须是B*Q\n",
    "    if valid_lens.dim() == 1:\n",
    "        # 如果是1d则扩展成2d 形状B Q \n",
    "        valid_lens = valid_lens.unsqueeze(1).repeat(1, shape[1])\n",
    "    # 因为sequence函数要求的X是2d valid_lens是1d\n",
    "    X=X.reshape(-1,shape[-1])\n",
    "    valid_lens=valid_lens.reshape(-1)\n",
    "    X_mask=sequence_mask(X, valid_lens,value=-1e6)\n",
    "    # 完事之后还要把输出的形状转为X原本的形状 \n",
    "    return F.softmax(X_mask, dim=-1).reshape(shape)"
   ],
   "id": "18e7b29624619a09",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.058814Z",
     "start_time": "2025-05-28T13:06:04.051167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# 位置编码\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"位置编码\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout=0.1, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # 创建一个足够长的P 长是指第2个维度 第一个维度应该是batch_size\n",
    "        # 每一个batch用的是一套位置编码 如果不用一套的话就无法学习到位置信息泛化能力极差\n",
    "        self.P=torch.zeros(1,max_len,num_hiddens)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        X=torch.arange(max_len,dtype=torch.float32).reshape(-1,1)/\\\n",
    "            torch.pow(10000,torch.arange(0,num_hiddens,2)/num_hiddens)\n",
    "        self.P[:,:,0::2]=torch.sin(X)\n",
    "        self.P[:,:,1::2]=torch.cos(X)\n",
    "    def forward(self, X):\n",
    "        X = X+self.P[:,:X.shape[1],:].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "# 先实现一个点积注意力\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.2):\n",
    "        super(DotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention_weights = None\n",
    "    # q(b,step,embed_size)\n",
    "    # k(b,键值对个数,embed_size)\n",
    "    # v(b,键值对个数,embed_size)\n",
    "    def forward(self, q, k, v,valid_lens):\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))/torch.sqrt(torch.tensor(q.shape[-1]))\n",
    "        self.attention_weights=masked_softmax(attn_weights,valid_lens)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), v)\n",
    "\n",
    "#@save\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"多头注意力\"\"\"\n",
    "    # qkv各自的embed_size, 隐藏层大小 头数量\n",
    "    # 需要并行运算多个头 因此num_hiddens 必须能够整除以num_heads\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=False, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = DotProductAttention(dropout)\n",
    "        self.W_q=nn.Linear(query_size,num_hiddens,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,num_hiddens,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,num_hiddens,bias=bias)\n",
    "        self.W_o=nn.Linear(num_hiddens,num_hiddens,bias=bias)\n",
    "\n",
    "    def forward(self, q, k, v,valid_lens=None):\n",
    "\n",
    "        queries=self.W_q(q)\n",
    "        keys=self.W_k(k)\n",
    "        values=self.W_v(v)\n",
    "\n",
    "        # 在这一步需要对qkv拆分为多头 并行计算attention\n",
    "        queries=queries.reshape(queries.shape[0],queries.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        keys=keys.reshape(keys.shape[0],keys.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "        values=values.reshape(values.shape[0],values.shape[1],self.num_heads,-1).permute(0,2,1,3)\n",
    "\n",
    "        queries=queries.reshape(-1,queries.shape[2],queries.shape[3])\n",
    "        keys=keys.reshape(-1,keys.shape[2],keys.shape[3])\n",
    "        values=values.reshape(-1,values.shape[2],values.shape[3])\n",
    "\n",
    "        if valid_lens is not None:\n",
    "            # 在轴0，将第一项（标量或者矢量）复制num_heads次，\n",
    "            # 然后如此复制第二项，然后诸如此类。\n",
    "            valid_lens = torch.repeat_interleave(\n",
    "                valid_lens, repeats=self.num_heads, dim=0)\n",
    "\n",
    "        attn_weights=self.attention(queries,keys,values,valid_lens)\n",
    "        attn_weights=attn_weights.reshape(-1,self.num_heads,attn_weights.shape[1],attn_weights.shape[2])\n",
    "        attn_weights=attn_weights.permute(0,2,1,3)\n",
    "        attn_weights=attn_weights.reshape(attn_weights.shape[0],attn_weights.shape[1],-1)\n",
    "\n",
    "        return self.W_o(attn_weights)\n"
   ],
   "id": "d38ebef52929b469",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.069144Z",
     "start_time": "2025-05-28T13:06:04.065846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \"\"\"基于位置的前馈网络\"\"\"\n",
    "    def __init__(self,ffn_num_input,ffn_num_hiddens,ffn_num_outputs,**kwargs):\n",
    "        super(PositionWiseFFN,self).__init__(**kwargs)\n",
    "        self.dense1=nn.Linear(ffn_num_input,ffn_num_hiddens)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(ffn_num_hiddens,ffn_num_outputs)\n",
    "    def forward(self,x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ],
   "id": "5564bf4d2e4464d8",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.086822Z",
     "start_time": "2025-05-28T13:06:04.079266Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ffn=PositionWiseFFN(ffn_num_input=4,ffn_num_hiddens=4,ffn_num_outputs=8)\n",
    "ffn.eval()\n",
    "ffn(torch.ones((2,3,4))).shape"
   ],
   "id": "1204f7b63ecc1632",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.109799Z",
     "start_time": "2025-05-28T13:06:04.107551Z"
    }
   },
   "cell_type": "code",
   "source": "# 残差连接和层规范化",
   "id": "fc174191f0bf2b8b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.126470Z",
     "start_time": "2025-05-28T13:06:04.123964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ln = nn.LayerNorm(2)\n",
    "bn = nn.BatchNorm1d(2)"
   ],
   "id": "7bd318b369af9d76",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.140670Z",
     "start_time": "2025-05-28T13:06:04.135079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.tensor([[1,2],[2,3]],dtype=torch.float32)\n",
    "x,x.shape"
   ],
   "id": "ad278bb0ac7ab58d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [2., 3.]]),\n",
       " torch.Size([2, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.308327Z",
     "start_time": "2025-05-28T13:06:04.302828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 残差连接和层规范化\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,normalized_shape,dropout,**kwargs):\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.ln=nn.LayerNorm(normalized_shape)\n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        return self.ln(self.dropout(Y) + X)"
   ],
   "id": "d879ec795cb9e9c6",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:04.338980Z",
     "start_time": "2025-05-28T13:06:04.335204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_norm = AddNorm([3, 4], 0.5)\n",
    "add_norm.eval()"
   ],
   "id": "a66677e2919f8b59",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AddNorm(\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (ln): LayerNorm((3, 4), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:08.183901Z",
     "start_time": "2025-05-28T13:06:08.176877Z"
    }
   },
   "cell_type": "code",
   "source": "add_norm(torch.ones((2, 3, 4)), torch.ones((2, 3, 4))).shape",
   "id": "acc33c8074dfc490",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:08.542449Z",
     "start_time": "2025-05-28T13:06:08.536882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,use_bias=False,**kwargs):\n",
    "        super(EncoderBlock,self).__init__()\n",
    "        self.attention = MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout,bias=use_bias)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm2=AddNorm(norm_shape, dropout)\n",
    "        \n",
    "    def forward(self,X,valid_lens):\n",
    "        Y=self.addnorm1(X,self.attention(X,X,X,valid_lens))\n",
    "        return self.addnorm2(Y,self.ffn(Y))"
   ],
   "id": "d7cc7bfcb9c9e3e9",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:08.749636Z",
     "start_time": "2025-05-28T13:06:08.732692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x=torch.ones((2,100,24))\n",
    "valid_lens=torch.tensor([3,2])\n",
    "encoder_blk=EncoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=8,dropout=0.5)\n",
    "encoder_blk.eval()"
   ],
   "id": "b00281a03e505be3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderBlock(\n",
       "  (attention): MultiHeadAttention(\n",
       "    (attention): DotProductAttention(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (W_q): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_k): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_v): Linear(in_features=24, out_features=24, bias=False)\n",
       "    (W_o): Linear(in_features=24, out_features=24, bias=False)\n",
       "  )\n",
       "  (addnorm1): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (ffn): PositionWiseFFN(\n",
       "    (dense1): Linear(in_features=24, out_features=48, bias=True)\n",
       "    (relu): ReLU()\n",
       "    (dense2): Linear(in_features=48, out_features=24, bias=True)\n",
       "  )\n",
       "  (addnorm2): AddNorm(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (ln): LayerNorm((100, 24), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:09.292252Z",
     "start_time": "2025-05-28T13:06:09.280928Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk(x,valid_lens).shape",
   "id": "1ca9657b5a63e5bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:09.706440Z",
     "start_time": "2025-05-28T13:06:09.701109Z"
    }
   },
   "cell_type": "code",
   "source": "encoder_blk.attention.attention.attention_weights",
   "id": "e8ed2966391c7a71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.3333, 0.3333, 0.3333,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         ...,\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.5000, 0.5000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:10.107686Z",
     "start_time": "2025-05-28T13:06:10.101563Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens,\n",
    "                 num_heads, num_layers, dropout, use_bias=False, **kwargs):\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i)\n",
    "                                 ,EncoderBlock(key_size,query_size,value_size,num_hiddens\n",
    "                                               ,norm_shape,ffn_num_input,ffn_num_hiddens\n",
    "                                               ,num_heads,dropout,use_bias))\n",
    "    def forward(self,X,valid_lens,*args):\n",
    "        # 因为位置编码值在-1和1之间，\n",
    "        # 因此嵌入值乘以嵌入维度的平方根进行缩放， 因为嵌入层会把整个层的元素都压缩在均值为0方差为1的分布中 \n",
    "        # 因此当num_hiddens越大单个值会越小所以这么乘 保证每个元素也是在-1 1之间\n",
    "        # 然后再与位置编码相加。\n",
    "        X=self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self.attention_weights = [None] * len(self.blks)\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X = blk(X, valid_lens)\n",
    "            self.attention_weights[\n",
    "                i] = blk.attention.attention.attention_weights\n",
    "        return X"
   ],
   "id": "5e5c0f360463c19c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:10.656777Z",
     "start_time": "2025-05-28T13:06:10.643606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    200, 24, 24, 24, 24, [100, 24], 24, 48, 8, 2, 0.5)\n",
    "encoder.eval()\n",
    "encoder(torch.ones((2, 100), dtype=torch.long), valid_lens).shape"
   ],
   "id": "ac16a9590078dc5b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 24])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:11.161032Z",
     "start_time": "2025-05-28T13:06:11.153517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"解码器中的第i个块\"\"\"\n",
    "    def __init__(self,key_size,query_size,value_size,num_hiddens,norm_shape\n",
    "                 ,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i,**kwargs):\n",
    "        super(DecoderBlock,self).__init__()\n",
    "        self.i=i\n",
    "        self.attention1=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm1=AddNorm(norm_shape, dropout)\n",
    "        self.attention2=MultiHeadAttention(key_size,query_size,value_size,num_hiddens,num_heads,dropout)\n",
    "        self.addnorm2=AddNorm(norm_shape,dropout)\n",
    "        self.ffn=PositionWiseFFN(ffn_num_input,ffn_num_hiddens,num_hiddens,**kwargs)\n",
    "        self.addnorm3=AddNorm(norm_shape, dropout)\n",
    "    def forward(self,X,state):\n",
    "        \"\"\"X每次的输入 可能是初始输入也可能是上次的输出 形状B L H\n",
    "            state: 一个包含解码器运行状态的列表。\n",
    "            state[0]: enc_outputs - 编码器的最终输出。这个在整个解码过程中都是固定的。形状： (batch_size, enc_num_steps, num_hiddens)。\n",
    "            state[1]: enc_valid_lens - 编码器输入的有效长度。用于对编码器输出进行填充掩码。形状： (batch_size,)。\n",
    "            state[2]: 一个列表，用于存储每个解码器块在自注意力层中积累的键值对历史。这对于高效的**序列生成（推理）**至关重要。\n",
    "            \"\"\"\n",
    "        enc_outputs,enc_valid_lens=state[0],state[1]\n",
    "        if state[2][self.i] is None:\n",
    "            key_values=X\n",
    "        else:# 其实这里主要是推理时发挥作用 因为训练时传入的是一个完成的X 直接复制给key_values\n",
    "            # 这个key_values每次都会叠加 叠加的是本次的X\n",
    "            key_values=torch.cat((state[2][self.i],X),dim=1)\n",
    "        state[2][self.i]=key_values\n",
    "        if self.training:\n",
    "            batch_size,num_steps,_=X.shape\n",
    "            # dec_valid_lens的开头:(batch_size,num_steps),其中每一行是[1,2,...,num_steps] 处理后就变成 batch_size*num_steps的矩阵了\n",
    "            # 基于这个valid_lens矩阵，训练状态下会生成一个解码有效长度目的是最后得到一个下三角矩阵 保证每一步x只能关注到当前之前步的编码\n",
    "            dec_valid_lens=torch.arange(1,num_steps+1,device=X.device).repeat(batch_size,1)\n",
    "        else:\n",
    "            dec_valid_lens=None\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X,key_values,key_values,dec_valid_lens)\n",
    "        Y = self.addnorm1(X,X2)\n",
    "        # 编码器-解码器注意力\n",
    "        # enc_outputs 形状（batch_size,num_steps,num_hiddens）\n",
    "        Y2 = self.attention2(Y,enc_outputs,enc_outputs,enc_valid_lens)\n",
    "        Z = self.addnorm2(Y,Y2)\n",
    "        return self.addnorm3(Z,self.ffn(Z)),state\n"
   ],
   "id": "ea5997d91b547af8",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:11.622073Z",
     "start_time": "2025-05-28T13:06:11.611626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "decoder_blk=DecoderBlock(key_size=24,query_size=24,value_size=24,num_hiddens=24,norm_shape=[100,24],ffn_num_input=24,ffn_num_hiddens=48,num_heads=4,dropout=0.5,i=0)\n",
    "decoder_blk.eval()\n",
    "X=torch.ones((2,100,24))\n",
    "state=[encoder_blk(X,valid_lens),valid_lens,[None]]\n",
    "state[0].shape,decoder_blk(X, state)[0].shape"
   ],
   "id": "dd5db11c3b823262",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 100, 24]), torch.Size([2, 100, 24]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:12.138922Z",
     "start_time": "2025-05-28T13:06:12.133991Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,vocab_size,key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,**kwargs):\n",
    "        super(TransformerDecoder,self).__init__()\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.pos_encoding=PositionalEncoding(num_hiddens,dropout)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module('block'+str(i),\n",
    "                                 DecoderBlock(key_size,query_size,value_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,dropout,i))\n",
    "        self.dense=nn.Linear(num_hiddens,vocab_size)\n",
    "    def init_state(self,enc_outputs,enc_valid_lens,*args):\n",
    "        return [enc_outputs,enc_valid_lens,[None]*self.num_layers]\n",
    "    \n",
    "    def forward(self,X,state):\n",
    "        # 将X放大避免整个embeding 被位置编码的大小淹没 因为位置编码是-1 1之间\n",
    "        X = self.pos_encoding(self.embedding(X)*torch.sqrt(torch.tensor(self.num_hiddens)))\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]\n",
    "        for i,blk in enumerate(self.blks):\n",
    "            X,state = blk(X, state)\n",
    "            # 保留自注意力权重信息\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # 保留编码解码注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "            \n",
    "        return self.dense(X),state\n",
    "        \n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ],
   "id": "511684e7933b9191",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:12.568868Z",
     "start_time": "2025-05-28T13:06:12.565364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_hiddens,num_layers,dropout,batch_size,num_steps=32,2,0.1,64,10\n",
    "lr,num_epochs,device=0.005,200,'cpu'\n",
    "ffn_num_input=32\n",
    "ffn_num_hiddens=64\n",
    "num_heads=4\n",
    "key_size,query_size,value_size=32,32,32\n",
    "norm_shape=[32]"
   ],
   "id": "72414a099f78a9d2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:13.699569Z",
     "start_time": "2025-05-28T13:06:13.697468Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "c2bd7dfcb549bd5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:06:14.250128Z",
     "start_time": "2025-05-28T13:06:14.242988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoder = TransformerEncoder(\n",
    "    len(src_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)\n",
    "decoder = TransformerDecoder(\n",
    "    len(tgt_vocab), key_size, query_size, value_size, num_hiddens,\n",
    "    norm_shape, ffn_num_input, ffn_num_hiddens, num_heads,\n",
    "    num_layers, dropout)"
   ],
   "id": "837ddd29753f00c1",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:07:24.167962Z",
     "start_time": "2025-05-28T13:07:24.164264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(EncoderDecoder,self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    def forward(self,enc_x,dec_x,*args):\n",
    "        enc_outputs=self.encoder(enc_x,*args)\n",
    "        state=self.decoder.init_state(enc_outputs,*args)\n",
    "        dec_outputs=self.decoder(dec_x,state)\n",
    "        return dec_outputs"
   ],
   "id": "37096b9058598ca6",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:07:24.584449Z",
     "start_time": "2025-05-28T13:07:24.582333Z"
    }
   },
   "cell_type": "code",
   "source": "net = EncoderDecoder(encoder, decoder)",
   "id": "84b75ba3c700144b",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:07:25.037730Z",
     "start_time": "2025-05-28T13:07:25.033692Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"在序列中屏蔽不相关的项\"\"\"\n",
    "    # 首先x是二维的 最内层维度是句子长度 注意：是训练集所以才知道句子真实长度\n",
    "    # 拿出总的长度 得到长度\n",
    "    maxlen = X.shape[1]\n",
    "    # 然后用总长度生成一个1维的向量 使用函数扩展成2维以便与valid_len进行广播\n",
    "    mask = torch.unsqueeze(torch.arange(0, maxlen, dtype=torch.long), dim=0)\n",
    "    # mask在0维度扩充 valid在1维度扩充 因为每一个valid对应的是每一个x valid的数字其实是x的第二维向量\n",
    "    mask = (mask < torch.unsqueeze(valid_len, dim=1))  # 这里小于号就够了 因为<eos>所在位置的索引其实是valid_len-1\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "\n",
    "# 拓展的softmax因为对填充值进行softmax其实没有什么意义\n",
    "class MaskedSoftmaxCELoss(nn.CrossEntropyLoss):\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        weights = torch.ones_like(label)\n",
    "        weights = sequence_mask(weights, valid_len)\n",
    "        self.reduction = 'none'\n",
    "        pred = pred.permute(0, 2, 1)\n",
    "        # 交叉熵损失期望的两个输入 x是 batch_size vocab_size seq_lenth\n",
    "        # y 是 batch_size seq_len\n",
    "        unweight_loss = super().forward(pred, label)\n",
    "        weights_loss = unweight_loss * weights\n",
    "        return weights_loss.mean(dim=1)\n",
    "\n",
    "loss = MaskedSoftmaxCELoss()"
   ],
   "id": "d7f7b4799048fa41",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:41:40.235860Z",
     "start_time": "2025-05-28T13:40:00.891201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dplearning_second_part.limu_dplearning.utils.useful_func import grad_clipping\n",
    "from torch import optim\n",
    "\n",
    "# train\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)\n",
    "for epoch in range(300):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        src,src_valid,tgt,tgt_valid=batch\n",
    "        Y=torch.cat((torch.tensor([tgt_vocab['<bos>']]).unsqueeze(0).repeat(tgt.shape[0],1),tgt),dim=1)[:,:-1]\n",
    "        # 易错点1：训练的时候应该使用带有bos的Y 表示强制教学 此时输出的y_hat实际上是没有bos的\n",
    "        y_hat,_=net(src,Y,src_valid)\n",
    "        # 点2 因为输出的y_hat 没有bos 因此在计算loss的时候应该使用原始序列作为目标序列\n",
    "        l=loss(y_hat,tgt,tgt_valid).sum()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        optimizer.step()\n",
    "    print(l)"
   ],
   "id": "b996d767ef33940d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9272, grad_fn=<SumBackward0>)\n",
      "tensor(2.9312, grad_fn=<SumBackward0>)\n",
      "tensor(2.4040, grad_fn=<SumBackward0>)\n",
      "tensor(2.4498, grad_fn=<SumBackward0>)\n",
      "tensor(2.1853, grad_fn=<SumBackward0>)\n",
      "tensor(3.3349, grad_fn=<SumBackward0>)\n",
      "tensor(2.3908, grad_fn=<SumBackward0>)\n",
      "tensor(2.2177, grad_fn=<SumBackward0>)\n",
      "tensor(2.6928, grad_fn=<SumBackward0>)\n",
      "tensor(2.6504, grad_fn=<SumBackward0>)\n",
      "tensor(2.8375, grad_fn=<SumBackward0>)\n",
      "tensor(1.9400, grad_fn=<SumBackward0>)\n",
      "tensor(2.0029, grad_fn=<SumBackward0>)\n",
      "tensor(2.8400, grad_fn=<SumBackward0>)\n",
      "tensor(1.9115, grad_fn=<SumBackward0>)\n",
      "tensor(2.1252, grad_fn=<SumBackward0>)\n",
      "tensor(2.0076, grad_fn=<SumBackward0>)\n",
      "tensor(2.2717, grad_fn=<SumBackward0>)\n",
      "tensor(1.9876, grad_fn=<SumBackward0>)\n",
      "tensor(2.0505, grad_fn=<SumBackward0>)\n",
      "tensor(2.4716, grad_fn=<SumBackward0>)\n",
      "tensor(2.2849, grad_fn=<SumBackward0>)\n",
      "tensor(2.0221, grad_fn=<SumBackward0>)\n",
      "tensor(1.8951, grad_fn=<SumBackward0>)\n",
      "tensor(2.0657, grad_fn=<SumBackward0>)\n",
      "tensor(1.7130, grad_fn=<SumBackward0>)\n",
      "tensor(2.4220, grad_fn=<SumBackward0>)\n",
      "tensor(2.6796, grad_fn=<SumBackward0>)\n",
      "tensor(2.5937, grad_fn=<SumBackward0>)\n",
      "tensor(2.3424, grad_fn=<SumBackward0>)\n",
      "tensor(2.4610, grad_fn=<SumBackward0>)\n",
      "tensor(2.2872, grad_fn=<SumBackward0>)\n",
      "tensor(2.7056, grad_fn=<SumBackward0>)\n",
      "tensor(1.9415, grad_fn=<SumBackward0>)\n",
      "tensor(3.1210, grad_fn=<SumBackward0>)\n",
      "tensor(2.6450, grad_fn=<SumBackward0>)\n",
      "tensor(2.1831, grad_fn=<SumBackward0>)\n",
      "tensor(2.4729, grad_fn=<SumBackward0>)\n",
      "tensor(2.7675, grad_fn=<SumBackward0>)\n",
      "tensor(2.3814, grad_fn=<SumBackward0>)\n",
      "tensor(2.1588, grad_fn=<SumBackward0>)\n",
      "tensor(2.4644, grad_fn=<SumBackward0>)\n",
      "tensor(2.6961, grad_fn=<SumBackward0>)\n",
      "tensor(2.3065, grad_fn=<SumBackward0>)\n",
      "tensor(2.6314, grad_fn=<SumBackward0>)\n",
      "tensor(2.3149, grad_fn=<SumBackward0>)\n",
      "tensor(2.4204, grad_fn=<SumBackward0>)\n",
      "tensor(2.3494, grad_fn=<SumBackward0>)\n",
      "tensor(2.3170, grad_fn=<SumBackward0>)\n",
      "tensor(2.9318, grad_fn=<SumBackward0>)\n",
      "tensor(2.0424, grad_fn=<SumBackward0>)\n",
      "tensor(2.9759, grad_fn=<SumBackward0>)\n",
      "tensor(2.1801, grad_fn=<SumBackward0>)\n",
      "tensor(2.1220, grad_fn=<SumBackward0>)\n",
      "tensor(2.1959, grad_fn=<SumBackward0>)\n",
      "tensor(2.3497, grad_fn=<SumBackward0>)\n",
      "tensor(2.4957, grad_fn=<SumBackward0>)\n",
      "tensor(2.3854, grad_fn=<SumBackward0>)\n",
      "tensor(2.1314, grad_fn=<SumBackward0>)\n",
      "tensor(1.8660, grad_fn=<SumBackward0>)\n",
      "tensor(2.2645, grad_fn=<SumBackward0>)\n",
      "tensor(2.4256, grad_fn=<SumBackward0>)\n",
      "tensor(2.1890, grad_fn=<SumBackward0>)\n",
      "tensor(2.7279, grad_fn=<SumBackward0>)\n",
      "tensor(1.8580, grad_fn=<SumBackward0>)\n",
      "tensor(2.3710, grad_fn=<SumBackward0>)\n",
      "tensor(2.3270, grad_fn=<SumBackward0>)\n",
      "tensor(1.9712, grad_fn=<SumBackward0>)\n",
      "tensor(2.4136, grad_fn=<SumBackward0>)\n",
      "tensor(2.2469, grad_fn=<SumBackward0>)\n",
      "tensor(1.8249, grad_fn=<SumBackward0>)\n",
      "tensor(2.5356, grad_fn=<SumBackward0>)\n",
      "tensor(2.1146, grad_fn=<SumBackward0>)\n",
      "tensor(2.3596, grad_fn=<SumBackward0>)\n",
      "tensor(2.3226, grad_fn=<SumBackward0>)\n",
      "tensor(1.9855, grad_fn=<SumBackward0>)\n",
      "tensor(2.0723, grad_fn=<SumBackward0>)\n",
      "tensor(2.5153, grad_fn=<SumBackward0>)\n",
      "tensor(2.0090, grad_fn=<SumBackward0>)\n",
      "tensor(2.0181, grad_fn=<SumBackward0>)\n",
      "tensor(2.2130, grad_fn=<SumBackward0>)\n",
      "tensor(2.1957, grad_fn=<SumBackward0>)\n",
      "tensor(2.0012, grad_fn=<SumBackward0>)\n",
      "tensor(2.1374, grad_fn=<SumBackward0>)\n",
      "tensor(2.0984, grad_fn=<SumBackward0>)\n",
      "tensor(1.8726, grad_fn=<SumBackward0>)\n",
      "tensor(2.1950, grad_fn=<SumBackward0>)\n",
      "tensor(2.0149, grad_fn=<SumBackward0>)\n",
      "tensor(2.3226, grad_fn=<SumBackward0>)\n",
      "tensor(1.9680, grad_fn=<SumBackward0>)\n",
      "tensor(2.2400, grad_fn=<SumBackward0>)\n",
      "tensor(2.0063, grad_fn=<SumBackward0>)\n",
      "tensor(2.1697, grad_fn=<SumBackward0>)\n",
      "tensor(1.9780, grad_fn=<SumBackward0>)\n",
      "tensor(2.5091, grad_fn=<SumBackward0>)\n",
      "tensor(1.9310, grad_fn=<SumBackward0>)\n",
      "tensor(1.8310, grad_fn=<SumBackward0>)\n",
      "tensor(1.7846, grad_fn=<SumBackward0>)\n",
      "tensor(2.5816, grad_fn=<SumBackward0>)\n",
      "tensor(1.9373, grad_fn=<SumBackward0>)\n",
      "tensor(2.3187, grad_fn=<SumBackward0>)\n",
      "tensor(1.9218, grad_fn=<SumBackward0>)\n",
      "tensor(1.6990, grad_fn=<SumBackward0>)\n",
      "tensor(2.5756, grad_fn=<SumBackward0>)\n",
      "tensor(2.7547, grad_fn=<SumBackward0>)\n",
      "tensor(2.2171, grad_fn=<SumBackward0>)\n",
      "tensor(2.8935, grad_fn=<SumBackward0>)\n",
      "tensor(2.0851, grad_fn=<SumBackward0>)\n",
      "tensor(1.6489, grad_fn=<SumBackward0>)\n",
      "tensor(2.5763, grad_fn=<SumBackward0>)\n",
      "tensor(2.4571, grad_fn=<SumBackward0>)\n",
      "tensor(2.2717, grad_fn=<SumBackward0>)\n",
      "tensor(2.5792, grad_fn=<SumBackward0>)\n",
      "tensor(1.9445, grad_fn=<SumBackward0>)\n",
      "tensor(2.0447, grad_fn=<SumBackward0>)\n",
      "tensor(1.8363, grad_fn=<SumBackward0>)\n",
      "tensor(2.4356, grad_fn=<SumBackward0>)\n",
      "tensor(3.0726, grad_fn=<SumBackward0>)\n",
      "tensor(1.8229, grad_fn=<SumBackward0>)\n",
      "tensor(2.0604, grad_fn=<SumBackward0>)\n",
      "tensor(2.0717, grad_fn=<SumBackward0>)\n",
      "tensor(2.8373, grad_fn=<SumBackward0>)\n",
      "tensor(3.0264, grad_fn=<SumBackward0>)\n",
      "tensor(2.7113, grad_fn=<SumBackward0>)\n",
      "tensor(1.9855, grad_fn=<SumBackward0>)\n",
      "tensor(1.5909, grad_fn=<SumBackward0>)\n",
      "tensor(2.2594, grad_fn=<SumBackward0>)\n",
      "tensor(2.6772, grad_fn=<SumBackward0>)\n",
      "tensor(2.1942, grad_fn=<SumBackward0>)\n",
      "tensor(2.0383, grad_fn=<SumBackward0>)\n",
      "tensor(3.4432, grad_fn=<SumBackward0>)\n",
      "tensor(1.8731, grad_fn=<SumBackward0>)\n",
      "tensor(1.6609, grad_fn=<SumBackward0>)\n",
      "tensor(1.9275, grad_fn=<SumBackward0>)\n",
      "tensor(2.1146, grad_fn=<SumBackward0>)\n",
      "tensor(1.8758, grad_fn=<SumBackward0>)\n",
      "tensor(2.8554, grad_fn=<SumBackward0>)\n",
      "tensor(2.0996, grad_fn=<SumBackward0>)\n",
      "tensor(2.3848, grad_fn=<SumBackward0>)\n",
      "tensor(2.0553, grad_fn=<SumBackward0>)\n",
      "tensor(1.8679, grad_fn=<SumBackward0>)\n",
      "tensor(1.8923, grad_fn=<SumBackward0>)\n",
      "tensor(2.5056, grad_fn=<SumBackward0>)\n",
      "tensor(2.8362, grad_fn=<SumBackward0>)\n",
      "tensor(2.0882, grad_fn=<SumBackward0>)\n",
      "tensor(2.0637, grad_fn=<SumBackward0>)\n",
      "tensor(2.0762, grad_fn=<SumBackward0>)\n",
      "tensor(2.7000, grad_fn=<SumBackward0>)\n",
      "tensor(2.1538, grad_fn=<SumBackward0>)\n",
      "tensor(2.6205, grad_fn=<SumBackward0>)\n",
      "tensor(2.1811, grad_fn=<SumBackward0>)\n",
      "tensor(2.5528, grad_fn=<SumBackward0>)\n",
      "tensor(2.3035, grad_fn=<SumBackward0>)\n",
      "tensor(1.8957, grad_fn=<SumBackward0>)\n",
      "tensor(1.6749, grad_fn=<SumBackward0>)\n",
      "tensor(2.6919, grad_fn=<SumBackward0>)\n",
      "tensor(2.4579, grad_fn=<SumBackward0>)\n",
      "tensor(2.9429, grad_fn=<SumBackward0>)\n",
      "tensor(2.0875, grad_fn=<SumBackward0>)\n",
      "tensor(2.4517, grad_fn=<SumBackward0>)\n",
      "tensor(2.3565, grad_fn=<SumBackward0>)\n",
      "tensor(1.7114, grad_fn=<SumBackward0>)\n",
      "tensor(2.4424, grad_fn=<SumBackward0>)\n",
      "tensor(2.4193, grad_fn=<SumBackward0>)\n",
      "tensor(2.3134, grad_fn=<SumBackward0>)\n",
      "tensor(2.2554, grad_fn=<SumBackward0>)\n",
      "tensor(2.2823, grad_fn=<SumBackward0>)\n",
      "tensor(2.3433, grad_fn=<SumBackward0>)\n",
      "tensor(3.2877, grad_fn=<SumBackward0>)\n",
      "tensor(1.8105, grad_fn=<SumBackward0>)\n",
      "tensor(1.6556, grad_fn=<SumBackward0>)\n",
      "tensor(1.7006, grad_fn=<SumBackward0>)\n",
      "tensor(2.0319, grad_fn=<SumBackward0>)\n",
      "tensor(2.1560, grad_fn=<SumBackward0>)\n",
      "tensor(2.1401, grad_fn=<SumBackward0>)\n",
      "tensor(2.3910, grad_fn=<SumBackward0>)\n",
      "tensor(2.6384, grad_fn=<SumBackward0>)\n",
      "tensor(2.0039, grad_fn=<SumBackward0>)\n",
      "tensor(1.9816, grad_fn=<SumBackward0>)\n",
      "tensor(2.0776, grad_fn=<SumBackward0>)\n",
      "tensor(2.0884, grad_fn=<SumBackward0>)\n",
      "tensor(2.2206, grad_fn=<SumBackward0>)\n",
      "tensor(1.6387, grad_fn=<SumBackward0>)\n",
      "tensor(1.8183, grad_fn=<SumBackward0>)\n",
      "tensor(1.8689, grad_fn=<SumBackward0>)\n",
      "tensor(1.8695, grad_fn=<SumBackward0>)\n",
      "tensor(1.9560, grad_fn=<SumBackward0>)\n",
      "tensor(1.9192, grad_fn=<SumBackward0>)\n",
      "tensor(1.9689, grad_fn=<SumBackward0>)\n",
      "tensor(2.2399, grad_fn=<SumBackward0>)\n",
      "tensor(2.1048, grad_fn=<SumBackward0>)\n",
      "tensor(1.9690, grad_fn=<SumBackward0>)\n",
      "tensor(2.1960, grad_fn=<SumBackward0>)\n",
      "tensor(2.4734, grad_fn=<SumBackward0>)\n",
      "tensor(2.0225, grad_fn=<SumBackward0>)\n",
      "tensor(2.2652, grad_fn=<SumBackward0>)\n",
      "tensor(2.2597, grad_fn=<SumBackward0>)\n",
      "tensor(3.1844, grad_fn=<SumBackward0>)\n",
      "tensor(1.9135, grad_fn=<SumBackward0>)\n",
      "tensor(2.0414, grad_fn=<SumBackward0>)\n",
      "tensor(2.3708, grad_fn=<SumBackward0>)\n",
      "tensor(1.7397, grad_fn=<SumBackward0>)\n",
      "tensor(1.8209, grad_fn=<SumBackward0>)\n",
      "tensor(2.0377, grad_fn=<SumBackward0>)\n",
      "tensor(2.0898, grad_fn=<SumBackward0>)\n",
      "tensor(1.7387, grad_fn=<SumBackward0>)\n",
      "tensor(1.6312, grad_fn=<SumBackward0>)\n",
      "tensor(2.3745, grad_fn=<SumBackward0>)\n",
      "tensor(2.0393, grad_fn=<SumBackward0>)\n",
      "tensor(1.7560, grad_fn=<SumBackward0>)\n",
      "tensor(1.8057, grad_fn=<SumBackward0>)\n",
      "tensor(2.2655, grad_fn=<SumBackward0>)\n",
      "tensor(2.2041, grad_fn=<SumBackward0>)\n",
      "tensor(2.5835, grad_fn=<SumBackward0>)\n",
      "tensor(2.2445, grad_fn=<SumBackward0>)\n",
      "tensor(2.2383, grad_fn=<SumBackward0>)\n",
      "tensor(1.9674, grad_fn=<SumBackward0>)\n",
      "tensor(2.3959, grad_fn=<SumBackward0>)\n",
      "tensor(1.9514, grad_fn=<SumBackward0>)\n",
      "tensor(2.0148, grad_fn=<SumBackward0>)\n",
      "tensor(1.7968, grad_fn=<SumBackward0>)\n",
      "tensor(1.7532, grad_fn=<SumBackward0>)\n",
      "tensor(2.4776, grad_fn=<SumBackward0>)\n",
      "tensor(1.6906, grad_fn=<SumBackward0>)\n",
      "tensor(1.9733, grad_fn=<SumBackward0>)\n",
      "tensor(2.2066, grad_fn=<SumBackward0>)\n",
      "tensor(1.7141, grad_fn=<SumBackward0>)\n",
      "tensor(2.0742, grad_fn=<SumBackward0>)\n",
      "tensor(1.7450, grad_fn=<SumBackward0>)\n",
      "tensor(1.8295, grad_fn=<SumBackward0>)\n",
      "tensor(1.8747, grad_fn=<SumBackward0>)\n",
      "tensor(2.1601, grad_fn=<SumBackward0>)\n",
      "tensor(2.0791, grad_fn=<SumBackward0>)\n",
      "tensor(2.0786, grad_fn=<SumBackward0>)\n",
      "tensor(1.8930, grad_fn=<SumBackward0>)\n",
      "tensor(2.0080, grad_fn=<SumBackward0>)\n",
      "tensor(2.1743, grad_fn=<SumBackward0>)\n",
      "tensor(2.0318, grad_fn=<SumBackward0>)\n",
      "tensor(1.9518, grad_fn=<SumBackward0>)\n",
      "tensor(1.8075, grad_fn=<SumBackward0>)\n",
      "tensor(1.9234, grad_fn=<SumBackward0>)\n",
      "tensor(2.1081, grad_fn=<SumBackward0>)\n",
      "tensor(2.1039, grad_fn=<SumBackward0>)\n",
      "tensor(1.9546, grad_fn=<SumBackward0>)\n",
      "tensor(2.7752, grad_fn=<SumBackward0>)\n",
      "tensor(2.1898, grad_fn=<SumBackward0>)\n",
      "tensor(1.9231, grad_fn=<SumBackward0>)\n",
      "tensor(2.0279, grad_fn=<SumBackward0>)\n",
      "tensor(1.9340, grad_fn=<SumBackward0>)\n",
      "tensor(2.4778, grad_fn=<SumBackward0>)\n",
      "tensor(1.7630, grad_fn=<SumBackward0>)\n",
      "tensor(2.1607, grad_fn=<SumBackward0>)\n",
      "tensor(1.8760, grad_fn=<SumBackward0>)\n",
      "tensor(2.2380, grad_fn=<SumBackward0>)\n",
      "tensor(2.4321, grad_fn=<SumBackward0>)\n",
      "tensor(1.9097, grad_fn=<SumBackward0>)\n",
      "tensor(1.7647, grad_fn=<SumBackward0>)\n",
      "tensor(1.4437, grad_fn=<SumBackward0>)\n",
      "tensor(2.3039, grad_fn=<SumBackward0>)\n",
      "tensor(1.6282, grad_fn=<SumBackward0>)\n",
      "tensor(2.5743, grad_fn=<SumBackward0>)\n",
      "tensor(2.1906, grad_fn=<SumBackward0>)\n",
      "tensor(2.5280, grad_fn=<SumBackward0>)\n",
      "tensor(1.9788, grad_fn=<SumBackward0>)\n",
      "tensor(2.3451, grad_fn=<SumBackward0>)\n",
      "tensor(2.0517, grad_fn=<SumBackward0>)\n",
      "tensor(2.1920, grad_fn=<SumBackward0>)\n",
      "tensor(2.2827, grad_fn=<SumBackward0>)\n",
      "tensor(1.7968, grad_fn=<SumBackward0>)\n",
      "tensor(2.1551, grad_fn=<SumBackward0>)\n",
      "tensor(2.3599, grad_fn=<SumBackward0>)\n",
      "tensor(2.1926, grad_fn=<SumBackward0>)\n",
      "tensor(1.8861, grad_fn=<SumBackward0>)\n",
      "tensor(1.7241, grad_fn=<SumBackward0>)\n",
      "tensor(2.2252, grad_fn=<SumBackward0>)\n",
      "tensor(2.3982, grad_fn=<SumBackward0>)\n",
      "tensor(2.1818, grad_fn=<SumBackward0>)\n",
      "tensor(2.0897, grad_fn=<SumBackward0>)\n",
      "tensor(2.1301, grad_fn=<SumBackward0>)\n",
      "tensor(1.9413, grad_fn=<SumBackward0>)\n",
      "tensor(1.5749, grad_fn=<SumBackward0>)\n",
      "tensor(2.3778, grad_fn=<SumBackward0>)\n",
      "tensor(2.5152, grad_fn=<SumBackward0>)\n",
      "tensor(2.2839, grad_fn=<SumBackward0>)\n",
      "tensor(1.8974, grad_fn=<SumBackward0>)\n",
      "tensor(1.7042, grad_fn=<SumBackward0>)\n",
      "tensor(2.2190, grad_fn=<SumBackward0>)\n",
      "tensor(2.5717, grad_fn=<SumBackward0>)\n",
      "tensor(1.8895, grad_fn=<SumBackward0>)\n",
      "tensor(2.1008, grad_fn=<SumBackward0>)\n",
      "tensor(2.1246, grad_fn=<SumBackward0>)\n",
      "tensor(2.2010, grad_fn=<SumBackward0>)\n",
      "tensor(2.9860, grad_fn=<SumBackward0>)\n",
      "tensor(1.8295, grad_fn=<SumBackward0>)\n",
      "tensor(2.0972, grad_fn=<SumBackward0>)\n",
      "tensor(2.1827, grad_fn=<SumBackward0>)\n",
      "tensor(2.2946, grad_fn=<SumBackward0>)\n",
      "tensor(2.6902, grad_fn=<SumBackward0>)\n",
      "tensor(2.3949, grad_fn=<SumBackward0>)\n",
      "tensor(2.2444, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:43:36.259813Z",
     "start_time": "2025-05-28T13:43:36.217175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dplearning_second_part.limu_dplearning.utils.useful_func import truncate_pad\n",
    "net.eval()\n",
    "# predict\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "for src_sentence,tgt_sentence in zip(engs,fras):\n",
    "    src_tokens=[src_vocab[i] for i in src_sentence.split(' ')]+[src_vocab['<eos>']]\n",
    "    src_data=truncate_pad(src_tokens,10,1)\n",
    "    # 易错点 srcdata在生成完成后需要unsqueeze 因为原本是1维的需要增加一个批次维度\n",
    "    src_data=torch.tensor(src_data).unsqueeze(0)\n",
    "    # 易错点 需要加上这个批次的有效长度建议1维 或者无维度\n",
    "    enc_valid_len=torch.tensor([len(src_tokens)])\n",
    "\n",
    "    enc_outputs=net.encoder(src_data,enc_valid_len)\n",
    "    state=net.decoder.init_state(enc_outputs,enc_valid_len)\n",
    "    # 易错点 这里必须得是long 因为embed层需要long输入 并且要unsequeeze 增加一个维度\n",
    "    dec_x=torch.tensor([tgt_vocab['<bos>']],dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    output_list=[]\n",
    "    for i in range(10):\n",
    "        output,state=net.decoder(dec_x,state)\n",
    "        # 易错点 需要用这一次的输出argmax之后 作为下一次的输入 因为输入必须得是long\n",
    "        dec_x=torch.argmax(output,dim=-1)\n",
    "\n",
    "        output_list.append(dec_x.squeeze(0).squeeze(0))\n",
    "        if tgt_vocab['<eos>']==torch.argmax(output,dim=-1).squeeze(0):\n",
    "            break\n",
    "    print([tgt_vocab.idx_to_token[i] for i in output_list])"
   ],
   "id": "81cc56cdfd972945",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['va', '!', '<eos>']\n",
      "[\"j'ai\", 'perdu', '.', '<eos>']\n",
      "['il', 'est', 'calme', '.', '<eos>']\n",
      "['je', 'suis', 'chez', 'moi', '.', '<eos>']\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-28T13:10:56.069433Z",
     "start_time": "2025-05-28T13:10:56.067659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# bert\n",
    "def get_token_and_segments(tokens_a,tokens_b=None):\n",
    "    \"\"\"获取bert输入的token以及对应的序列编号\"\"\"\n",
    "    # 前面加一个cls 后面加一个sep\n",
    "    tokens=['<cls>']+tokens_a +['<sep>']\n",
    "    segments=[0]*len(tokens)\n",
    "    if tokens_b is not None:\n",
    "        tokens+=tokens_b+['<sep>']\n",
    "        segments+=[1]*len(tokens_b+1)\n",
    "    return tokens,segments"
   ],
   "id": "8825161c094860e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self,vocab_size,num_hiddens,norm_shape,ffn_num_input,ffn_num_hiddens,num_heads,num_layers,dropout,max_len=1000,key_size=768,query_size=768,value_size=768):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embedding=nn.Embedding(vocab_size,num_hiddens)\n",
    "        self.segment_embedding=nn.Embedding(2,num_hiddens)\n",
    "        self.blks=nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f'{i}',EncoderBlock(\n",
    "                key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                ffn_num_input, ffn_num_hiddens, num_heads, dropout, True))\n",
    "        # 在BERT中，位置嵌入是可学习的，因此我们创建一个足够长的位置嵌入参数\n",
    "        self.pos_embedding=nn.Parameter(torch.randn(size=(1,max_len,num_hiddens)))\n",
    "\n",
    "    def forward(self,tokens,segments,valid_len):\n",
    "\n"
   ],
   "id": "c72ed2a0a391435e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
