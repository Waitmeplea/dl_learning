{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1梯度   \n",
    "连结一个多元函数对其所有变量的偏导数，以得到该函数的梯度（gradient）向量。   \n",
    "具体而言，设函数f：Rn—R的输入是一个\n",
    "n维向量，但输出是一个标量。 函数fx相对于x的梯度是一个包含n个偏导数的向量:"
   ],
   "id": "3c9c3fc2bdba3ff0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "其中 $\\nabla_x f(x)$ 通常在没有歧义时被 $\\nabla f(x)$ 取代。\n",
    "\n",
    "假设 $x$ 为 $n$ 维向量，在微分多元函数时经常使用以下规则：\n",
    "\n",
    "1. **规则 1**: 对于所有 $A \\in \\mathbb{R}^{m \\times n}$，都有  \n",
    "   $$\\nabla_x (A x) = A^T$$  \n",
    "   **注**: 此处存在笔误，正确结果应为 $(A + A^T)x$（当 $A$ 对称时退化为 $2Ax$）。\n",
    "2. **规则 2**: 对于所有 $A \\in \\mathbb{R}^{n \\times m}$，都有  \n",
    "   $$\\nabla_x (x^T A) = A$$\n",
    "3. **规则 3**: 对于所有 $A \\in \\mathbb{R}^{n \\times n}$，都有  \n",
    "   $$\\nabla_x (x^T A x) = (A + A^T)x$$\n",
    "4. **规则 4**: 向量的平方范数  \n",
    "   $$\\nabla_x \\|x\\|^2 = \\nabla_x (x^T x) = 2x$$\n",
    "---\n",
    "同样，对于任何矩阵 $X$，都有：  \n",
    "$$\\nabla_X \\|X\\|_F^2 = 2X$$  "
   ],
   "id": "ca7c1419ebfd2bdd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2自动微分",
   "id": "68c964ff999217e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T00:59:56.915413Z",
     "start_time": "2025-04-03T00:59:56.909849Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "547dc8df479d8e55",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T00:59:57.481246Z",
     "start_time": "2025-04-03T00:59:57.464858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4.0)\n",
    "#在我们计算关于x的梯度之前，需要一个地方来存储梯度。 重要的是，我们不会在每次对一个参数求导时都分配新的内存。\n",
    "# 因为我们经常会成千上万次地更新相同的参数，每次都分配新的内存可能很快就会将内存耗尽。 \n",
    "# 注意，一个标量函数关于向量的梯度是向量，并且与具有相同的形状。"
   ],
   "id": "ca801b009a0eef2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在PyTorch中，`x.requires_grad_(True)`用于启用张量`x`的梯度跟踪功能。以下是其作用及底层机制的详细说明：\n",
    "### **1. 核心作用**\n",
    "- **启用梯度跟踪**：设置`x.requires_grad = True`后，PyTorch的Autograd系统会开始追踪所有涉及`x`的操作，构建动态计算图。\n",
    "- **自动求导的基础**：只有被标记为需要梯度的张量，才能在反向传播时自动计算其梯度（通过`.backward()`方法）。\n",
    "### **2. 底层机制**\n",
    "#### **(1) 设置标志位**\n",
    "- **内部属性**：`requires_grad`是张量的一个布尔属性，默认值为`False`。\n",
    "- **就地修改**：`x.requires_grad_(True)`是一个就地（in-place）操作，直接修改张量`x`的`requires_grad`属性为`True`。\n",
    "#### **(2) 构建计算图**\n",
    "- **操作记录**：当`x`参与任何数学运算（如加减乘除、矩阵乘法等）时，PyTorch会记录这些操作，并生成一个动态计算图。\n",
    "- **依赖关系**：计算图记录了从输入（`x`）到输出（如损失函数）的所有中间步骤，为反向传播提供路径。\n",
    "#### **(3) 梯度计算**\n",
    "- **反向传播触发**：调用`.backward()`时，Autograd会根据计算图，从输出向输入反向传播，利用链式法则计算梯度。\n",
    "- **梯度存储**：计算出的梯度会存储在张量的`.grad`属性中（如`x.grad`）。\n",
    "---\n",
    "#### **(2) 禁用梯度跟踪**\n",
    "```python\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    y = x * 2  # y.requires_grad = False\n",
    "y.backward()  # 报错：y无梯度跟踪\n",
    "```\n",
    "### **6. 总结**\n",
    "- **核心功能**：`x.requires_grad_(True)`激活梯度跟踪，是PyTorch自动求导的基础。\n",
    "- **底层实现**：通过设置标志位和动态构建计算图实现。"
   ],
   "id": "a532c430b9ebc318"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:31:54.939593Z",
     "start_time": "2025-04-03T01:31:54.920825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test=(torch.tensor([1,2,3],dtype=float)).requires_grad_(True)\n",
    "test\n",
    "##当启用梯度追踪的时候 张量里会有一个属性requires_grad=True 可以访问\n",
    "test.requires_grad"
   ],
   "id": "c63ace091be9914",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:05:33.174869Z",
     "start_time": "2025-04-03T01:05:33.169378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#启用torch的梯度跟踪\n",
    "x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True)\n",
    "x.grad  # 默认值是None"
   ],
   "id": "502b8485e40e8d6a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:06:06.096776Z",
     "start_time": "2025-04-03T01:06:06.087570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##定义一个fx 因变量为y\n",
    "y = 2 * torch.dot(x, x)\n",
    "y"
   ],
   "id": "61021af6d6b3737c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:06:32.475174Z",
     "start_time": "2025-04-03T01:06:32.463943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#接下来，通过调用反向传播函数来自动计算y关于x每个分量的梯度，并打印这些梯度。\n",
    "y.backward()\n",
    "x.grad"
   ],
   "id": "f80280d98c441e54",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "计算图不会累计，每次前向传播时旧图会被释放  \n",
    "梯度会累积：当需要计算x的另一个函数的时候 需要清除梯度，如果不清零会影响新的关于x的函数梯度计算"
   ],
   "id": "243b639e39eb8e08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:25:17.167564Z",
     "start_time": "2025-04-03T01:25:17.154052Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#清除梯度\n",
    "##带下划线的代表就地操作 无zero\n",
    "x.grad.zero_()"
   ],
   "id": "3d59374f068dd91a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 分离计算",
   "id": "574439bca606ce25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-03T01:28:53.359779Z",
     "start_time": "2025-04-03T01:28:53.340242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#例如，假设y是作为x的函数计算的，而z则是作为y和x的函数计算的。 想象一下，我们想计算z关于x的梯度，但由于某种原因，希望将y视为一个常数， 并且只考虑到x在y被计算后发挥的作用。\n",
    "#这里可以分离y来返回一个新变量u，该变量与y具有相同的值， 但丢弃计算图中如何计算y的任何信息。 \n",
    "# 换句话说，梯度不会向后流经u到x。 因此，下面的反向传播函数计算z=u*x关于x的偏导数，同时将u作为常数处理， 而不是z=x*x*x关于x的偏导数。\n",
    "x.grad.zero_()\n",
    "y = x * x\n",
    "u = y.detach()\n",
    "z = u * x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad == u\n",
    "\n",
    "###使用y.detach() 对y的进行分离"
   ],
   "id": "37c218ea1dcf8ed7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 使用自动微分的一个好处是： 即使构建函数的计算图需要通过Python控制流（例如，条件、循环或任意函数调用），我们仍然可以计算得到的变量的梯度。 在下面的代码中，while循环的迭代次数和if语句的结果都取决于输入a的值。",
   "id": "21c6b6bb08ba7d0a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "49fad64d4e69bcee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
