{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-01T07:40:15.314287Z",
     "start_time": "2025-05-01T07:40:15.311758Z"
    }
   },
   "source": [
    "import torch\n",
    "from matplotlib.pyplot import xlabel, yscale, xscale\n",
    "from torch import nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import collections\n",
    "import re\n",
    "import requests\n",
    "import random \n",
    "from torch.nn import functional as F"
   ],
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:40:16.239746Z",
     "start_time": "2025-05-01T07:40:16.228974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('../data/time_machine.txt','r') as f:\n",
    "    content=f.readlines()\n",
    "##读取文章\n",
    "def read_time_machine():\n",
    "    with open('../data/time_machine.txt','r') as f:\n",
    "        content=f.readlines()\n",
    "    return [ re.sub('[^A-Za-z]', ' ', i.replace('\\n','')).strip().lower() for i in content ]\n",
    "\n",
    "## 定义一个拆分词元的函数 结果是词元组成的list\n",
    "def tokenize(content,token='word'):\n",
    "    if token=='word':\n",
    "        token_list=[token.lower() for i in content for token in i.split(' ')]\n",
    "    else:\n",
    "        token_list=[token.lower() for i in content for token in i]\n",
    "    return token_list\n",
    "\n",
    "##定义一个统计频率的函数 可以处理1d2d\n",
    "def count_corpus(token_list):\n",
    "    if isinstance(token_list[0], list):\n",
    "      tokens=[token for i in token_list for token in i ]\n",
    "    tokens_count=collections.Counter(token_list)\n",
    "    return tokens_count\n",
    "\n",
    "class Vocal():\n",
    "    def __init__(self,token_list=None,min_feq=0,reserved_tokens=None):\n",
    "        self.token_list=token_list\n",
    "        if token_list is None:\n",
    "            self.token_list=[]\n",
    "        if reserved_tokens is None:\n",
    "            reserved_tokens=[]\n",
    "        counter_info=count_corpus(token_list)\n",
    "        self._token_feq=[]\n",
    "        ##只接受符合条件的词\n",
    "        for items in sorted(counter_info.items(),key=lambda x:x[1],reverse=True):\n",
    "            if items[1]<=min_feq:\n",
    "                break\n",
    "            else:\n",
    "                self._token_feq.append(items)\n",
    "\n",
    "        ##\n",
    "        if '<unk>' in reserved_tokens:\n",
    "            self.idx_to_token=reserved_tokens\n",
    "        else:\n",
    "            self.idx_to_token=['unk']+reserved_tokens\n",
    "\n",
    "        self.token_to_idx={token:i for i,token in enumerate(self.idx_to_token)}\n",
    "        for token,_ in self._token_feq:\n",
    "            self.idx_to_token.append(token)\n",
    "            self.token_to_idx[token]=len(self.token_to_idx)\n",
    "    def __len__(self):\n",
    "        return len(self.token_to_idx)\n",
    "    ##实现一个索引方法 但是传入的索引是token 返回是idx,保证未知token显示0值\n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens,(tuple,list)):\n",
    "            return self.token_to_idx.get(tokens,0)\n",
    "        return [self.__getitem__(i) for i in tokens]\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return 0\n",
    "\n",
    "    @property\n",
    "    def tokens_freq(self):\n",
    "        return self._token_feq\n",
    "\n",
    "def load_corpus_time_machine(max_tokens=-1):  #@save\n",
    "    \"\"\"返回时光机器数据集的词元索引列表和词表\"\"\"\n",
    "    tokens=tokenize(read_time_machine(),'char')\n",
    "    vocal=Vocal(tokens)\n",
    "\n",
    "    # 因为时光机器数据集中的每个文本行不一定是一个句子或一个段落，\n",
    "    # 所以将所有文本行展平到一个列表中\n",
    "    corpus = [vocal[token] for line in tokens for token in line]\n",
    "    if max_tokens > 0:\n",
    "        corpus = corpus[:max_tokens]\n",
    "    return corpus, vocal\n",
    "\n",
    "##产生随机序列\n",
    "def seq_data_iter_random(corpus,batch_size,num_steps):\n",
    "    \"\"\"使用随机抽样生成一个小批量子序列\"\"\"\n",
    "    ##randint是左闭右闭 所以必须得减一不然0和numsteps实际上是重复的\n",
    "    corpus=corpus[random.randint(0,num_steps-1):]\n",
    "    ##因为y要比x多一位 一共可以有这么多子序列\n",
    "    num_subseqs=(len(corpus)-1)//num_steps\n",
    "\n",
    "    ## 然后找出每一个num seq的起始索引拿出来放到列表里 这里不能用corpus的长度 而是num_steps*num_subseqs\n",
    "    seq_start_index=[i for i in range(0,num_steps*num_subseqs,num_steps)]\n",
    "    ##打乱索引\n",
    "    random.shuffle(seq_start_index)\n",
    "\n",
    "    ## 再除以batchsize 有多少个batch\n",
    "    num_batch=num_subseqs//batch_size\n",
    "    ### 然后给每个batch找x和y 索引起始位置在seq_start_index里\n",
    "    for i in range(num_batch):\n",
    "        index_list=seq_start_index[i*batch_size:(i+1)*batch_size]\n",
    "        x=[]\n",
    "        y=[]\n",
    "        for _index in index_list:\n",
    "           x.append(corpus[_index:_index+num_steps])\n",
    "           y.append(corpus[_index+1:_index+1+num_steps])\n",
    "        yield torch.tensor(x),torch.tensor(y)\n",
    "\n",
    "# def seq_data_iter_sequential(corpus,batch_size,num_steps):\n",
    "#     random.seed(42)\n",
    "#     corpus=corpus[random.randint(0,num_steps):]\n",
    "#     batch_num=(len(corpus)-1)//(batch_size*num_steps)\n",
    "#     Xs=torch.tensor(corpus[:batch_num*batch_size*num_steps]).reshape(-1,batch_size,num_steps)\n",
    "#     Ys=torch.tensor(corpus[1:batch_num*batch_size*num_steps+1]).reshape(-1,batch_size,num_steps)\n",
    "#     for i in range(batch_num):\n",
    "#         yield Xs[i],Ys[i]\n",
    "# s1=seq_data_iter_sequential(corpus,batch_size,num_steps)\n",
    "# 这个方案是错的 连续要求在不同的batch上保持连续 而不是在一个batch的多个样本上保持连续 并且 batch_num=(len(corpus)-1)//(batch_size*num_steps) 这样也不太好\n",
    "#因为batch_num 计算方式耦合了 batch_size 和 num_steps，\n",
    "\n",
    "\n",
    "def seq_data_iter_sequential(corpus,batch_size,num_steps):\n",
    "    random.seed(0)\n",
    "    corpus=corpus[random.randint(0,num_steps):]\n",
    "    num_tokens=(len(corpus)-1)//batch_size*batch_size ## 保证是batch_size的倍数\n",
    "    ###batch_size需要放在最外维度：常规样本维度安排 外层是样本个数 也就是batch_size\n",
    "    Xs=torch.tensor(corpus[:num_tokens]).reshape(batch_size,-1)\n",
    "    Ys=torch.tensor(corpus[1:num_tokens+1]).reshape(batch_size,-1)\n",
    "    ##维度1是每个batchsize有多少token\n",
    "    batch_num=Xs.shape[1]//num_steps\n",
    "    for i in range(batch_num):\n",
    "        yield Xs[:,i*num_steps:(i+1)*num_steps],Ys[:,i*num_steps:(i+1)*num_steps]\n"
   ],
   "id": "1789bd52fa52c4f",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:40:18.228967Z",
     "start_time": "2025-05-01T07:40:18.178774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus,vocal=load_corpus_time_machine()\n",
    "batch_size,num_steps=5,10"
   ],
   "id": "224589d581a3a3ad",
   "outputs": [],
   "execution_count": 154
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:41:34.297149Z",
     "start_time": "2025-05-01T07:41:34.293980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SeqDataLoader:  #@save\n",
    "    def __init__(self,batch_size,num_steps,use_random_iter,max_token):\n",
    "        if use_random_iter:\n",
    "            self.data_iter_fn=seq_data_iter_random\n",
    "        else:\n",
    "            self.data_iter_fn=seq_data_iter_sequential\n",
    "        self.corpus,self.vocal=load_corpus_time_machine(max_token)\n",
    "        self.batch_size,self.num_steps = batch_size,num_steps\n",
    "\n",
    "    ##__iter__方法使得整个类变成可迭代的\n",
    "    def __iter__(self):\n",
    "        return self.data_iter_fn(self.corpus,self.batch_size,self.num_steps)"
   ],
   "id": "f28981c05f3e720d",
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:41:34.748363Z",
     "start_time": "2025-05-01T07:41:34.745398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data_time_machine(batch_size,num_steps,use_random_iter=True,max_token=-1):\n",
    "    data_iter=SeqDataLoader(batch_size,num_steps,use_random_iter,max_token)\n",
    "    return data_iter,data_iter.vocal"
   ],
   "id": "8a0b05c58d45bfc4",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-01T07:41:35.278143Z",
     "start_time": "2025-05-01T07:41:35.235270Z"
    }
   },
   "cell_type": "code",
   "source": "s=load_data_time_machine(10,5)",
   "id": "79f52106262ec1df",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "35b5e9fb24a6029a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
