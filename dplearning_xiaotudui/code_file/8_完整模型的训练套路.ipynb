{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 注意一些细节",
   "id": "42b97b75dcd21f5a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-01T00:24:45.121806Z",
     "start_time": "2025-04-01T00:24:43.178851Z"
    }
   },
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from self_model import * "
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T00:26:17.558783Z",
     "start_time": "2025-04-01T00:24:45.122821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data=torchvision.datasets.CIFAR10(root='../data',train=True,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "test_data=torchvision.datasets.CIFAR10(root='../data',train=False,transform=torchvision.transforms.ToTensor(),download=True)\n",
    "print('训练集数据量大小为{}'.format(len(train_data)))\n",
    "print('测试集数据量大小为{}'.format(len(test_data)))"
   ],
   "id": "177d54ba7e462a1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [01:22<00:00, 2.06MB/s] \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:04:33.837602Z",
     "start_time": "2025-04-01T03:04:33.818252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader=DataLoader(train_data,batch_size=64,shuffle=True)\n",
    "test_dataloader=DataLoader(test_data,batch_size=64,shuffle=True)"
   ],
   "id": "ab3a8dbab62ae2d3",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:04:34.299523Z",
     "start_time": "2025-04-01T03:04:34.285832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "net=Net()\n",
    "##损失函数\n",
    "loss_func=nn.CrossEntropyLoss()\n",
    "##优化器\n",
    "learning_rate=1e-2\n",
    "optm=torch.optim.SGD(net.parameters(),lr=learning_rate,momentum=0.9)"
   ],
   "id": "554fb1fa64bffaab",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T03:05:37.605166Z",
     "start_time": "2025-04-01T03:04:34.881579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "###设置训练时的一些参数\n",
    "total_train_step=0\n",
    "total_test_step=0\n",
    "##训练的轮数\n",
    "epoch=10\n",
    "for i in range(epoch):\n",
    "    print('-------------------第{}轮训练开始-------------------'.format(i))\n",
    "    for data in train_dataloader:\n",
    "        imgs,label=data\n",
    "        optm.zero_grad()\n",
    "        out_put=net.forward(imgs)\n",
    "        loss=loss_func(out_put,label)\n",
    "        loss.backward()\n",
    "        optm.step()\n",
    "        \n",
    "        if total_train_step%100==0:\n",
    "            print('训练次数{},损失值{}'.format(total_train_step,loss.item()))\n",
    "        total_train_step+=1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_test_loss=0\n",
    "        total_tes_acc=0\n",
    "        for data in test_dataloader:\n",
    "            imgs,label=data\n",
    "            out_put=net.forward(imgs)\n",
    "            loss=loss_func(out_put,label)\n",
    "            total_test_loss+=loss.item()\n",
    "            total_test_step+=1\n",
    "            total_tes_acc+=(out_put.argmax(dim=1)==label).sum()\n",
    "        print('整体数据损失{}'.format(total_test_loss))\n",
    "        print('整体数据正确率{}'.format(total_tes_acc/len(test_data)))\n",
    "        total_test_step+=1\n",
    "    torch.save(net.state_dict(),'model_{}.pth'.format(i))\n",
    "    print('模型已保存{}'.format(total_test_loss))\n",
    "####  可以使用tensorboard来可视化训练过程中数据，和训练效果"
   ],
   "id": "6257eb7766c4f70b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------第0轮训练开始-------------------\n",
      "训练次数0,损失值2.3045668601989746\n",
      "训练次数100,损失值1.8871444463729858\n",
      "训练次数200,损失值1.7419120073318481\n",
      "训练次数300,损失值1.6242985725402832\n",
      "训练次数400,损失值1.5689527988433838\n",
      "训练次数500,损失值1.5177866220474243\n",
      "训练次数600,损失值1.390172004699707\n",
      "训练次数700,损失值1.680038571357727\n",
      "整体数据损失208.696240067482\n",
      "整体数据正确率0.5342000126838684\n",
      "模型已保存208.696240067482\n",
      "-------------------第1轮训练开始-------------------\n",
      "训练次数800,损失值1.193179726600647\n",
      "训练次数900,损失值1.2019984722137451\n",
      "训练次数1000,损失值1.2389649152755737\n",
      "训练次数1100,损失值1.1770185232162476\n",
      "训练次数1200,损失值1.1630011796951294\n",
      "训练次数1300,损失值0.9772892594337463\n",
      "训练次数1400,损失值1.1520715951919556\n",
      "训练次数1500,损失值1.1956537961959839\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[47], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m out_put\u001B[38;5;241m=\u001B[39mnet\u001B[38;5;241m.\u001B[39mforward(imgs)\n\u001B[0;32m     12\u001B[0m loss\u001B[38;5;241m=\u001B[39mloss_func(out_put,label)\n\u001B[1;32m---> 13\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m optm\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m total_train_step\u001B[38;5;241m%\u001B[39m\u001B[38;5;241m100\u001B[39m\u001B[38;5;241m==\u001B[39m\u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    616\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    617\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    618\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    619\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    624\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    625\u001B[0m     )\n\u001B[1;32m--> 626\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    627\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    628\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\work softwar\\python\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    821\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    822\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 823\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    824\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    825\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    826\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    827\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 模型.train() 和模型.eval() 这两个方法实际上是针对特定的层才有意义，例如dropout 和bn层，当train时就开始训练，eval的时候则使用原始模型执行",
   "id": "a8da8f767f779d63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ece14952355907"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
