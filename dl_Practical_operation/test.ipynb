{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:16:27.830110Z",
     "start_time": "2025-06-19T12:16:27.493666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tokenizers\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "plt.ion()  # 交互模式\n",
    "device=torch.device('cuda:0')"
   ],
   "id": "e9e17fe40ffa0444",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:17:41.148924Z",
     "start_time": "2025-06-19T12:16:34.873929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载 IMDB 数据集\n",
    "cache_dir=r'.\\data_file'\n",
    "imdb_dataset = load_dataset(\"imdb\",cache_dir=cache_dir)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, data_type):\n",
    "        super(IMDBDataset, self).__init__()\n",
    "        self.dataset=imdb_dataset[data_type]\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index]\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ],
   "id": "89a1a3965201f17d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since imdb couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'plain_text' at data_file\\imdb\\plain_text\\0.0.0\\e6281661ce1c48d982bc483cf8a173c1bbeb5d31 (last modified on Thu Jun  5 21:25:29 2025).\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:17:48.510243Z",
     "start_time": "2025-06-19T12:17:48.507992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "imdb_data=IMDBDataset('train')\n",
    "imdb_data_test=IMDBDataset('test')"
   ],
   "id": "ef8db58fc86ea517",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:18:00.058623Z",
     "start_time": "2025-06-19T12:17:59.994618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_size = int(0.8 * len(imdb_data_test))  # 20000\n",
    "val_size = len(imdb_data_test) - test_size  # 5000\n",
    "test_dataset,val_dataset=random_split(imdb_data_test, [test_size, val_size])"
   ],
   "id": "ce454879005fd70b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:18:04.855252Z",
     "start_time": "2025-06-19T12:18:01.182274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "id": "42125ef9ac9bbc37",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:19:48.184613Z",
     "start_time": "2025-06-19T12:19:44.750849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrain_model = AutoModel.from_pretrained('bert-base-uncased',cache_dir=r'D:\\bigdata_project\\models_file')\n",
    "pretrain_model=pretrain_model.to(device)\n",
    "token_tool = AutoTokenizer.from_pretrained('bert-base-uncased',cache_dir=r'D:\\bigdata_project\\models_file')"
   ],
   "id": "f765ab63e751b943",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:19:51.511717Z",
     "start_time": "2025-06-19T12:19:51.354677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup  # 用于去除HTML标签\n",
    "\n",
    "def clean_text(text):\n",
    "    # 去除HTML标签（常见于IMDB数据集）\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    \n",
    "    # 替换或删除特殊字符（保留基本标点）\n",
    "    text = re.sub(r\"@[\\w]+\", \"\", text)                # 移除@提及\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)               # 移除URL\n",
    "    # text = re.sub(r\"[^a-zA-Z0-9!?.,:;'\\\"\\-]\", \" \", text)  # 保留基本字符，其他替换为空格\n",
    "    text = re.sub(r\"\\s+\", \" \", text)                  # 合并多个空格\n",
    "    \n",
    "    # 可选：处理缩写（如 don't → do not）\n",
    "    text = text.replace(\"n't\", \" not\").replace(\"'s\", \" is\").strip().lower()\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def token_embed(text_dict):\n",
    "    text=[clean_text(sample['text']) for sample in text_dict]\n",
    "    label=torch.tensor([sample['label'] for sample in text_dict])\n",
    "\n",
    "    tokenresult=token_tool(text, max_length=512,padding='max_length',return_tensors='pt',truncation=True,add_special_tokens=True)\n",
    "    return {'input_ids':tokenresult['input_ids']\n",
    "            ,'token_type_ids':tokenresult['token_type_ids']\n",
    "            ,'attention_mask':tokenresult['attention_mask']\n",
    "            ,'label':label}\n",
    "\n",
    "train_data_iter=DataLoader(imdb_data,batch_size=64,shuffle=True,drop_last=True,collate_fn=token_embed)\n",
    "val_data_iter=DataLoader(val_dataset,batch_size=64,shuffle=True,drop_last=True,collate_fn=token_embed)\n",
    "test_data_iter=DataLoader(test_dataset,batch_size=64,shuffle=True,drop_last=True,collate_fn=token_embed)"
   ],
   "id": "880380aa071bca3d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:19:56.339692Z",
     "start_time": "2025-06-19T12:19:56.312856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.pre_bert=pretrain_model\n",
    "        self.fc1=nn.Linear(768, 256)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.fc2=nn.Linear(256, 2)\n",
    "        self.dropout=nn.Dropout(0.2)\n",
    "        self.sequential=nn.Sequential(self.fc1,self.relu,self.dropout,self.fc2)\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        with torch.no_grad():\n",
    "            out_temp=self.pre_bert(input_ids=input_ids,token_type_ids=token_type_ids,attention_mask=attention_mask)\n",
    "        out_temp=out_temp.last_hidden_state[:,0]\n",
    "        return self.sequential(out_temp)\n",
    "bert=Bert().to(device)"
   ],
   "id": "5c9b49d970f9bd5",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e07a336e2c50fe2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-19T12:20:00.958742Z",
     "start_time": "2025-06-19T12:20:00.907972Z"
    }
   },
   "cell_type": "code",
   "source": "writer= SummaryWriter('runs/experiment_1')",
   "id": "f170c8a0b6903fc5",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-19T12:30:47.169458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer=optim.Adam(bert.parameters(),lr=1e-5)\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "loss_list=[]\n",
    "train_accuracy_list=[]\n",
    "accuracy_list=[]\n",
    "\n",
    "for epoch in range(500):\n",
    "    for i,sample in enumerate(train_data_iter):\n",
    "        bert.train()\n",
    "        sample={k:v.to(device) for k,v in sample.items()}\n",
    "        optimizer.zero_grad()\n",
    "        result=bert(sample['input_ids'],sample['token_type_ids'],sample['attention_mask'])\n",
    "        loss=loss_fn(result,sample['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i%100==0:\n",
    "            bert.eval()\n",
    "            accuracy_rate=0\n",
    "            for num,sample in enumerate(val_data_iter):\n",
    "                sample={k:v.to(device) for k,v in sample.items()}\n",
    "                with torch.no_grad():\n",
    "                    result=bert(sample['input_ids'],sample['token_type_ids'],sample['attention_mask'])\n",
    "                    accuracy_rate+=(((torch.argmax(result,dim=1)-sample['label'])==0).sum()/64).cpu().detach().numpy()\n",
    "            accuracy_rate=accuracy_rate/(num+1)\n",
    "            \n",
    "            accuracy_list.append(accuracy_rate)\n",
    "            loss_list.append(loss.cpu().detach().numpy())\n",
    "            train_accuracy_list.append((((torch.argmax(result,dim=1)-sample['label'])==0).sum()/64).cpu().detach().numpy())\n",
    "            print(f'损失值：{loss_list[-1]},训练集准确度：{train_accuracy_list[-1]},测试集准确度：{accuracy_list[-1]}')\n",
    "            writer.add_scalar('loss',loss_list[-1],epoch+i//100)\n",
    "            writer.add_scalar('train_accuracy',train_accuracy_list[-1],epoch+i//100)\n",
    "            writer.add_scalar('accuracy_rate',accuracy_list[-1],epoch+i//100)\n",
    "\n",
    "\n",
    "            #     # 实时绘图\n",
    "            # plt.clf()\n",
    "            # plt.plot(loss_list, label='Train Loss')\n",
    "            # plt.plot(accuracy_list, label='Validation accuracy')\n",
    "            # plt.plot(train_accuracy_list, label='Train accuracy')\n",
    "            # plt.xlabel('Epoch')\n",
    "            # plt.ylabel('Loss')\n",
    "            # plt.legend()\n",
    "            # plt.pause(0.1)"
   ],
   "id": "28d9e21e97939c2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:46:48.231351Z",
     "start_time": "2025-06-18T08:46:48.227402Z"
    }
   },
   "cell_type": "code",
   "source": "(torch.argmax(result,dim=1)-torch.tensor(sample['label'],dtype=torch.long)==0).sum()",
   "id": "73a647888cd7e96a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T08:50:46.659477Z",
     "start_time": "2025-06-18T08:50:46.655330Z"
    }
   },
   "cell_type": "code",
   "source": "bert",
   "id": "3b3b7f1e8dc18709",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bert(\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (sequential): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "40b24719c830991a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
