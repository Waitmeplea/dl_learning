{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T09:43:14.359800Z",
     "start_time": "2025-06-05T09:43:10.099795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import datasets\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch"
   ],
   "id": "e34a6523d9c544e6",
   "outputs": [],
   "execution_count": 152
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T06:24:57.350384Z",
     "start_time": "2025-06-05T06:24:40.229664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载 IMDB 数据集\n",
    "cache_dir=r'D:\\code_file\\dl_Practical_operation\\data_file'\n",
    "imdb_dataset = load_dataset(\"imdb\",cache_dir=cache_dir)\n",
    "# .map(): 对所有样本应用一个函数 (非常重要，用于预处理):"
   ],
   "id": "a66ec0afd7a835cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bfa0dcac66f145df9764c1630f4a99cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f5dbbdb5b06e45dc951fb552ed9870b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eca264ef1fa94040bf71a40ab492243f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-05T07:21:31.065788Z",
     "start_time": "2025-06-05T07:21:30.951074Z"
    }
   },
   "source": [
    "# 查看数据集结构\n",
    "paragraphs=[i for i in imdb_dataset['unsupervised']['text']]"
   ],
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T07:30:01.111456Z",
     "start_time": "2025-06-05T07:30:00.945173Z"
    }
   },
   "cell_type": "code",
   "source": "paragraphs_sentence=[paragraph.split('.') for paragraph in paragraphs ]",
   "id": "2d7cd3cb58a1a4fd",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T09:35:40.436368Z",
     "start_time": "2025-06-05T09:35:10.427886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize(content):\n",
    "    tokenized_content=[]\n",
    "    for paragraph_lines in content:\n",
    "        tokenize_list=[i.replace('<br />','').lower().split() for i in paragraph_lines]\n",
    "        tokenized_content.append(tokenize_list)\n",
    "    return tokenized_content\n",
    "\n",
    "def flatten_list(lst):\n",
    "    for i in lst:\n",
    "        if isinstance(i,list):\n",
    "            yield from flatten_list(i)\n",
    "        else:\n",
    "            yield i\n",
    "            \n",
    "class Vocab:\n",
    "    def __init__(self,token_list,min_freq=5,reserved_tokens=None):\n",
    "        \"\"\"传入的词表可以是嵌套列表\"\"\"\n",
    "        token_freq=Counter(flatten_list(token_list))\n",
    "        self._token_freq=[]\n",
    "        self._reserved_tokens=[] if reserved_tokens is None else reserved_tokens\n",
    "        if '<unk>' not in self._reserved_tokens:\n",
    "            self._reserved_tokens=['<unk>']+self._reserved_tokens\n",
    "            \n",
    "        # 每个词的词频 然后过滤掉词频不足的词\n",
    "        for item in sorted(token_freq.items(),key=lambda x:x[1],reverse=True):\n",
    "            if item[1]<min_freq:\n",
    "                break\n",
    "            self._token_freq.append(item)\n",
    "\n",
    "        # idx_to_token 和 token_to_idx\n",
    "        self.id2token=self._reserved_tokens+[i[0] for i in self._token_freq]\n",
    "        self.token2idx={}\n",
    "        for num,token in enumerate(self.id2token):\n",
    "            self.token2idx[token]=num\n",
    "    \n",
    "    def to_idx(self,token):\n",
    "        default=self.token2idx['<unk>']\n",
    "        return self.token2idx.get(token,default)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.id2token[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.id2token)\n",
    "    \n",
    "    @property\n",
    "    def token_freq(self):\n",
    "        return self._token_freq\n",
    "    \n",
    "paragraph_tokenize=tokenize(paragraphs_sentence)"
   ],
   "id": "83b02340a8b5a379",
   "outputs": [],
   "execution_count": 141
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T09:35:46.804007Z",
     "start_time": "2025-06-05T09:35:43.341334Z"
    }
   },
   "cell_type": "code",
   "source": "v=Vocab(paragraph_tokenize)",
   "id": "2ef495baee01a9fc",
   "outputs": [],
   "execution_count": 142
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T09:39:30.860121Z",
     "start_time": "2025-06-05T09:39:30.853713Z"
    }
   },
   "cell_type": "code",
   "source": "paragraph_tokenize[0]",
   "id": "fa1a263a15f8bff0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is', 'just', 'a', 'precious', 'little', 'diamond'],\n",
       " ['the', 'play,', 'the', 'script', 'are', 'excellent'],\n",
       " ['i',\n",
       "  'cant',\n",
       "  'compare',\n",
       "  'this',\n",
       "  'movie',\n",
       "  'with',\n",
       "  'anything',\n",
       "  'else,',\n",
       "  'maybe',\n",
       "  'except',\n",
       "  'the',\n",
       "  'movie',\n",
       "  '\"leon\"',\n",
       "  'wonderfully',\n",
       "  'played',\n",
       "  'by',\n",
       "  'jean',\n",
       "  'reno',\n",
       "  'and',\n",
       "  'natalie',\n",
       "  'portman'],\n",
       " ['but'],\n",
       " [],\n",
       " [],\n",
       " ['what',\n",
       "  'can',\n",
       "  'i',\n",
       "  'say',\n",
       "  'about',\n",
       "  'this',\n",
       "  'one?',\n",
       "  'this',\n",
       "  'is',\n",
       "  'the',\n",
       "  'best',\n",
       "  'movie',\n",
       "  'anne',\n",
       "  'parillaud',\n",
       "  'has',\n",
       "  'ever',\n",
       "  'played',\n",
       "  'in',\n",
       "  '(see',\n",
       "  'please',\n",
       "  '\"frankie',\n",
       "  'starlight\",',\n",
       "  \"she's\",\n",
       "  'speaking',\n",
       "  'english',\n",
       "  'there)',\n",
       "  'to',\n",
       "  'see',\n",
       "  'what',\n",
       "  'i',\n",
       "  'mean'],\n",
       " ['the',\n",
       "  'story',\n",
       "  'of',\n",
       "  'young',\n",
       "  'punk',\n",
       "  'girl',\n",
       "  'nikita,',\n",
       "  'taken',\n",
       "  'into',\n",
       "  'the',\n",
       "  'depraved',\n",
       "  'world',\n",
       "  'of',\n",
       "  'the',\n",
       "  'secret',\n",
       "  'government',\n",
       "  'forces',\n",
       "  'has',\n",
       "  'been',\n",
       "  'exceptionally',\n",
       "  'over',\n",
       "  'used',\n",
       "  'by',\n",
       "  'americans'],\n",
       " ['never',\n",
       "  'mind',\n",
       "  'the',\n",
       "  '\"point',\n",
       "  'of',\n",
       "  'no',\n",
       "  'return\"',\n",
       "  'and',\n",
       "  'especially',\n",
       "  'the',\n",
       "  '\"la',\n",
       "  'femme',\n",
       "  'nikita\"',\n",
       "  'tv',\n",
       "  'series'],\n",
       " ['they',\n",
       "  'cannot',\n",
       "  'compare',\n",
       "  'the',\n",
       "  'original',\n",
       "  'believe',\n",
       "  'me!',\n",
       "  'trash',\n",
       "  'these',\n",
       "  'videos'],\n",
       " ['buy', 'this', 'one,', 'do', 'not', 'rent', 'it,', 'buy', 'it'],\n",
       " ['btw',\n",
       "  'beware',\n",
       "  'of',\n",
       "  'the',\n",
       "  'subtitles',\n",
       "  'of',\n",
       "  'the',\n",
       "  'la',\n",
       "  'company',\n",
       "  'which',\n",
       "  '\"translate\"',\n",
       "  'the',\n",
       "  'us',\n",
       "  'release'],\n",
       " ['what',\n",
       "  'a',\n",
       "  'disgrace!',\n",
       "  'if',\n",
       "  'you',\n",
       "  'cant',\n",
       "  'understand',\n",
       "  'french,',\n",
       "  'get',\n",
       "  'a',\n",
       "  'dubbed',\n",
       "  'version'],\n",
       " ['but', \"you'll\", 'regret', 'later', ':)']]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 150
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-05T09:45:53.786973Z",
     "start_time": "2025-06-05T09:45:53.771852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "# 从0实现一个Encoderblock\n",
    "#1、点积注意力\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1,**kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, valid_lens=None):\n",
    "        #q.shape[-1]是静态维度值（整数）将其包装为张量是冗余操作\n",
    "        # d_lens=torch.tensor(q.shape[-1],device=q.device)\n",
    "        d_lens=q.shape[-1]\n",
    "        #对于标量值，PyTorch会自动处理设备兼容性 所以不用显示todevice\n",
    "        attention_scores=torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(d_lens)\n",
    "        self.attention_weights=masked_softmax(attention_scores, valid_lens)\n",
    "        return torch.matmul(self.dropout(self.attention_weights),v)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,hidden_size,num_heads,dropout=0.1,bias=False,**kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        assert hidden_size%num_heads==0,'整除条件不满足！'\n",
    "        # 三个调整size的 全连接\n",
    "        # 易错点 这里的全连接层都是没有偏置项 因为后续会有layer_normal 即使添加偏置项后续也会在减均值的过程中被吸收掉\n",
    "        #         一个更广义的规则：\n",
    "        # 如果一个线性层（或卷积层）的输出紧接着一个归一化层（Batch Norm, Layer Norm, Instance Norm, Group Norm），那么这个线性层/卷积层中的偏置项就是冗余的，通常会将其设置为 False。\n",
    "        self.W_q=nn.Linear(query_size,hidden_size,bias=bias)\n",
    "        self.W_k=nn.Linear(key_size,hidden_size,bias=bias)\n",
    "        self.W_v=nn.Linear(value_size,hidden_size,bias=bias)\n",
    "        # 最终输出用的全连接\n",
    "        self.W_o=nn.Linear(hidden_size,hidden_size,bias=bias)\n",
    "        # 注意力函数\n",
    "        self.attention=DotProductAttention(dropout=dropout)\n",
    "        # 头数\n",
    "        self.num_heads=num_heads\n",
    "        # 隐藏层数\n",
    "        self.hidden_size=hidden_size\n",
    "\n",
    "\n",
    "    def forward(self,q,k,v,valid_lens=None):\n",
    "        #调整qkv最后一层\n",
    "        # reshape出头数 并放在第二各维度 避免影响遮掩的softmax\n",
    "        # 错了一个地方 self.hidden_size/self.num_heads结果默认是浮点即使结果是整数 reshape无法接受浮点 因此要用//\n",
    "        # q_temp=self.W_q(q).reshape(q.shape[0],q.shape[1],self.num_heads,self.hidden_size/self.num_heads).permute(0,2,1,3)\n",
    "        q_temp=self.W_q(q).reshape(q.shape[0],q.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "        k_temp=self.W_k(k).reshape(k.shape[0],k.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "        v_temp=self.W_v(v).reshape(v.shape[0],v.shape[1],self.num_heads,self.hidden_size//self.num_heads).permute(0,2,1,3)\n",
    "\n",
    "        # 转为三维 将 1 2维度合并\n",
    "        q_temp=q_temp.reshape(-1,q.shape[1],self.hidden_size//self.num_heads)\n",
    "        k_temp=k_temp.reshape(-1,k.shape[1],self.hidden_size//self.num_heads)\n",
    "        v_temp=v_temp.reshape(-1,v.shape[1],self.hidden_size//self.num_heads)\n",
    "\n",
    "        if valid_lens is not None:\n",
    "        # 这里很重要有一个知识点 看上面 其实是在batch_size 后增加了一个维度num_head 然后又reshape成batch_size*num_heads\n",
    "        # 这跟torch和numpy的存储方式有关系 contiguous (行主序)  当然也正是这种存储方式才使得我们要把num_heads 挪到第二维\n",
    "        # 由于每一个batch下增加的多个num_heads 其实都是归属在这个样本下的不同的注意力头的结果 对于这个样本其实他的valid_lens是不变的 也需要重复num_heads次\n",
    "        # 所以对于valid_lens 最简单的做法就是复制num_head次就行 所以使用repeat_interleave\n",
    "        # 当valid_lens 为2d明显要在batch_size维度进行复制，dim=0\n",
    "        # 当valid_lens为1维时，维度大小=batch_size 这跟我们实现的masked_softmax函数有关 显然也是在batch_size维度复制 所以无论valid_lens为多少维度 都是在dim=0维复制\n",
    "            valid_lens=valid_lens.repeat_interleave(self.num_heads,dim=0)\n",
    "\n",
    "\n",
    "        attention_result_total=self.attention(q_temp,k_temp,v_temp,valid_lens)\n",
    "        outputs=attention_result_total.reshape(q.shape[0],self.num_heads,q.shape[1],-1).permute(0,2,1,3).reshape(q.shape[0],q.shape[1],-1)\n",
    "        return self.W_o(outputs)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_len,hidden_size,dropout=0.1,**kwargs):\n",
    "        super(PositionalEncoding, self).__init__(**kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.P=torch.zeros(1,max_len,hidden_size)\n",
    "        # 易错点这里建议不用除法， 直接 ：：2 否则少一个\n",
    "        self.temp=torch.arange(1,max_len+1).unsqueeze(1)/(torch.pow(10000,torch.arange(0,hidden_size,2)/hidden_size))\n",
    "        #1,2 用 1位置  如果一共只有3个 那就是 只有\n",
    "        self.P[:,:,0::2]=torch.sin(self.temp)\n",
    "        self.P[:,:,1::2]=torch.cos(self.temp)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # 注意p和x在第二个维度不一定一样,device也不一定一样\n",
    "        x = x + self.P[:,:x.shape[1],:].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,norm_shape,dropout=0.1,**kwargs):\n",
    "        super(AddNorm, self).__init__(**kwargs)\n",
    "        self.norm=nn.LayerNorm(norm_shape)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x,y):\n",
    "        return self.norm(x+self.dropout(y))\n",
    "\n",
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,ffninput_size,ffnhidden_size,ffnoutput_size,**kwargs):\n",
    "        super(PositionWiseFFN, self).__init__(**kwargs)\n",
    "        self.dense1 = nn.Linear(ffninput_size,ffnhidden_size)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(ffnhidden_size,ffnoutput_size)\n",
    "    def forward(self,x):\n",
    "        x_temp = self.relu(self.dense1(x))\n",
    "        return self.dense2(x_temp)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,key_size,query_size,value_size,hidden_size,num_heads,norm_shape,ffninput_size,ffnhidden_size,dropout=0.1,bias=False,**kwargs):\n",
    "        super(EncoderBlock, self).__init__(**kwargs)\n",
    "        # 位置编码 max=1000 hidden_size 和query的size一样 不是在块里完成的\n",
    "        # self.position_enc = PositionalEncoding(1000,query_size,dropout=dropout)\n",
    "        # 多头自注意力key_size,query_size,value_size,hidden_size这四个应该是全都相等\n",
    "        self.attention=MultiHeadAttention(key_size,query_size,value_size,hidden_size,num_heads,dropout=dropout,bias=bias)\n",
    "        #位置前馈 ffninput_size=ffnoutput_size=hidden_size\n",
    "        self.position_ffn=PositionWiseFFN(ffninput_size,ffnhidden_size,hidden_size,**kwargs)\n",
    "        # norm_shape = (l,hidden_size)\n",
    "        self.add_norm=AddNorm(norm_shape,dropout=dropout)\n",
    "\n",
    "    def forward(self,x_position,valid_lens=None):\n",
    "        y_attention=self.attention(x_position,x_position,x_position,valid_lens=valid_lens)\n",
    "        x_first=self.add_norm(x_position,y_attention)\n",
    "        return self.add_norm(x_first,self.position_ffn(x_first))\n"
   ],
   "id": "9025d6a584c14092",
   "outputs": [],
   "execution_count": 153
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TransformerEncoderClassfier(nn.Module):\n",
    "    def __init__(self):"
   ],
   "id": "460c87e9e7a10699"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
